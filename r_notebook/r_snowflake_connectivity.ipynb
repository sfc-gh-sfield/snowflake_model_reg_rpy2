{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "intro"
      },
      "source": "# R Snowflake Connectivity\n\nThis notebook covers **connecting to Snowflake from R** in Workspace Notebooks.\n\n**Prerequisites:** Run `r_setup_interop.ipynb` Section 1 first to install R.\n\n**Connection Methods:**\n\n| Method | Auth | Query Pushdown | Best For |\n|--------|------|----------------|----------|\n| **Reticulate + Snowpark** | Session token | \u2705 Yes | Easiest - no setup |\n| **ADBC + adbcsnowflake** | PAT token | \u2705 Yes | Direct R queries |\n| **DuckDB + Snowflake ext** | Secret | \u2705 Yes | Local caching + dplyr |\n| **Iceberg REST** | PAT/JWT | \u274c No | Direct file access |\n\n**Quick Start:**\n1. Run Section 1 (Prerequisites)\n2. Try Section 3 (Reticulate) first - it works immediately\n3. Explore other methods as needed\n\n---"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "prereq_header"
      },
      "source": "# Section 1: Prerequisites\n\n**Before using this notebook:**\n\n1. Run `r_setup_interop.ipynb` Section 1 to install R and rpy2\n2. Ensure you have database access (most methods use SNOWFLAKE_SAMPLE_DATA)\n\nThe cells below set up helper functions and verify R is available."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa92858-c66e-4875-bbe0-0e8f9567e2d2",
      "metadata": {
        "language": "python",
        "name": "PY__setup_r_helpers",
        "title": "PY__setup_r_helpers"
      },
      "outputs": [],
      "source": "# Setup R helpers\nimport sys\nsys.path.insert(0, '.')  # Ensure current directory is in path\n\nfrom r_helpers import setup_r_environment\n\nresult = setup_r_environment()\n\nif result['success']:\n    print(\"\u2713 R environment configured successfully\")\n    print(f\"  R version: {result['r_version']}\")\n    print(f\"  rpy2 installed: {result['rpy2_installed']}\")\n    print(f\"  %%R magic registered: {result['magic_registered']}\")\nelse:\n    print(\"\u2717 Setup failed:\")\n    for error in result['errors']:\n        print(f\"  - {error}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c5904b-aaee-4c87-83ec-c391ae8f47db",
      "metadata": {
        "language": "python",
        "name": "R__version",
        "title": "R__version"
      },
      "outputs": [],
      "source": "%%R\n# Print R version (simple output works fine)\nR.version.string"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d5316e-ecac-490c-ae5f-bda9d1bdc534",
      "metadata": {
        "language": "python",
        "name": "PY__run_diagnostics",
        "title": "PY__run_diagnostics"
      },
      "outputs": [],
      "source": "from r_helpers import check_environment, print_diagnostics\n\n# Run and display diagnostics\nprint_diagnostics()"
    },
    {
      "cell_type": "markdown",
      "id": "f35bd66a-491d-4ee4-9e5e-fb66a5fb74fc",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 2: Snowflake Database Connectivity\n\nThis section demonstrates connecting to Snowflake from R using ADBC.\n\n**Prerequisites:**\n- Run the setup script with `--adbc` flag (Section 1.1)\n- Have appropriate Snowflake permissions\n\n## Authentication Options\n\n| Method | Status | Notes |\n|--------|--------|-------|\n| Python `get_active_session()` | \u2705 Works | Use for Snowpark queries, bridge to R via rpy2 |\n| ADBC with PAT | \u2705 Works | Direct R-to-Snowflake, requires PAT token |\n| SPCS OAuth Token | \u274c Blocked | Container token not authorized for ADBC |\n| Username/Password | \u274c Blocked | SPCS requires OAuth |\n\n## Connection Management\n\nThis notebook uses connection pooling - the ADBC connection is stored as `r_sf_con` in R's global environment and reused across cells. This avoids the overhead of creating new connections for each query."
    },
    {
      "cell_type": "markdown",
      "id": "d554f970-0734-4b7a-bac6-de8a4851afd3",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.1 Setup Python Session\n\nThis cell loads configuration and establishes the Snowflake session.\n\n**Configuration:**\n- Copy `notebook_config.yaml.template` to `notebook_config.yaml`\n- Edit with your account details (for Local IDE)\n- The config provides database, schema, warehouse, and query settings\n\n**Environments:**\n- **Workspace Notebook**: Uses `get_active_session()` (built-in OAuth)\n- **Local IDE (VSCode/Cursor)**: Uses config file for key-pair auth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7308601a-bba7-43b7-b962-eb71ccd7b3bf",
      "metadata": {
        "language": "python",
        "name": "PY__setup_session_config",
        "title": "PY__setup_session_config"
      },
      "outputs": [],
      "source": "# Setup Snowflake session and load configuration\nimport os\nimport sys\n\n# =============================================================================\n# Load Configuration File\n# =============================================================================\nCONFIG_FILE = 'notebook_config.yaml'\nCONFIG_TEMPLATE = 'notebook_config.yaml.template'\n\ndef load_config():\n    \"\"\"Load configuration from YAML file.\"\"\"\n    try:\n        import yaml\n    except ImportError:\n        print(\"Installing PyYAML...\")\n        import subprocess\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\", \"-q\"], check=True)\n        import yaml\n    \n    if os.path.exists(CONFIG_FILE):\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f)\n        print(f\"\u2713 Loaded config from {CONFIG_FILE}\")\n        return config\n    elif os.path.exists(CONFIG_TEMPLATE):\n        print(f\"\u2717 Config not found!\")\n        print(f\"  Copy {CONFIG_TEMPLATE} to {CONFIG_FILE} and customize.\")\n        return {}\n    else:\n        print(\"\u2717 No config file found, using defaults\")\n        return {}\n\nCONFIG = load_config()\n\n# Extract config sections for easy access\nCONN_CONFIG = CONFIG.get('connection', {})\nDEFAULTS = CONFIG.get('defaults', {})\nQUERY_CONFIG = CONFIG.get('sample_queries', {})\nICEBERG_CONFIG = CONFIG.get('iceberg', {})\n\n# =============================================================================\n# Environment Detection and Session Setup\n# =============================================================================\ndef detect_environment():\n    \"\"\"\n    Detect if running in Snowflake Workspace Notebook or local IDE.\n    Returns: ('workspace', session) or ('local', config_dict)\n    \"\"\"\n    workspace_indicators = [\n        os.path.exists('/snowflake/session/token'),\n        'SNOWFLAKE_HOST' in os.environ,\n        '/home/udf' in os.getcwd(),\n    ]\n    \n    if any(workspace_indicators):\n        try:\n            from snowflake.snowpark.context import get_active_session\n            session = get_active_session()\n            return ('workspace', session)\n        except Exception as e:\n            return ('workspace_error', str(e))\n    else:\n        return ('local', None)\n\n# Detect environment\nENV_TYPE, ENV_RESULT = detect_environment()\n\nif ENV_TYPE == 'workspace':\n    session = ENV_RESULT\n    \n    # Get connection details from session, with config overrides\n    ACCOUNT = session.sql('SELECT CURRENT_ACCOUNT()').collect()[0][0]\n    USER = session.sql('SELECT CURRENT_USER()').collect()[0][0]\n    DATABASE = DEFAULTS.get('database') or session.get_current_database()\n    SCHEMA = DEFAULTS.get('schema') or session.get_current_schema()\n    WAREHOUSE = DEFAULTS.get('warehouse') or session.get_current_warehouse()\n    ROLE = session.get_current_role()\n    \n    # Build unified config\n    ENV_CONFIG = {\n        'account': ACCOUNT,\n        'user': USER,\n        'database': DATABASE,\n        'schema': SCHEMA,\n        'warehouse': WAREHOUSE,\n        'role': ROLE,\n    }\n    \n    print(f\"\\nEnvironment: Workspace Notebook\")\n    print(f\"  Account:   {ACCOUNT}\")\n    print(f\"  User:      {USER}\")\n    print(f\"  Role:      {ROLE}\")\n    print(f\"  Database:  {DATABASE}\")\n    print(f\"  Schema:    {SCHEMA}\")\n    print(f\"  Warehouse: {WAREHOUSE}\")\n    \nelif ENV_TYPE == 'local':\n    session = None\n    \n    # Use config file values for local IDE\n    ENV_CONFIG = {\n        'account': CONN_CONFIG.get('account', '<YOUR_ACCOUNT>'),\n        'user': CONN_CONFIG.get('user', '<YOUR_USER>'),\n        'database': DEFAULTS.get('database', 'SNOWFLAKE_SAMPLE_DATA'),\n        'schema': DEFAULTS.get('schema', 'TPCH_SF1'),\n        'warehouse': DEFAULTS.get('warehouse', '<YOUR_WAREHOUSE>'),\n        'role': DEFAULTS.get('role', 'PUBLIC'),\n        'private_key_path': CONN_CONFIG.get('private_key_path', '~/.ssh/snowflake_rsa_key.p8'),\n    }\n    \n    print(f\"\\nEnvironment: Local IDE\")\n    print(f\"  Account:   {ENV_CONFIG['account']}\")\n    print(f\"  User:      {ENV_CONFIG['user']}\")\n    print(f\"  Database:  {ENV_CONFIG['database']}\")\n    print(f\"  Warehouse: {ENV_CONFIG['warehouse']}\")\n    print(f\"  Key path:  {ENV_CONFIG['private_key_path']}\")\n    \n    if '<YOUR_' in str(ENV_CONFIG.values()):\n        print(\"\\n\u26a0\ufe0f  Some config values need to be set!\")\n        print(f\"   Edit {CONFIG_FILE} with your values\")\nelse:\n    print(f\"Warning: Environment detection issue: {ENV_RESULT}\")\n    ENV_CONFIG = {}\n    session = None\n\n# Make query config easily accessible\nROW_LIMIT = QUERY_CONFIG.get('default_row_limit', 1000)\nLARGE_ROW_LIMIT = QUERY_CONFIG.get('large_row_limit', 10000)\nSAMPLE_START_DATE = QUERY_CONFIG.get('sample_start_date', '1995-01-01')\nTABLES = QUERY_CONFIG.get('tables', {'nation': 'NATION', 'customer': 'CUSTOMER', 'orders': 'ORDERS'})"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Authentication Methods Overview\n\nThis notebook supports multiple authentication methods for different connectivity approaches:\n\n| Section | Method | Auth Type | Status | Environment |\n|---------|--------|-----------|--------|-------------|\n| **3.1** | `get_active_session()` | Built-in OAuth | \u2705 Working | Workspace |\n| **3.3-3.6** | ADBC + PAT | PAT Token | \u2705 Working | Workspace |\n| **4.2** | ADBC + Key Pair | JWT | \u2705 Working | Both |\n| **5** | Reticulate | Session OAuth | \u2705 Working | Workspace |\n| **7** (DuckDB) | Key Pair | JWT | \u2705 Working | Local IDE |\n| **7.3.1** | Python Bridge | Session OAuth | \u2705 Working | Both |\n| **8** | Horizon Catalog | JWT | \u2705 Working | Local IDE |\n\n**Recommended Path for Most Users:**\n1. Run **Section 3.1** (required - sets up session)\n2. Choose ONE of:\n   - **Section 5** (Reticulate) - Easiest, uses built-in auth, works everywhere\n   - **Section 7.3.1** (Python Bridge) - For dplyr workflows, works everywhere  \n   - **Section 7** (DuckDB Direct) - For Local IDE only (key-pair required)\n   - **Section 3** (ADBC) - For direct R-to-Snowflake in Workspace",
      "id": "b4129cb2-87dd-4ac0-9865-249bcc5fad1f"
    },
    {
      "cell_type": "markdown",
      "id": "009cd9dc-7edb-45c5-a9ac-4ef46cd7343f",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.2 Create Programmatic Access Token (PAT) (For ADBC - Optional)\n\n**Used by:** Section 3 (R-ADBC) only. Skip if using Section 5 (Reticulate) or Section 7 (DuckDB).\n\nPAT enables direct R-to-Snowflake ADBC connections. The session from Section 3.1 is used to create the token."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c5b82f-6b67-475f-ba93-82c671356582",
      "metadata": {
        "language": "python",
        "name": "PY__create_pat",
        "title": "PY__create_pat"
      },
      "outputs": [],
      "source": "# Create PAT for authentication (requires session from 3.1)\nfrom r_helpers import PATManager\n\n# Uses the 'session' variable from Section 3.1\nif session is None:\n    print(\"Please run Section 3.1 first!\")\nelse:\n    pat_manager = PATManager(session)\n    pat_result = pat_manager.create_pat()  # Creates PAT with 1 day expiry\n    \n    if pat_result['success']:\n        print(f\"\u2713 PAT created successfully\")\n        print(f\"  Token: {pat_result['token'][:20]}...\")\n        print(f\"  Expires: {pat_result['expires_at']}\")\n    else:\n        print(f\"\u2717 PAT creation failed: {pat_result.get('error', 'Unknown error')}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e60a83b-c4aa-4aa2-bb04-ea156b006f94",
      "metadata": {
        "language": "python",
        "name": "PY__check_pat_status",
        "title": "PY__check_pat_status"
      },
      "outputs": [],
      "source": "# Check PAT status at any time\nstatus = pat_mgr.get_status()\nprint(\"PAT Status:\")\nfor key, value in status.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "markdown",
      "id": "842178d4-957e-4c55-8c11-e0b39f1246be",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.3 Validate ADBC Prerequisites\n\nBefore connecting, validate that all ADBC prerequisites are met."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6864614e-d4dc-4470-af1d-8d7664faaa2b",
      "metadata": {
        "language": "python",
        "name": "PY__validate_adbc",
        "title": "PY__validate_adbc"
      },
      "outputs": [],
      "source": "from r_helpers import validate_adbc_connection\n\nvalid, message = validate_adbc_connection()\nprint(message)"
    },
    {
      "cell_type": "markdown",
      "id": "6eff7ee7-16d2-411d-b76c-ee5773148b32",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.4 Initialize R Connection Management\n\nLoad the connection management functions into R. This provides:\n- `get_snowflake_connection()` - Get or create connection (stored as `r_sf_con`)\n- `close_snowflake_connection()` - Close and release connection\n- `is_snowflake_connected()` - Check connection status\n- `snowflake_connection_status()` - Get detailed status"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a8002d-624c-444d-80de-e99924526040",
      "metadata": {
        "language": "python",
        "name": "PY__init_r_connection",
        "title": "PY__init_r_connection"
      },
      "outputs": [],
      "source": "from r_helpers import init_r_connection_management\n\nsuccess, msg = init_r_connection_management()\nprint(msg)"
    },
    {
      "cell_type": "markdown",
      "id": "160d5092-fac0-42e8-b661-c7f286bdd7c6",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.5 Connect to Snowflake from R (ADBC)\n\nUse `get_snowflake_connection()` to establish or reuse the ADBC connection.\n\nThe connection is stored as `r_sf_con` in R's global environment and is automatically reused in subsequent cells."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34f2297-86d4-4141-ade0-4e16e852a595",
      "metadata": {
        "language": "python",
        "name": "R__adbc_connect",
        "title": "R__adbc_connect"
      },
      "outputs": [],
      "source": "%%R\n# Get or create the Snowflake connection\n# Connection is stored globally as r_sf_con\nr_sf_con <- get_snowflake_connection()\n\n# Show connection status (uses print_connection_status() for clean output)\nprint_connection_status()"
    },
    {
      "cell_type": "markdown",
      "id": "7253f50d-eec9-4108-a3bf-2f917bbddd8a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.6 Query Snowflake from R\n\nRun queries using the `r_sf_con` connection. The connection is automatically reused across cells."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4d1703-f628-4683-b543-fe90d645e996",
      "metadata": {
        "language": "python",
        "name": "R__adbc_test",
        "title": "R__adbc_test"
      },
      "outputs": [],
      "source": "%%R\n# Simple test query using r_sf_con\nr_sf_con |>\n  read_adbc(\"SELECT CURRENT_USER() AS USER, CURRENT_ROLE() AS ROLE, CURRENT_WAREHOUSE() AS WAREHOUSE\") |>\n  tibble::as_tibble()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb06f5a2-bd50-460c-835e-10336cf2ce26",
      "metadata": {
        "language": "python",
        "name": "R__adbc_nations",
        "title": "R__adbc_nations"
      },
      "outputs": [],
      "source": "%%R\n# Query sample data from Snowflake\n# Using the shared SNOWFLAKE_SAMPLE_DATA database\nnations <- r_sf_con |>\n  read_adbc(\"\n    SELECT N_NATIONKEY, N_NAME, N_REGIONKEY \n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION \n    ORDER BY N_NATIONKEY\n    LIMIT 10\n  \") |>\n  tibble::as_tibble()\n\nnations"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7b830e-423c-44dc-b991-5f77041f4596",
      "metadata": {
        "language": "python",
        "name": "R__adbc_orders",
        "title": "R__adbc_orders"
      },
      "outputs": [],
      "source": "%%R\n# More complex query with aggregation\nlibrary(dplyr)\n\norders_summary <- r_sf_con |>\n  read_adbc(\"\n    SELECT \n      O_ORDERSTATUS,\n      COUNT(*) as ORDER_COUNT,\n      SUM(O_TOTALPRICE) as TOTAL_VALUE,\n      AVG(O_TOTALPRICE) as AVG_VALUE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    GROUP BY O_ORDERSTATUS\n    ORDER BY ORDER_COUNT DESC\n  \") |>\n  tibble::as_tibble()\n\norders_summary"
    },
    {
      "cell_type": "markdown",
      "id": "dplyr-demo-intro",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.7 dplyr with Snowflake via ADBC (Future)\n\n> **\u26a0\ufe0f NOT WORKING YET** - This section documents the intended approach, but it's blocked by missing driver methods.\n\n### The Goal\n\nR users want dplyr syntax with lazy evaluation and query pushdown:\n\n```r\ntbl(con, \"CUSTOMER\") %>%\n  filter(C_MKTSEGMENT == \"BUILDING\") %>%\n  collect()  # SQL runs on Snowflake\n```\n\n### Current Blockers\n\n1. **adbcsnowflake** driver missing `GetParameterSchema` method ([apache/arrow-adbc](https://github.com/apache/arrow-adbc))\n2. **dbplyr** ADBC backend in development ([tidyverse/dbplyr#1787](https://github.com/tidyverse/dbplyr/issues/1787))\n\n### Working Alternative\n\nUse the **hybrid approach** in Section 7.3.1: fetch via ADBC \u2192 load to DuckDB \u2192 use dplyr locally.\n\n### Track Progress\n\n- [tidyverse/dbplyr#1787](https://github.com/tidyverse/dbplyr/issues/1787) - Hadley's ADBC backend work\n- [apache/arrow-adbc](https://github.com/apache/arrow-adbc) - Snowflake driver updates"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dplyr-snowflake-demo",
      "metadata": {
        "language": "python",
        "name": "R__dplyr_snowflake",
        "title": "R__dplyr_snowflake"
      },
      "outputs": [],
      "source": "%%R\n# NOT WORKING YET - adbi + adbcsnowflake blocked by missing GetParameterSchema\n# See markdown cell above for details\n#\n# When the ecosystem is ready, this will work:\n#\n# library(adbi)\n# library(DBI)\n# library(dplyr)\n# library(dbplyr)\n#\n# con <- dbConnect(\n#     adbi::adbi(\"adbcsnowflake\"),\n#     username = Sys.getenv(\"SNOWFLAKE_USER\"),\n#     `adbc.snowflake.sql.account` = Sys.getenv(\"SNOWFLAKE_ACCOUNT\"),\n#     `adbc.snowflake.sql.auth_type` = \"auth_pat\",\n#     `adbc.snowflake.sql.client_option.auth_token` = Sys.getenv(\"SNOWFLAKE_PAT\"),\n#     ...\n# )\n#\n# # Then use dplyr!\n# tbl(con, \"CUSTOMER\") %>%\n#     filter(C_MKTSEGMENT == \"BUILDING\") %>%\n#     collect()\n\ncat(\"adbi + dbplyr for Snowflake is not yet working.\\n\")\ncat(\"Use Section 7.3.1 (Python Bridge) or hybrid approach instead.\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9781d24e-c833-48cf-aad8-199ab9a66dda",
      "metadata": {
        "language": "python",
        "name": "R__adbc_verify",
        "title": "R__adbc_verify"
      },
      "outputs": [],
      "source": "%%R\n# Verify connection is being reused (not recreated)\ncat(\"Connection still valid:\", is_snowflake_connected(), \"\\n\")"
    },
    {
      "cell_type": "markdown",
      "id": "548ca051-cac6-49f9-96a5-96c671d6c933",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.8 Query from Python, Analyze in R\n\nAn alternative pattern: use Python's Snowpark session for querying, then pass data to R for analysis."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd3f588-93a1-4a0a-a758-e6def9a507aa",
      "metadata": {
        "language": "python",
        "name": "PY__query_customers_python",
        "title": "PY__query_customers_python"
      },
      "outputs": [],
      "source": "# Query Snowflake via Python\ncustomers_df = session.sql(f\"\"\"\n    SELECT \n        C_CUSTKEY,\n        C_NAME,\n        C_NATIONKEY,\n        C_ACCTBAL\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n    LIMIT 100\n\"\"\").to_pandas()\n\nprint(f\"Retrieved {len(customers_df)} rows\")\ncustomers_df.head()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97291af9-fb1a-44ba-bdd0-382c37bfe407",
      "metadata": {
        "language": "python",
        "name": "R__analyze_customers",
        "title": "R__analyze_customers"
      },
      "outputs": [],
      "source": "%%R -i customers_df\n# Analyze the data in R\nlibrary(dplyr)\n\ncat(\"Summary Statistics for Customer Account Balance:\\n\")\nrprint(summary(customers_df$C_ACCTBAL))\n\ncat(\"\\nCustomers by Nation (top 5):\\n\")\nresult <- customers_df %>%\n  group_by(C_NATIONKEY) %>%\n  summarise(\n    count = n(),\n    avg_balance = mean(C_ACCTBAL),\n    total_balance = sum(C_ACCTBAL)\n  ) %>%\n  arrange(desc(count)) %>%\n  head(5)\n\nrprint(result)  # Use rprint() for clean output"
    },
    {
      "cell_type": "markdown",
      "id": "fa4dd5c0-f9b0-4c2b-8173-5e72053d84cd",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.9 Check Connection Status\n\nYou can check the connection status from either Python or R."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65cbb49-52c4-4233-8e5f-0b49fb52b95d",
      "metadata": {
        "language": "python",
        "name": "PY__check_connection_python",
        "title": "PY__check_connection_python"
      },
      "outputs": [],
      "source": "# Check status from Python\nfrom r_helpers import get_r_connection_status\n\nstatus = get_r_connection_status()\nprint(\"R Connection Status (from Python):\")\nfor key, value in status.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8276ca-a924-440f-9e58-0476efa92af5",
      "metadata": {
        "language": "python",
        "name": "R__check_connection",
        "title": "R__check_connection"
      },
      "outputs": [],
      "source": "%%R\n# Get or create the Snowflake connection\n# Connection is stored globally as r_sf_con\nr_sf_con <- get_snowflake_connection()\n\n# Show connection status (uses print_connection_status() for clean output)\nprint_connection_status()"
    },
    {
      "cell_type": "markdown",
      "id": "93b418d9-faa8-435f-b2fd-5da6f377951c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.9 Clean Up\n\nClose ADBC connection and optionally remove the PAT."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e5e418-d204-4d6f-b220-aab224bdaef3",
      "metadata": {
        "language": "python",
        "name": "R__close_connection",
        "title": "R__close_connection"
      },
      "outputs": [],
      "source": "%%R\n# Close the Snowflake connection\nclose_snowflake_connection()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b970b88d-e34b-4a8e-b86b-48c60e2ca6ec",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Alternative: Close from Python\n# from r_helpers import close_r_connection\n# success, msg = close_r_connection()\n# print(msg)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c429c3-d3ff-4a73-bd38-acb15f244eb1",
      "metadata": {
        "language": "python",
        "name": "PY__remove_pat",
        "title": "PY__remove_pat"
      },
      "outputs": [],
      "source": "# Cleanup - remove PAT\n# pat_mgr.remove_pat()\n# print(\"PAT removed\")"
    },
    {
      "cell_type": "markdown",
      "id": "7c34f713-51a1-49b1-91ea-1a8d48c6b0fb",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 3: Alternative Authentication - Key Pair (JWT)\n\nThis section demonstrates Key Pair (JWT) authentication as an alternative to PAT.\n\n## Authentication Methods for R ADBC\n\n| Method | Status | Notes |\n|--------|--------|-------|\n| **PAT (Programmatic Access Token)** | \u2705 Working | **Recommended** - easiest to set up (see Section 3) |\n| **Key Pair (JWT)** | \u2705 Working | Alternative - no token expiry, shown below |\n| SPCS OAuth Token | \u274c Blocked | Container token restricted to specific connectors |\n| Username/Password | \u274c Blocked | SPCS enforces OAuth for internal connections |\n\n> **Note:** For tests of non-working methods, see `archive/auth_methods_not_working.ipynb`\n\n## Prerequisites\n\n- ADBC installed (`--adbc` flag during setup)\n- RSA key pair generated\n- Public key registered with your Snowflake user"
    },
    {
      "cell_type": "markdown",
      "id": "c881522f-bb36-4052-a258-f9453b36b5e6",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.1 Load Alternative Auth Test Functions\n\nLoad the R functions for testing different authentication methods."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f299b22-9a97-4fbc-9193-5a0f6ba41c7a",
      "metadata": {
        "language": "python",
        "name": "PY__load_alt_auth",
        "title": "PY__load_alt_auth"
      },
      "outputs": [],
      "source": "from r_helpers import init_r_alt_auth\n\nsuccess, msg = init_r_alt_auth()\nprint(msg)"
    },
    {
      "cell_type": "markdown",
      "id": "f6b2a13e-1808-44df-bc82-f2e860e34329",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.2 Key Pair (JWT) Authentication (Alternative for ADBC)\n\n**Used by:** Section 3 (R-ADBC) as an alternative to PAT, and Section 8 (Iceberg) for Horizon Catalog API.\n\nKey pair authentication uses RSA keys instead of passwords/PAT. This method is MFA-compatible and doesn't expire like PAT tokens."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9574456f-8ab5-498f-b95e-c7941ed73936",
      "metadata": {
        "language": "python",
        "name": "PY__keypair_setup",
        "title": "PY__keypair_setup"
      },
      "outputs": [],
      "source": "# Key-pair authentication setup\nfrom r_helpers import KeyPairAuth\n\n# Initialize key pair auth helper\nkp_auth = KeyPairAuth()\n\n# Generate a new key pair (or use load_private_key() for existing key)\n# Note: Requires 'cryptography' package: pip install cryptography\nresult = kp_auth.generate_key_pair(\n    key_size=2048,\n    output_dir=\"/tmp\",\n    passphrase=None  # Set a passphrase for encrypted key\n)\n\nif result['success']:\n    print(\"\u2713 Key pair generated successfully\")\n    print(f\"  Private key: {result['private_key_path']}\")\n    print(f\"  Public key:  {result['public_key_path']}\")\n    print(f\"\\n  Public key for Snowflake registration:\")\n    print(f\"  {result['public_key_for_snowflake'][:50]}...\")\nelse:\n    print(f\"\u2717 Key generation failed: {result['error']}\")"
    },
    {
      "cell_type": "markdown",
      "id": "735ac1b2-caec-4807-9dda-acb7eeabe26f",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Step 2: Register Public Key with Snowflake\n\nRun this SQL to register the public key with your user (requires ACCOUNTADMIN or appropriate privileges)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093fc815-1bd0-4f97-899d-c60c1addd15a",
      "metadata": {
        "language": "python",
        "name": "PY__generate_key_sql",
        "title": "PY__generate_key_sql"
      },
      "outputs": [],
      "source": "# Generate key registration SQL\nif result['success']:\n    sql = kp_auth.register_public_key_sql(result['public_key_for_snowflake'])\n    print(\"Run this SQL to register the public key:\")\n    print(\"-\" * 60)\n    print(sql)\n    print(\"-\" * 60)\n    print(\"\\nOr run via Snowpark session:\")\n    print(\"  session.sql(sql).collect()\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28554e82-cf03-48f1-bf54-697101761076",
      "metadata": {
        "language": "python",
        "name": "PY__register_key_sql",
        "title": "PY__register_key_sql"
      },
      "outputs": [],
      "source": "# Register public key in Snowflake\nsession.sql(sql).collect()"
    },
    {
      "cell_type": "markdown",
      "id": "229490a2-9617-4de1-a555-564dce292bef",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Step 3: Configure and Test Key Pair Auth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539bf7b1-8623-474c-ae55-8401f263f053",
      "metadata": {
        "language": "python",
        "name": "PY__test_keypair",
        "title": "PY__test_keypair"
      },
      "outputs": [],
      "source": "# Configure environment for key pair auth\nconfig = kp_auth.configure_for_adbc()\nprint(\"Key Pair Auth Configuration:\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e962629-43d8-49d8-84de-ab7506019bb3",
      "metadata": {
        "language": "python",
        "name": "R__keypair_auth",
        "title": "R__keypair_auth"
      },
      "outputs": [],
      "source": "%%R\n# Test key pair authentication\n# Note: Public key must be registered with user first!\nresult <- test_keypair_auth()\nrprint(result)"
    },
    {
      "cell_type": "markdown",
      "id": "9d37aef7-8b00-41e1-9cba-f50380f8dcf9",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.3 Authentication Summary\n\n### Working Methods\n\n| Method | Auth Type | Best For |\n|--------|-----------|----------|\n| **PAT** | `auth_pat` | Most use cases - easy programmatic setup |\n| **Key Pair** | `auth_jwt` | Long-lived credentials without expiry |\n\n### Non-Working Methods (Blocked by SPCS)\n\n| Method | Reason |\n|--------|--------|\n| SPCS OAuth Token | Restricted to specific Snowflake connectors |\n| Username/Password | SPCS enforces OAuth internally |\n\n> See `archive/auth_methods_not_working.ipynb` for test code if needed."
    },
    {
      "cell_type": "markdown",
      "id": "5e2f1f8f-eee0-4da1-b29a-071880cd4a71",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 4: Reticulate - Access Snowpark from R\n\nThis section demonstrates using **reticulate** to access the Python Snowpark session directly from R. This is an alternative to ADBC that leverages the notebook's built-in authentication.\n\n## Advantages of Reticulate Approach\n\n| Feature | Reticulate + Snowpark | ADBC |\n|---------|----------------------|------|\n| Authentication | Uses notebook's built-in auth | Requires PAT or Key Pair |\n| Setup | No additional auth setup | PAT creation or key registration |\n| Connection | Shares Python session | Separate R connection |\n| Best for | Quick queries, prototyping | Production R pipelines |\n\n## How It Works\n\n1. R accesses Python's Snowpark session via reticulate\n2. Execute SQL queries using `session$sql()`\n3. Convert results to pandas DataFrame with `.to_pandas()`\n4. Reticulate automatically converts pandas \u2192 R data.frame\n\n## Output Pattern\n\nFor best display in Notebooks, use `%%R -o variable` to export R data frames to Python, then display them in a subsequent Python cell. This lets the Notebook render the DataFrame with proper formatting."
    },
    {
      "cell_type": "markdown",
      "id": "59d0ea0c-5ae6-4efd-aa08-0da369432f46",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.1 Setup Reticulate\n\nConfigure reticulate to use the notebook's Python environment.\n\n> **Note:** You may see a warning about reticulate/rpy2 compatibility. This is safe to ignore if using reticulate >= 1.25 (installed by default). The issue was fixed in reticulate PR #1188."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233e9988-bb37-4569-9960-6b9d72f2048a",
      "metadata": {
        "language": "python",
        "name": "R__setup_reticulate",
        "title": "R__setup_reticulate"
      },
      "outputs": [],
      "source": "%%R\nlibrary(reticulate)\n\n# Use the same Python that's running the notebook kernel\n# This ensures we access the same Snowpark session\nuse_python(Sys.which(\"python3\"), required = TRUE)\n\n# Verify Python is accessible\npy_config()"
    },
    {
      "cell_type": "markdown",
      "id": "a744bd12-1ed8-4e64-9129-a25142bd0e1e",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.2 Access Snowpark Session from R\n\nImport the Snowpark module and get the active session. This uses the notebook's built-in authentication - no PAT required!"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4f44dd-af95-4cb4-8d75-70dc663cc358",
      "metadata": {
        "language": "python",
        "name": "R__access_snowpark",
        "title": "R__access_snowpark"
      },
      "outputs": [],
      "source": "%%R\n# Import Snowpark module\nsnowpark <- import(\"snowflake.snowpark\")\n\n# Get the active session (uses notebook's built-in auth)\nsession <- snowpark$Session$builder$getOrCreate()\n\n# Verify connection\nrcat(\"Connected to Snowflake via Snowpark!\")\nrcat(\"Account: \", session$get_current_account())\nrcat(\"User: \", session$get_current_user())\nrcat(\"Database: \", session$get_current_database())\nrcat(\"Schema: \", session$get_current_schema())"
    },
    {
      "cell_type": "markdown",
      "id": "50bdd984-6ac7-4db3-a7fa-25807dfa7a9a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.3 Query Snowflake and Get R DataFrame\n\nExecute SQL queries and convert results to R data frames.\n\n**Output Pattern:** Use `%%R -o variable` to export results to Python, then display in the next cell for nice Notebook formatting."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ebfdab-d6b3-4056-b3fc-94dfe140a945",
      "metadata": {
        "language": "python",
        "name": "R__query_nations",
        "title": "R__query_nations"
      },
      "outputs": [],
      "source": "%%R -o nations_df\n# Execute a query and get Snowpark DataFrame\n# Use -o to export result to Python for nice display\nnations_df <- session$sql(\"\n    SELECT N_NATIONKEY, N_NAME, N_REGIONKEY \n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION \n    LIMIT 10\n\")$to_pandas()\n\n# Print data type (R sees this as a data.frame)\ncat(\"R data type:\", class(nations_df), \"\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd668a1b-6f73-4e27-ac85-f7f174233ce4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the exported DataFrame (nice Notebook rendering)\nnations_df"
    },
    {
      "cell_type": "markdown",
      "id": "e746a041-5ed3-4e76-928f-c4a0f5a4f98b",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.4 R Analysis on Snowflake Data\n\nPerform R analysis using dplyr on data retrieved via Snowpark. Use `-o` to export the result for display."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e853f863-b88b-44b1-8a3f-5179ab84c357",
      "metadata": {
        "language": "python",
        "name": "R__customer_analysis",
        "title": "R__customer_analysis"
      },
      "outputs": [],
      "source": "%%R -o customer_analysis\n# Query customer data with aggregation\ncustomers_df <- session$sql(\"\n    SELECT \n        C_MKTSEGMENT,\n        COUNT(*) as CUSTOMER_COUNT,\n        AVG(C_ACCTBAL) as AVG_BALANCE,\n        MIN(C_ACCTBAL) as MIN_BALANCE,\n        MAX(C_ACCTBAL) as MAX_BALANCE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n    GROUP BY C_MKTSEGMENT\n    ORDER BY AVG_BALANCE DESC\n\")$to_pandas()\n\n# Use dplyr for additional analysis\nlibrary(dplyr)\n\ncustomer_analysis <- customers_df %>%\n    mutate(\n        BALANCE_RANGE = MAX_BALANCE - MIN_BALANCE,\n        SEGMENT_SIZE = case_when(\n            CUSTOMER_COUNT > 30000 ~ \"Large\",\n            CUSTOMER_COUNT > 29000 ~ \"Medium\",\n            TRUE ~ \"Small\"\n        )\n    )\n\ncat(\"Analysis complete - result exported to Python\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e5910f-a6ad-4329-a355-80ec1eef5f0d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the R analysis result (exported via -o)\ncustomer_analysis"
    },
    {
      "cell_type": "markdown",
      "id": "4d900768-6b13-4381-9477-23d9cdc568b4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.5 Helper Function for Snowpark Queries\n\nCreate a convenience function to simplify querying. Use `-o` to export results for display."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86189b02-9ffd-4fa3-9b58-a52b12e67f08",
      "metadata": {
        "language": "python",
        "name": "R__snowpark_helper",
        "title": "R__snowpark_helper"
      },
      "outputs": [],
      "source": "%%R -o orders_summary\n#' Query Snowflake via Snowpark and return R data.frame\n#' \n#' @param sql SQL query string\n#' @return R data.frame with query results\nsnowpark_query <- function(sql) {\n    session$sql(sql)$to_pandas()\n}\n\n# Example usage - export result with -o\norders_summary <- snowpark_query(\"\n    SELECT \n        O_ORDERSTATUS,\n        COUNT(*) as ORDER_COUNT,\n        SUM(O_TOTALPRICE) as TOTAL_VALUE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    GROUP BY O_ORDERSTATUS\n\")\n\ncat(\"Query complete - orders_summary exported to Python\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bc16d2-a25f-4493-bc59-867c0a381dc8",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the orders summary (exported via -o)\norders_summary"
    },
    {
      "cell_type": "markdown",
      "id": "4bf7b467-24bc-4eb3-b80d-e6fb1340fdaf",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.6 Reticulate vs ADBC Comparison\n\n| Aspect | Reticulate + Snowpark | ADBC (Section 3 & 4) |\n|--------|----------------------|----------------------|\n| **Authentication** | Automatic (notebook's session) | PAT or Key Pair required |\n| **Setup complexity** | Minimal | Moderate |\n| **Data path** | Snowflake \u2192 Snowpark \u2192 pandas \u2192 R | Snowflake \u2192 Arrow \u2192 R |\n| **Performance** | Good for moderate data | Better for large data (Arrow) |\n| **R-native** | No (via Python) | Yes (native R driver) |\n| **Best for** | Quick analysis, prototyping | Production R workflows |\n\n### When to Use Each\n\n**Use Reticulate + Snowpark when:**\n- You need quick access without auth setup\n- Working interactively/prototyping\n- Data sizes are moderate (< 1M rows)\n- You're already using Python and R together\n\n**Use ADBC when:**\n- Building production R pipelines\n- Working with large datasets\n- Need pure R solution\n- Require connection pooling/management"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-section-header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 5: DuckDB Integration (Local Caching)\n\n## Goal: dplyr on Snowflake Data\n\n**R users want to write dplyr, not SQL.** This section provides a working approach using DuckDB as a local cache.\n\n### Why DuckDB?\n\nNative `adbi + dbplyr \u2192 Snowflake` is **not yet working** (see Section 3.7). The workaround:\n\n```\nSnowflake \u2192 (ADBC or Python) \u2192 DuckDB (local) \u2192 dplyr\n```\n\nThis \"hybrid\" approach lets you:\n- Fetch data once from Snowflake\n- Use full dplyr workflows locally on DuckDB\n- Get fast iteration without repeated Snowflake queries\n\n### When to Use This Section\n\n- **Local caching** - Fetch once, analyze many times without hitting Snowflake\n- **dplyr workflows** - Use familiar dplyr syntax with lazy evaluation on DuckDB\n- **Cross-engine queries** - Join Snowflake data with local files/parquet\n- **Complex analytics** - Use DuckDB's window functions, spatial extensions, etc.\n\n## Architecture\n\n```\nR (dplyr/dbplyr)\n    \u2195 DBI\nDuckDB (in-memory or file)\n    \u2191 Data loaded via:\n      \u2022 Python bridge (Section 7.3.1)\n      \u2022 ADBC fetch \u2192 DuckDB insert\nSnowflake\n```"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-env-detection",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.1 Prerequisites\n\n**Session Setup**: Environment detection and session setup is now handled in **Section 3.1**.\nMake sure you've run that cell before proceeding with DuckDB integration.\n\nThe `ENV_TYPE` variable tells you which environment you're in:\n- `'workspace'` - Workspace Notebook (uses OAuth)\n- `'local'` - Local IDE (uses key-pair auth)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-env-setup",
      "metadata": {
        "language": "python",
        "name": "PY__verify_session",
        "title": "PY__verify_session"
      },
      "outputs": [],
      "source": "# Verify session from Section 3.1\nif 'ENV_TYPE' not in dir():\n    print(\"Please run Section 3.1 first to set up the session!\")\nelse:\n    print(f\"Environment: {ENV_TYPE}\")\n    print(f\"Session: {'Available' if session else 'Not available (local mode)'}\")\n    if ENV_CONFIG:\n        print(f\"Account: {ENV_CONFIG.get('account', 'N/A')}\")"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-local-config",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.2 Configure Connection (Local IDE Only)\n\n**Skip this section if running in Workspace Notebook.**\n\nFor local IDEs, set these environment variables before starting your notebook:\n\n```bash\nexport SNOWFLAKE_ACCOUNT=\"your_account\"     # e.g., \"xy12345\"  \nexport SNOWFLAKE_USER=\"your_user\"\nexport SNOWFLAKE_DATABASE=\"SNOWFLAKE_SAMPLE_DATA\"\nexport SNOWFLAKE_WAREHOUSE=\"COMPUTE_WH\"\nexport SNOWFLAKE_PRIVATE_KEY_PATH=\"~/.ssh/snowflake_rsa_key.p8\"\n```\n\nThen restart your notebook and run Section 3.1 to detect the environment."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-local-config-cell",
      "metadata": {
        "language": "python",
        "name": "PY__local_ide_config",
        "title": "PY__local_ide_config"
      },
      "outputs": [],
      "source": "# Local IDE verification (optional)\n# Environment variables should be set before starting the notebook\n# This cell just displays the current configuration\n\nif ENV_TYPE == 'local':\n    print(\"Local IDE Configuration:\")\n    for key in ['account', 'user', 'database', 'warehouse', 'private_key_path']:\n        print(f\"  {key}: {ENV_CONFIG.get(key, 'N/A')}\")\n    \n    # Check if key file exists\n    key_path = os.path.expanduser(ENV_CONFIG.get('private_key_path', ''))\n    if os.path.exists(key_path):\n        print(f\"\\n\u2713 Private key file found\")\n    else:\n        print(f\"\\n\u2717 Private key file NOT found: {key_path}\")\nelse:\n    print(\"Running in Workspace - no local configuration needed\")"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-r-setup",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.3 DuckDB + Snowflake Setup in R\n\nThis section configures DuckDB with the Snowflake extension for direct database connectivity.\n\n### Authentication Options (per [iqea-ai/duckdb-snowflake](https://github.com/iqea-ai/duckdb-snowflake))\n\n| Method | Status | Notes |\n|--------|--------|-------|\n| **Key-pair** | \u2705 Tested | Recommended for production |\n| **Password** | \u2705 Tested | For development only |\n| **OAuth** | \u26a0\ufe0f Known Issues | Not recommended |\n\n### Environment-Specific Guidance\n\n| Environment | Recommended Approach | Key Access |\n|-------------|---------------------|------------|\n| **Local IDE** | Direct DuckDB\u2192Snowflake | Key file on disk |\n| **SPCS / Remote Dev** | Direct DuckDB\u2192Snowflake | Mount secret via service spec |\n| **Workspace Notebooks** | **Python Bridge (7.3.1)** | No key needed - uses session |\n\n### Why Python Bridge for Workspace Notebooks?\n\nWorkspace Notebooks (vNext) have limitations for private key access:\n- `st.secrets` not supported (Streamlit not available)\n- No service spec control (can't mount secrets)\n- UDF workaround requires elevated privileges (CREATE SECRET, CREATE INTEGRATION)\n\nThe **Python Bridge** avoids these issues entirely by using the existing Snowpark session."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-r-setup-cell",
      "metadata": {
        "language": "python",
        "name": "R__duckdb_setup",
        "title": "R__duckdb_setup"
      },
      "outputs": [],
      "source": "%%R\n# Install duckdb if not available (via micromamba - faster)\nif (!requireNamespace(\"duckdb\", quietly = TRUE)) {\n    cat(\"Installing duckdb R package via micromamba...\\n\")\n    system(\"/root/.local/share/mamba/bin/micromamba install -n r_env -c conda-forge r-duckdb -y\",\n           ignore.stdout = TRUE)\n    .libPaths(\"/root/.local/share/mamba/envs/r_env/lib/R/library\")\n}\n\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(dbplyr)\n\ncat(\"Loading DuckDB with Snowflake extension...\\n\")\n\n# Connect to DuckDB (in-memory for speed, or file for persistence)\nduckdb_con <- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\n\n# Load the Snowflake extension\ntryCatch({\n    dbExecute(duckdb_con, \"INSTALL snowflake FROM community\")\n    dbExecute(duckdb_con, \"LOAD snowflake\")\n    cat(\"\u2713 Snowflake extension loaded\\n\")\n}, error = function(e) {\n    cat(\"\u2717 Error loading extension:\", conditionMessage(e), \"\\n\")\n})\n\ncat(\"DuckDB ready. Configure Snowflake secret in next cell.\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-snowflake-secret",
      "metadata": {
        "language": "python",
        "name": "PY__duckdb_auth_prep",
        "title": "PY__duckdb_auth_prep"
      },
      "outputs": [],
      "source": "# PY__duckdb_auth_prep\n# Prepare DuckDB authentication for Snowflake extension\n#\n# Environment-specific behavior:\n# - Local IDE: Uses key-pair from notebook_config.yaml\n# - SPCS/Remote Dev: Mount secret via service spec, then read file\n# - Workspace Notebooks: Skip direct connection, use Python Bridge (7.3.1)\n\nimport rpy2.robjects as ro\n\nif ENV_TYPE == 'workspace':\n    print(\"=\" * 60)\n    print(\"WORKSPACE NOTEBOOK - Authentication Setup\")\n    print(\"=\" * 60)\n    print(\"\\nDirect DuckDB\u2192Snowflake requires private key access.\")\n    print(\"Workspace Notebooks have limited options for secure key access:\")\n    print(\"  - st.secrets: Not supported (Streamlit not available)\")\n    print(\"  - Service spec: Not controllable from notebooks\")\n    print(\"  - UDF+EAI: Requires elevated privileges\")\n    print(\"\\n\u2192 RECOMMENDED: Use Python Bridge (Section 7.3.1)\")\n    print(\"  This uses the existing Snowpark session - no key needed.\")\n    ro.globalenv['duckdb_auth'] = ro.ListVector({'method': 'none'})\n    \nelif ENV_TYPE == 'spcs' or ENV_TYPE == 'remote_dev':\n    # SPCS/Remote Dev: Check for mounted secret\n    secret_path = '/secrets/snowflake_private_key'\n    if os.path.exists(secret_path):\n        with open(secret_path, 'r') as f:\n            private_key = f.read()\n        ro.globalenv['duckdb_auth'] = ro.ListVector({\n            'method': 'keypair',\n            'account': ENV_CONFIG.get('account', ''),\n            'user': ENV_CONFIG.get('user', ''),\n            'database': ENV_CONFIG.get('database', ''),\n            'warehouse': ENV_CONFIG.get('warehouse', ''),\n            'private_key': private_key\n        })\n        print(\"\u2713 Key-pair auth configured from mounted secret\")\n    else:\n        print(f\"\u26a0 No secret mounted at {secret_path}\")\n        print(\"  Add to service spec: snowflakeSecret with directoryPath: /secrets\")\n        ro.globalenv['duckdb_auth'] = ro.ListVector({'method': 'none'})\n        \nelse:\n    # Local IDE: Use key-pair from config file\n    key_path = os.path.expanduser(ENV_CONFIG.get('private_key_path', ''))\n    if os.path.exists(key_path):\n        with open(key_path, 'r') as f:\n            private_key = f.read()\n        \n        ro.globalenv['duckdb_auth'] = ro.ListVector({\n            'method': 'keypair',\n            'account': ENV_CONFIG.get('account', ''),\n            'user': ENV_CONFIG.get('user', ''),\n            'database': ENV_CONFIG.get('database', ''),\n            'warehouse': ENV_CONFIG.get('warehouse', ''),\n            'private_key': private_key\n        })\n        print(\"\u2713 Key-pair auth configured for DuckDB Snowflake extension\")\n        print(f\"  Account: {ENV_CONFIG.get('account', '')}\")\n        print(f\"  User: {ENV_CONFIG.get('user', '')}\")\n    else:\n        print(\"\u26a0 Private key not found at:\", key_path)\n        print(\"  Configure 'private_key_path' in notebook_config.yaml\")\n        ro.globalenv['duckdb_auth'] = ro.ListVector({'method': 'none'})"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-create-secret-r",
      "metadata": {
        "language": "python",
        "name": "R__duckdb_secret",
        "title": "R__duckdb_secret"
      },
      "outputs": [],
      "source": "%%R\n# Create DuckDB Snowflake secret (Local IDE only)\n\nif (duckdb_auth$method == \"none\") {\n    cat(\"DuckDB direct Snowflake connection not configured.\\n\")\n    cat(\"\\nFor Workspace Notebooks: Use Python Bridge (Section 7.3.1)\\n\")\n    cat(\"For Local IDE: Configure key-pair auth in previous cell\\n\")\n} else if (duckdb_auth$method == \"keypair\") {\n    cat(\"Creating Snowflake secret with key-pair auth...\\n\")\n    cat(\"  Account:\", duckdb_auth$account, \"\\n\")\n    cat(\"  User:\", duckdb_auth$user, \"\\n\")\n    \n    secret_sql <- sprintf(\"\nCREATE OR REPLACE SECRET snowflake_secret (\n    TYPE snowflake,\n    ACCOUNT '%s',\n    USER '%s',\n    DATABASE '%s',\n    WAREHOUSE '%s',\n    AUTH_TYPE 'key_pair',\n    PRIVATE_KEY '%s'\n)\",\n        duckdb_auth$account,\n        duckdb_auth$user,\n        duckdb_auth$database,\n        duckdb_auth$warehouse,\n        gsub(\"'\", \"''\", duckdb_auth$private_key)\n    )\n    \n    tryCatch({\n        dbExecute(duckdb_con, secret_sql)\n        cat(\"\u2713 Key-pair secret created successfully\\n\")\n        cat(\"\\nRun next cell to attach Snowflake database\\n\")\n    }, error = function(e) {\n        cat(\"\u2717 Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-attach-snowflake",
      "metadata": {
        "language": "python",
        "name": "R__duckdb_attach",
        "title": "R__duckdb_attach"
      },
      "outputs": [],
      "source": "%%R\n# Attach Snowflake as a catalog in DuckDB (Local IDE only)\n# For Workspace Notebooks, skip to Section 7.3.1 Python Bridge\n\nif (!exists(\"duckdb_auth\") || is.null(duckdb_auth) || duckdb_auth$method == \"none\") {\n    cat(\"Skipping - use Python Bridge (Section 7.3.1) for Workspace Notebooks\\n\")\n} else {\n    cat(\"Attaching Snowflake database...\\n\")\n    \n    tryCatch({\n        dbExecute(duckdb_con, \"ATTACH '' AS sf (TYPE snowflake, SECRET snowflake_secret, READ_ONLY)\")\n        cat(\"\u2713 Snowflake attached as 'sf' catalog\\n\\n\")\n        \n        # List schemas\n        cat(\"Available schemas:\\n\")\n        schemas <- dbGetQuery(duckdb_con, \n            \"SELECT schema_name FROM sf.information_schema.schemata ORDER BY schema_name LIMIT 10\")\n        rprint(schemas)\n        \n    }, error = function(e) {\n        cat(\"\u2717 Error:\", conditionMessage(e), \"\\n\")\n        cat(\"\\nTroubleshooting:\\n\")\n        cat(\"  - Verify account name format (e.g., 'xy12345' not full URL)\\n\")\n        cat(\"  - Check private key is valid PKCS8 format\\n\")\n        cat(\"  - Ensure public key is registered in Snowflake\\n\")\n    })\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-workspace-alternative",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 5.3.1 Python Bridge (Recommended for Workspace Notebooks)\n\nThis approach queries Snowflake via Python Snowpark and transfers data to R/DuckDB for local analysis.\n\n**Why use this?**\n- Works reliably in both Workspace Notebooks and Local IDEs\n- Uses the existing Snowpark session authentication\n- No additional credential setup needed\n- Ideal for dplyr/dbplyr workflows on fetched data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-python-bridge",
      "metadata": {
        "language": "python",
        "name": "PY__python_bridge",
        "title": "PY__python_bridge"
      },
      "outputs": [],
      "source": "# Python Bridge: Query Snowflake, analyze with R/DuckDB\n# This example uses SNOWFLAKE_SAMPLE_DATA (available to all Snowflake accounts)\n\nif session is None:\n    print(\"No session available. Run Section 3.1 first!\")\nelse:\n    import rpy2.robjects as ro\n    from rpy2.robjects import pandas2ri\n    from rpy2.robjects.conversion import localconverter\n    \n    # Query TPCH sample data\n    # Note: Uses SNOWFLAKE_SAMPLE_DATA which is available to all accounts\n    query = \"\"\"\n        SELECT O_ORDERKEY, O_CUSTKEY, O_ORDERSTATUS, \n               O_TOTALPRICE::FLOAT as O_TOTALPRICE, \n               O_ORDERDATE\n        FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n        WHERE O_ORDERDATE >= '1995-01-01'\n        LIMIT 10000\n    \"\"\"\n    \n    print(\"Querying SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS...\")\n    orders_df = session.sql(query).to_pandas()\n    \n    # Transfer to R environment\n    with localconverter(ro.default_converter + pandas2ri.converter):\n        r_orders = ro.conversion.py2rpy(orders_df)\n        ro.globalenv['sf_orders'] = r_orders\n    \n    print(f\"\u2713 Transferred {len(orders_df):,} rows to R as 'sf_orders'\")\n    print(f\"  Columns: {', '.join(orders_df.columns)}\")\n    print(\"\\nUse %%R cells to analyze with dplyr:\")\n    print(\"  library(dplyr)\")\n    print(\"  sf_orders %>% group_by(O_ORDERSTATUS) %>% summarize(n = n())\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-bridge-analysis",
      "metadata": {
        "language": "python",
        "name": "R__duckdb_analyze",
        "title": "R__duckdb_analyze"
      },
      "outputs": [],
      "source": "%%R\n# Analyze data transferred via Python bridge\n# This cell works in Workspace Notebooks without DuckDB\n\nif (exists(\"sf_orders\")) {\n    library(dplyr)\n    \n    # Convert date column if needed (pandas dates may come through differently)\n    sf_orders <- sf_orders %>%\n        mutate(\n            O_ORDERDATE = as.Date(O_ORDERDATE),\n            order_year = as.character(format(O_ORDERDATE, \"%Y\"))\n        )\n    \n    result <- sf_orders %>%\n        group_by(order_year, O_ORDERSTATUS) %>%\n        summarise(\n            orders = n(),\n            total_value = sum(O_TOTALPRICE, na.rm = TRUE),\n            .groups = \"drop\"\n        ) %>%\n        arrange(order_year, desc(orders))\n    \n    cat(\"Order analysis using dplyr (via Python bridge):\\n\")\n    rprint(result)\n} else {\n    cat(\"Note: sf_orders not found. Run the Python bridge cell above first.\\n\")\n    cat(\"Or use the DuckDB approach if in local IDE.\\n\")\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-query-examples",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.4 Query Snowflake with dplyr\n\nThe recommended pattern for dplyr workflows:\n1. **Direct SQL** for fetching data from Snowflake\n2. **Cache locally** in DuckDB for iterative analysis\n3. **Use dplyr** on local cached tables\n\n**Important**: Use 2-part table names (`sf.schema.table`) when database is set in the secret."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-direct-sql",
      "metadata": {
        "language": "python",
        "name": "R__dplyr_sql",
        "title": "R__dplyr_sql"
      },
      "outputs": [],
      "source": "%%R\n# Pattern 1: Direct SQL Query to Snowflake\n# Best for: simple aggregations, data exploration\n# NOTE: Requires DuckDB ATTACH (Local IDE only)\n\n# Check if sf catalog exists (only available in Local IDE with key-pair auth)\nsf_exists <- tryCatch({\n    dbGetQuery(duckdb_con, \"SELECT 1 FROM duckdb_databases() WHERE database_name = 'sf'\")\n    TRUE\n}, error = function(e) FALSE)\n\nif (!sf_exists || nrow(dbGetQuery(duckdb_con, \"SELECT 1 FROM duckdb_databases() WHERE database_name = 'sf'\")) == 0) {\n    cat(\"Note: 'sf' catalog not attached (Workspace Notebook mode)\\n\")\n    cat(\"Use the Python Bridge approach in Section 7.3.1 instead.\\n\")\n} else {\n    cat(\"Direct SQL query to Snowflake...\\n\\n\")\n    \n    tryCatch({\n        customers <- dbGetQuery(duckdb_con, \"\n            SELECT C_MKTSEGMENT, COUNT(*) as customers, ROUND(AVG(C_ACCTBAL), 2) as avg_balance\n            FROM sf.tpch_sf1.customer\n            GROUP BY C_MKTSEGMENT\n            ORDER BY customers DESC\n        \")\n        \n        cat(\"Customer analysis by market segment:\\n\")\n        rprint(customers)\n        \n    }, error = function(e) {\n        cat(\"Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-dplyr-query",
      "metadata": {
        "language": "python",
        "name": "R__dplyr_cache",
        "title": "R__dplyr_cache"
      },
      "outputs": [],
      "source": "%%R\n# Pattern 2: Cache locally, then use dplyr\n# Best for: complex analysis, multiple operations on same data\n# NOTE: Requires DuckDB ATTACH (Local IDE only)\n\nsf_exists <- tryCatch({\n    nrow(dbGetQuery(duckdb_con, \"SELECT 1 FROM duckdb_databases() WHERE database_name = 'sf'\")) > 0\n}, error = function(e) FALSE)\n\nif (!sf_exists) {\n    cat(\"Note: 'sf' catalog not attached (Workspace Notebook mode)\\n\")\n    cat(\"Use the Python Bridge in Section 7.3.1 - data is cached in sf_orders.\\n\")\n} else {\n    cat(\"Caching Snowflake data locally...\\n\\n\")\n    \n    tryCatch({\n        # Pull data into local DuckDB table\n        dbExecute(duckdb_con, \"\n            CREATE OR REPLACE TABLE local_orders AS\n            SELECT O_ORDERKEY, O_CUSTKEY, O_ORDERSTATUS, \n                   O_TOTALPRICE, O_ORDERDATE, O_ORDERPRIORITY\n            FROM sf.tpch_sf1.orders\n            LIMIT 50000\n        \")\n        \n        # Use dplyr on local table\n        local_orders <- tbl(duckdb_con, \"local_orders\")\n        \n        result <- local_orders %>%\n            group_by(O_ORDERSTATUS, O_ORDERPRIORITY) %>%\n            summarise(\n                count = n(),\n                avg_price = mean(O_TOTALPRICE, na.rm = TRUE),\n                .groups = \"drop\"\n            ) %>%\n            collect()\n        \n        cat(\"Order analysis (cached locally):\\n\")\n        rprint(result)\n        \n    }, error = function(e) {\n        cat(\"Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-cache-pattern",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.5 Advanced Patterns\n\nAdditional patterns for DuckDB + Snowflake workflows."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-cache-example",
      "metadata": {
        "language": "python",
        "name": "R__dplyr_join",
        "title": "R__dplyr_join"
      },
      "outputs": [],
      "source": "%%R\n# Pattern 3: Join local cached tables\n# Best for: multi-table analysis after caching\n# NOTE: Requires DuckDB ATTACH (Local IDE only)\n\nsf_exists <- tryCatch({\n    nrow(dbGetQuery(duckdb_con, \"SELECT 1 FROM duckdb_databases() WHERE database_name = 'sf'\")) > 0\n}, error = function(e) FALSE)\n\nif (!sf_exists) {\n    cat(\"Note: 'sf' catalog not attached (Workspace Notebook mode)\\n\")\n    cat(\"For multi-table joins, use Python Bridge to fetch each table.\\n\")\n} else {\n    cat(\"Caching and joining tables...\\n\\n\")\n    \n    tryCatch({\n        # Cache customers\n        dbExecute(duckdb_con, \"\n            CREATE OR REPLACE TABLE local_customers AS\n            SELECT C_CUSTKEY, C_NAME, C_MKTSEGMENT, C_NATIONKEY\n            FROM sf.tpch_sf1.customer\n        \")\n        \n        # Join with previous local_orders\n        result <- dbGetQuery(duckdb_con, \"\n            SELECT c.C_MKTSEGMENT, \n                   COUNT(DISTINCT o.O_ORDERKEY) as orders,\n                   ROUND(SUM(o.O_TOTALPRICE), 2) as total_sales\n            FROM local_orders o\n            JOIN local_customers c ON o.O_CUSTKEY = c.C_CUSTKEY\n            GROUP BY c.C_MKTSEGMENT\n            ORDER BY total_sales DESC\n        \")\n        \n        cat(\"Sales by market segment (joined tables):\\n\")\n        rprint(result)\n        \n    }, error = function(e) {\n        cat(\"Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-window-functions",
      "metadata": {
        "language": "python",
        "name": "R__dplyr_window",
        "title": "R__dplyr_window"
      },
      "outputs": [],
      "source": "%%R\n# Pattern 4: Window functions with dplyr\n# Best for: rankings, running totals, lag/lead analysis\n# NOTE: Requires local cached data from previous cells\n\nif (!dbExistsTable(duckdb_con, \"local_orders\")) {\n    cat(\"Note: local_orders table not found\\n\")\n    cat(\"Run Pattern 2 first to cache data, or use Python Bridge.\\n\")\n} else {\n    cat(\"Window function analysis...\\n\\n\")\n    \n    tryCatch({\n        local_orders <- tbl(duckdb_con, \"local_orders\")\n        \n        result <- local_orders %>%\n            group_by(O_ORDERPRIORITY) %>%\n            mutate(\n                priority_rank = row_number(),\n                running_total = cumsum(O_TOTALPRICE)\n            ) %>%\n            filter(priority_rank <= 3) %>%\n            select(O_ORDERPRIORITY, O_ORDERKEY, O_TOTALPRICE, \n                   priority_rank, running_total) %>%\n            arrange(O_ORDERPRIORITY, priority_rank) %>%\n            collect()\n        \n        cat(\"Top 3 orders per priority (window functions):\\n\")\n        rprint(head(result, 15))\n        \n    }, error = function(e) {\n        cat(\"Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-cleanup",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.6 Cleanup\n\nClose the DuckDB connection when done."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-cleanup-cell",
      "metadata": {
        "language": "python",
        "name": "R__duckdb_cleanup",
        "title": "R__duckdb_cleanup"
      },
      "outputs": [],
      "source": "%%R\n# Cleanup: Disconnect from DuckDB\n# Uncomment to close connection\n\n# dbDisconnect(duckdb_con)\n# cat(\"DuckDB connection closed\\n\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 6: Iceberg Integration via Horizon Catalog\n\nThis section demonstrates accessing **Snowflake-managed Iceberg tables** from R/DuckDB using the Horizon Catalog REST API.\n\n### Architecture\n\n```\nR (DuckDB)  \u2192  Horizon Catalog REST API  \u2192  Snowflake-managed Iceberg Table\n                     \u2193\n            Vended S3 credentials \u2192 Direct data file access\n```\n\n### Key Features (GA)\n\n- **External engine reads via Horizon are GA** (Build '26)\n- **Snowflake-managed internal storage** - No external cloud storage required!\n- **Warehouse-free reads** - Cloud Services billing only\n- **Vended credentials** - Horizon provides temporary S3-style credentials\n\n### Approach\n\n1. **Create External Volume** with `STORAGE_PROVIDER = 'SNOWFLAKE'` (internal storage)\n2. **Create Iceberg table** using CTAS from sample data\n3. **Generate PAT/JWT** for Horizon API authentication\n4. **Query from DuckDB** via Iceberg REST catalog",
      "id": "03aca72e-6b2d-4426-8c31-aedc0993cfb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.1 Create Iceberg Table on Internal Storage (One-Time Setup)\n\nCreate a Snowflake-managed Iceberg table using **internal Snowflake storage** (no external S3/Azure/GCS required).\n\n**Steps:**\n1. Create an External Volume with `STORAGE_PROVIDER = 'SNOWFLAKE'`\n2. Create an Iceberg table using CTAS from TPCH sample data\n\n**Required Privileges:**\n- `CREATE EXTERNAL VOLUME` on account (or use existing volume)\n- `CREATE ICEBERG TABLE` on schema\n\n**Note:** Internal storage for Iceberg is GA in most regions. If you encounter errors, you may need feature enablement.",
      "id": "dd6d1586-c995-4256-8c32-9476c19fde2a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "PY__create_iceberg_table",
        "title": "PY__create_iceberg_table"
      },
      "source": "# PY__create_iceberg_table\n# Create Snowflake-managed Iceberg table on internal storage\n# Uses ICEBERG_CONFIG from notebook_config.yaml\n\nif session is None:\n    print(\"No session available. Run Section 3.1 first!\")\nelse:\n    # Configuration from YAML\n    iceberg_table = ICEBERG_CONFIG.get('test_table_name', 'NATION_ICEBERG')\n    external_vol = ICEBERG_CONFIG.get('external_volume', 'iceberg_internal_vol')\n    target_db = DEFAULTS.get('database', 'SIMON')  # Use user's DB, not SNOWFLAKE_SAMPLE_DATA\n    target_schema = DEFAULTS.get('schema', 'PUBLIC')\n    \n    # Use user's own database for Iceberg table (need write access)\n    if target_db == 'SNOWFLAKE_SAMPLE_DATA':\n        target_db = CONN_CONFIG.get('user', 'SIMON')  # Fall back to username as DB\n        target_schema = 'PUBLIC'\n    \n    print(\"=\" * 60)\n    print(\"STEP 1: Create External Volume with Internal Storage\")\n    print(\"=\" * 60)\n    \n    # Create external volume with Snowflake-managed internal storage\n    create_vol_sql = f\"\"\"\n    CREATE EXTERNAL VOLUME IF NOT EXISTS {external_vol}\n        STORAGE_LOCATIONS = (\n            (NAME = 'sf_internal', STORAGE_PROVIDER = 'SNOWFLAKE')\n        )\n        COMMENT = 'Internal storage for Iceberg demo tables'\n    \"\"\"\n    \n    try:\n        session.sql(create_vol_sql).collect()\n        print(f\"\u2713 External volume '{external_vol}' ready\")\n    except Exception as e:\n        if 'already exists' in str(e).lower():\n            print(f\"\u2713 External volume '{external_vol}' already exists\")\n        else:\n            print(f\"\u26a0 Volume creation failed: {e}\")\n            print(\"  You may need CREATE EXTERNAL VOLUME privilege or feature enablement\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"STEP 2: Create Iceberg Table from TPCH Sample Data\")\n    print(\"=\" * 60)\n    \n    full_table_name = f\"{target_db}.{target_schema}.{iceberg_table}\"\n    print(f\"Creating: {full_table_name}\")\n    print(f\"External volume: {external_vol}\")\n    \n    create_table_sql = f\"\"\"\n    CREATE OR REPLACE ICEBERG TABLE {full_table_name}\n        EXTERNAL_VOLUME = '{external_vol}'\n        CATALOG = 'SNOWFLAKE'\n        BASE_LOCATION = '{iceberg_table.lower()}/'\n        AS SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION\n    \"\"\"\n    \n    try:\n        session.sql(create_table_sql).collect()\n        print(f\"\u2713 Iceberg table '{iceberg_table}' created successfully!\")\n        \n        # Verify\n        count = session.sql(f\"SELECT COUNT(*) as cnt FROM {full_table_name}\").collect()[0]['CNT']\n        print(f\"\u2713 Table contains {count} rows\")\n        \n        # Store for later cells\n        ICEBERG_FULL_TABLE = full_table_name\n        print(f\"\\n\u2713 Ready for Horizon Catalog queries in Section 8.2+\")\n        \n    except Exception as e:\n        print(f\"\u2717 Table creation failed: {e}\")\n        print(\"\\nPossible issues:\")\n        print(\"  - Need CREATE ICEBERG TABLE privilege\")\n        print(\"  - Internal storage may require feature enablement\")\n        print(\"  - Check if external volume supports internal storage\")",
      "outputs": [],
      "execution_count": null,
      "id": "b8faf8af-39e9-4e59-88ce-8b0f6e893f5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.2 Horizon Catalog Authentication\n\nGet an access token for the Horizon Catalog REST API.\n\n**Authentication Options:**\n1. **PAT (Personal Access Token)** - Easiest, already set up in Section 3\n2. **JWT (Key-pair)** - More secure, requires private key\n\nFor Workspace Notebooks, we'll use a PAT (if available) or fall back to JWT.",
      "id": "864680f1-fc49-4ec5-8ba1-63b8e83551ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "PY__horizon_auth_helper",
        "title": "PY__horizon_auth_helper"
      },
      "outputs": [],
      "source": "# PY__horizon_auth\n# Get access token for Horizon Catalog\n# In Workspace: Try to use session token\n# In Local IDE: Use JWT from private key\n\nimport os\n\nhorizon_access_token = None\n\nif ENV_TYPE == 'workspace':\n    print(\"=\" * 60)\n    print(\"WORKSPACE NOTEBOOK - Horizon Authentication\")\n    print(\"=\" * 60)\n    \n    # Try to extract session token from Snowpark session\n    try:\n        # Method 1: Direct token access from REST client\n        if hasattr(session, '_conn') and hasattr(session._conn, '_rest'):\n            rest = session._conn._rest\n            if hasattr(rest, '_token'):\n                horizon_access_token = rest._token\n                print(\"\u2713 Extracted session token from Snowpark session\")\n            elif hasattr(rest, 'token'):\n                horizon_access_token = rest.token\n                print(\"\u2713 Extracted session token (alt method)\")\n        \n        # Method 2: From session file (SPCS)\n        if not horizon_access_token:\n            token_file = '/snowflake/session/token'\n            if os.path.exists(token_file):\n                with open(token_file, 'r') as f:\n                    horizon_access_token = f.read().strip()\n                print(f\"\u2713 Read token from {token_file}\")\n        \n        # Method 3: Environment variable\n        if not horizon_access_token:\n            horizon_access_token = os.environ.get('SNOWFLAKE_TOKEN') or os.environ.get('SF_TOKEN')\n            if horizon_access_token:\n                print(\"\u2713 Got token from environment variable\")\n                \n    except Exception as e:\n        print(f\"\u26a0 Token extraction failed: {e}\")\n    \n    if not horizon_access_token:\n        print(\"\\n\u26a0 Could not extract session token automatically.\")\n        print(\"\\nOptions:\")\n        print(\"  1. Create a PAT (Section 3.4), then set:\")\n        print(\"     horizon_access_token = 'your_pat_token'\")\n        print(\"  2. Use Python Bridge (Section 7.3.1) - no token needed\")\n        \nelif ENV_TYPE == 'local':\n    # Local IDE: Generate JWT\n    import jwt\n    import time\n    import hashlib\n    import base64\n    import requests\n    from cryptography.hazmat.primitives import serialization\n    \n    key_path = os.path.expanduser(ENV_CONFIG.get('private_key_path', ''))\n    \n    if os.path.exists(key_path):\n        print(\"Generating JWT for Horizon authentication...\")\n        \n        with open(key_path, 'rb') as f:\n            private_key = serialization.load_pem_private_key(f.read(), password=None)\n        \n        public_key = private_key.public_key()\n        public_key_bytes = public_key.public_bytes(\n            encoding=serialization.Encoding.DER,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        fingerprint = hashlib.sha256(public_key_bytes).digest()\n        fingerprint_b64 = base64.b64encode(fingerprint).decode()\n        \n        account = ENV_CONFIG.get('account', '').upper()\n        user = ENV_CONFIG.get('user', '').upper()\n        \n        now = int(time.time())\n        payload = {\n            'iss': f'{account}.{user}.SHA256:{fingerprint_b64}',\n            'sub': f'{account}.{user}',\n            'iat': now,\n            'exp': now + 3600\n        }\n        \n        token = jwt.encode(payload, private_key, algorithm='RS256')\n        \n        # Exchange for access token\n        token_url = f\"https://{account}.snowflakecomputing.com/oauth/token\"\n        \n        try:\n            resp = requests.post(token_url, data={\n                'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer',\n                'assertion': token,\n                'scope': 'session:role:SYSADMIN'\n            })\n            \n            if resp.status_code == 200:\n                horizon_access_token = resp.json().get('access_token')\n                print(f\"\u2713 Access token obtained\")\n            else:\n                print(f\"\u26a0 Token exchange failed: {resp.status_code}\")\n        except Exception as e:\n            print(f\"\u26a0 Error: {e}\")\n    else:\n        print(f\"\u26a0 Private key not found: {key_path}\")\n\nif horizon_access_token:\n    print(f\"\\n\u2713 Token ready ({len(horizon_access_token)} chars)\")\nelse:\n    print(\"\\n\u26a0 No token available\")",
      "id": "6db07766-776c-4435-a857-40aa62aecfda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.3 Query Horizon Catalog Metadata\n\nUse the Horizon REST API to list namespaces and find our Iceberg table.\n\n**Endpoint**: `https://<account>.snowflakecomputing.com/polaris/api/catalog/v1/`",
      "id": "45cdca4c-2c80-439d-bcd7-f8a7c5768b3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "PY__query_horizon_api",
        "title": "PY__query_horizon_api"
      },
      "outputs": [],
      "source": "# PY__query_horizon_api\n# Query Horizon Catalog REST API\nimport requests\nimport json as json_module\n\ndef query_horizon(endpoint, access_token, account):\n    \"\"\"Query the Horizon Catalog REST API.\"\"\"\n    base_url = f\"https://{account}.snowflakecomputing.com/polaris/api/catalog/v1\"\n    \n    response = requests.get(\n        f\"{base_url}/{endpoint}\",\n        headers={\n            'Authorization': f'Bearer {access_token}',\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        }\n    )\n    \n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error {response.status_code}: {response.text[:200]}\")\n        return None\n\n# Check if we have a token\nif 'horizon_access_token' not in dir() or horizon_access_token is None:\n    print(\"\u26a0 No access token. Run Section 8.2 first or set horizon_access_token manually.\")\nelse:\n    account = ENV_CONFIG.get('account', '').upper()\n    \n    print(\"=\" * 60)\n    print(\"Horizon Catalog API Queries\")\n    print(\"=\" * 60)\n    \n    # List namespaces (databases)\n    print(\"\\n1. Listing namespaces (databases)...\")\n    namespaces = query_horizon(\"namespaces\", horizon_access_token, account)\n    if namespaces:\n        print(f\"   Found {len(namespaces.get('namespaces', []))} namespaces\")\n        for ns in namespaces.get('namespaces', [])[:5]:\n            print(f\"   - {ns}\")\n        if len(namespaces.get('namespaces', [])) > 5:\n            print(f\"   ... and {len(namespaces.get('namespaces', [])) - 5} more\")\n    \n    # List tables in our target database\n    target_db = DEFAULTS.get('database', 'SIMON')\n    if target_db == 'SNOWFLAKE_SAMPLE_DATA':\n        target_db = CONN_CONFIG.get('user', 'SIMON')\n    \n    print(f\"\\n2. Listing tables in {target_db}...\")\n    tables = query_horizon(f\"namespaces/{target_db}/tables\", horizon_access_token, account)\n    if tables:\n        print(f\"   Found {len(tables.get('identifiers', []))} tables\")\n        for t in tables.get('identifiers', []):\n            print(f\"   - {t.get('namespace', [''])[0]}.{t.get('name', '')}\")\n    \n    # Get our Iceberg table metadata\n    iceberg_table = ICEBERG_CONFIG.get('test_table_name', 'NATION_ICEBERG')\n    target_schema = DEFAULTS.get('schema', 'PUBLIC')\n    \n    print(f\"\\n3. Getting metadata for {target_db}.{target_schema}.{iceberg_table}...\")\n    table_meta = query_horizon(\n        f\"namespaces/{target_db}.{target_schema}/tables/{iceberg_table}\", \n        horizon_access_token, account\n    )\n    if table_meta:\n        print(f\"   \u2713 Table found!\")\n        print(f\"   Format: {table_meta.get('metadata', {}).get('format-version', 'unknown')}\")\n        print(f\"   Location: {table_meta.get('metadata-location', 'unknown')[:50]}...\")",
      "id": "0e6d4d3a-c73e-4d82-b1e2-e6de4cc02e71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.4 Query Iceberg Table from DuckDB\n\nUse DuckDB's Iceberg extension to connect to Horizon Catalog and query the table we created.\n\n**Architecture**:\n```\nDuckDB (R)  \u2192  Horizon REST API  \u2192  Vended S3 credentials  \u2192  Data files\n```\n\n**Requirements**:\n- Access token from Section 8.2\n- DuckDB iceberg extension (installed below)",
      "id": "2f208a8d-d7be-4e06-bdf9-37a7eeb1044b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iceberg-duckdb-setup-py",
      "metadata": {
        "language": "python",
        "name": "PY__iceberg_duckdb_setup",
        "title": "PY__iceberg_duckdb_setup"
      },
      "outputs": [],
      "source": "# PY__iceberg_duckdb_setup\n# Prepare config for R DuckDB Iceberg cell\nimport rpy2.robjects as ro\n\n# Get config values\naccount = ENV_CONFIG.get('account', '').upper()\ntarget_db = DEFAULTS.get('database', 'SIMON')\nif target_db == 'SNOWFLAKE_SAMPLE_DATA':\n    target_db = CONN_CONFIG.get('user', 'SIMON')\ntarget_schema = DEFAULTS.get('schema', 'PUBLIC')\nif target_schema == 'TPCH_SF1':\n    target_schema = 'PUBLIC'\niceberg_table = ICEBERG_CONFIG.get('test_table_name', 'NATION_ICEBERG')\n\n# Get access token\ntoken = horizon_access_token if 'horizon_access_token' in dir() and horizon_access_token else ''\n\n# Create R ListVector with config\nro.globalenv['iceberg_r_config'] = ro.ListVector({\n    'access_token': token,\n    'account': account,\n    'database': target_db,\n    'schema': target_schema,\n    'table_name': iceberg_table\n})\n\nprint(f\"\u2713 Config prepared for R:\")\nprint(f\"  Account: {account}\")\nprint(f\"  Database: {target_db}\")\nprint(f\"  Schema: {target_schema}\")\nprint(f\"  Table: {iceberg_table}\")\nprint(f\"  Token: {'set' if token else 'NOT SET - run Section 8.2'}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "R__iceberg_duckdb",
        "title": "R__iceberg_duckdb"
      },
      "outputs": [],
      "source": "%%R\n# DuckDB Iceberg Integration\n# Query Snowflake-managed Iceberg table via Horizon Catalog\n\nlibrary(DBI)\nlibrary(duckdb)\n\n# Connect to DuckDB\ncat(\"Setting up DuckDB with Iceberg extension...\\n\\n\")\niceberg_con <- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\n\n# Try to install and load extensions\nextensions_ok <- TRUE\n\ntryCatch({\n    # Install required extensions\n    dbExecute(iceberg_con, \"INSTALL httpfs\")\n    dbExecute(iceberg_con, \"INSTALL avro\")  # Required by iceberg\n    dbExecute(iceberg_con, \"INSTALL iceberg\")\n    \n    # Load them\n    dbExecute(iceberg_con, \"LOAD httpfs\")\n    dbExecute(iceberg_con, \"LOAD iceberg\")\n    cat(\"\u2713 Extensions loaded\\n\")\n}, error = function(e) {\n    cat(\"\u26a0 Extension installation failed:\\n\")\n    cat(\"  \", conditionMessage(e), \"\\n\")\n    cat(\"\\nThe DuckDB iceberg extension requires 'avro' which may not be\\n\")\n    cat(\"available in this environment.\\n\")\n    cat(\"\\n\u2192 ALTERNATIVE: Use Python DuckDB (cell below) or Python Bridge (Section 7.3.1)\\n\")\n    extensions_ok <<- FALSE\n})\n\nif (extensions_ok && exists(\"iceberg_r_config\")) {\n    # Extract config\n    access_token <- iceberg_r_config$access_token\n    account <- iceberg_r_config$account\n    target_db <- iceberg_r_config$database\n    target_schema <- iceberg_r_config$schema\n    iceberg_table <- iceberg_r_config$table_name\n    \n    cat(\"\\nConfiguration:\\n\")\n    cat(\"  Account:\", account, \"\\n\")\n    cat(\"  Database:\", target_db, \"\\n\")\n    cat(\"  Schema:\", target_schema, \"\\n\")\n    cat(\"  Table:\", iceberg_table, \"\\n\")\n    \n    if (!is.null(access_token) && access_token != \"\") {\n        # Create Iceberg secret\n        cat(\"\\nCreating Iceberg secret...\\n\")\n        secret_sql <- sprintf(\"\n            CREATE OR REPLACE SECRET horizon_secret (\n                TYPE iceberg,\n                TOKEN '%s'\n            )\n        \", access_token)\n        \n        tryCatch({\n            dbExecute(iceberg_con, secret_sql)\n            cat(\"\u2713 Secret created\\n\")\n            \n            # Attach Horizon Catalog\n            cat(\"\\nAttaching Horizon Catalog...\\n\")\n            endpoint <- sprintf(\"https://%s.snowflakecomputing.com/polaris/api/catalog\", account)\n            \n            attach_sql <- sprintf(\"\n                ATTACH '%s' AS horizon (\n                    TYPE iceberg,\n                    ENDPOINT '%s',\n                    SECRET horizon_secret\n                )\n            \", target_db, endpoint)\n            \n            dbExecute(iceberg_con, attach_sql)\n            cat(\"\u2713 Horizon Catalog attached\\n\")\n            \n            # Query the table\n            cat(\"\\nQuerying\", iceberg_table, \"...\\n\")\n            query_sql <- sprintf(\n                \"SELECT * FROM horizon.\\\"%s\\\".%s LIMIT 10\",\n                target_schema, iceberg_table\n            )\n            \n            result <- dbGetQuery(iceberg_con, query_sql)\n            cat(\"\\n\u2713 Query successful! Returned\", nrow(result), \"rows:\\n\\n\")\n            print(result)\n            \n        }, error = function(e) {\n            cat(\"\\n\u26a0 Error:\", conditionMessage(e), \"\\n\")\n        })\n    } else {\n        cat(\"\\n\u26a0 No access token. Run Section 8.2 first.\\n\")\n    }\n} else if (!exists(\"iceberg_r_config\")) {\n    cat(\"\\n\u26a0 Config not found. Run PY__iceberg_duckdb_setup first.\\n\")\n}\n\ncat(\"\\nDone.\\n\")",
      "id": "f7c10a85-51bb-488e-812f-2ee00e84ea52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iceberg-duckdb-python",
      "metadata": {
        "language": "python",
        "name": "PY__iceberg_duckdb_python",
        "title": "PY__iceberg_duckdb_python"
      },
      "outputs": [],
      "source": "# PY__iceberg_duckdb_python\n# Alternative: Use Python DuckDB for Iceberg\n\n# Install duckdb if not available\ntry:\n    import duckdb\nexcept ImportError:\n    print(\"Installing duckdb...\")\n    import subprocess\n    subprocess.check_call(['pip', 'install', 'duckdb', '-q'])\n    import duckdb\n    print(\"\u2713 duckdb installed\")\n\nif 'horizon_access_token' not in dir() or not horizon_access_token:\n    print(\"\u26a0 No access token. Run Section 8.2 first.\")\nelse:\n    # Get config\n    account = ENV_CONFIG.get('account', '').upper()\n    target_db = DEFAULTS.get('database', CONN_CONFIG.get('user', 'SIMON'))\n    if target_db == 'SNOWFLAKE_SAMPLE_DATA':\n        target_db = CONN_CONFIG.get('user', 'SIMON')\n    target_schema = 'PUBLIC' if DEFAULTS.get('schema') == 'TPCH_SF1' else DEFAULTS.get('schema', 'PUBLIC')\n    iceberg_table = ICEBERG_CONFIG.get('test_table_name', 'NATION_ICEBERG')\n    \n    print(\"Setting up Python DuckDB with Iceberg extension...\")\n    print(f\"  Account: {account}\")\n    print(f\"  Database: {target_db}\")\n    print(f\"  Schema: {target_schema}\")\n    print(f\"  Table: {iceberg_table}\")\n    \n    try:\n        con = duckdb.connect(':memory:')\n        \n        # Install extensions\n        print(\"\\nInstalling extensions...\")\n        con.execute(\"INSTALL httpfs\")\n        con.execute(\"INSTALL iceberg\")\n        con.execute(\"LOAD httpfs\")\n        con.execute(\"LOAD iceberg\")\n        print(\"\u2713 Extensions loaded\")\n        \n        # Create secret\n        con.execute(f\"\"\"\n            CREATE OR REPLACE SECRET horizon_secret (\n                TYPE iceberg,\n                TOKEN '{horizon_access_token}'\n            )\n        \"\"\")\n        print(\"\u2713 Secret created\")\n        \n        # Attach Horizon\n        endpoint = f\"https://{account}.snowflakecomputing.com/polaris/api/catalog\"\n        print(f\"\\nAttaching Horizon Catalog...\")\n        print(f\"  Endpoint: {endpoint}\")\n        \n        con.execute(f\"\"\"\n            ATTACH '{target_db}' AS horizon (\n                TYPE iceberg,\n                ENDPOINT '{endpoint}',\n                SECRET horizon_secret\n            )\n        \"\"\")\n        print(\"\u2713 Horizon Catalog attached\")\n        \n        # Query\n        print(f\"\\nQuerying {target_schema}.{iceberg_table}...\")\n        result = con.execute(f'SELECT * FROM horizon.\"{target_schema}\".{iceberg_table} LIMIT 10').fetchdf()\n        print(f\"\\n\u2713 Query successful! {len(result)} rows:\")\n        display(result)\n        \n        con.close()\n        \n    except Exception as e:\n        print(f\"\\n\u26a0 Error: {e}\")\n        print(\"\\nThis may be due to:\")\n        print(\"  - Extension not available in this environment\")\n        print(\"  - Vended credentials not configured\")\n        print(\"  - Authentication issue with Horizon\")\n        print(\"\\n\u2192 Use Python Bridge (Section 7.3.1) as reliable fallback\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.5 Recommended Alternative: Snowflake + DuckDB Hybrid\n\nUntil full Iceberg REST catalog support is available, use the working DuckDB Snowflake extension approach from Section 7:\n\n1. **Query Snowflake via ADBC** (using DuckDB Snowflake extension)\n2. **Cache results locally** in DuckDB\n3. **Use dplyr/dbplyr** on the local cache\n\nThis provides the same benefits (local processing, R ecosystem) with full support today.",
      "id": "63661b21-0a26-48ac-b343-13d430484784"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python",
        "name": "R__iceberg_hybrid",
        "title": "R__iceberg_hybrid"
      },
      "outputs": [],
      "source": "%%R\n# Hybrid Approach: Best of Both Worlds\n# Use the working DuckDB + Snowflake pattern for Iceberg-like benefits\n\n# Prerequisites: Run Section 7.3 first to create duckdb_con\n\n# Example: Query Snowflake Iceberg table, cache locally\n# (Even though it's an Iceberg table in Snowflake, query via SQL works!)\n\n# Check if duckdb_con exists\nif (!exists(\"duckdb_con\")) {\n    cat(\"duckdb_con not found.\\n\")\n    cat(\"Run Section 7.3 first to set up DuckDB connection.\\n\")\n} else {\n    # Query the Iceberg table via standard Snowflake SQL\n    dbExecute(duckdb_con, \"\n        CREATE OR REPLACE TABLE nation_iceberg_local AS \n        SELECT * FROM sf.PUBLIC.NATION_ICEBERG\n    \")\n    \n    # Now use dplyr on the local cache\n    library(dplyr)\n    library(dbplyr)\n    \n    tbl(duckdb_con, 'nation_iceberg_local') %>%\n        group_by(N_REGIONKEY) %>%\n        summarise(\n            nations = n(),\n            sample_name = first(N_NAME)\n        ) %>%\n        collect() %>%\n        print()\n    \n    cat(\"\\n\u2713 Hybrid pattern complete.\\n\")\n}",
      "id": "33ff8779-28b0-4f0e-9abe-70401f1844a0"
    },
    {
      "cell_type": "markdown",
      "id": "e96125a8-ea1b-4e54-bacb-f106bf2cee79",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| `ModuleNotFoundError: No module named 'rpy2'` | Run Section 1.2 to install rpy2 |\n| `R.version.string` returns error | Verify PATH and R_HOME are set correctly |\n| ADBC `auth_pat` error | Ensure PAT was created and stored in `SNOWFLAKE_PAT` |\n| Network policy error | PAT may need `MINS_TO_BYPASS_NETWORK_POLICY_REQUIREMENT` |\n| `adbcsnowflake` not found | Ensure setup script ran with `--adbc` flag |\n| Setup script fails | Check `setup_r.log` for detailed error messages |\n| `r_sf_con` not found | Run `get_snowflake_connection()` to create connection |\n\n### Run Full Diagnostics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f31ce02-b7b8-4cc0-a2fb-38e3b45bfedd",
      "metadata": {
        "language": "python",
        "name": "PY__environment_check",
        "title": "PY__environment_check"
      },
      "outputs": [],
      "source": "# Comprehensive diagnostic check\nfrom r_helpers import print_diagnostics\nprint_diagnostics()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdef786-f0f5-474c-8faa-b6af52a56ef1",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Environment diagnostics\nimport os\nimport shutil\n\nprint(\"Quick Environment Check:\")\nprint(f\"  R_HOME: {os.environ.get('R_HOME', 'NOT SET')}\")\nprint(f\"  R binary: {shutil.which('R') or 'NOT FOUND'}\")\nprint(f\"  SNOWFLAKE_ACCOUNT: {os.environ.get('SNOWFLAKE_ACCOUNT', 'NOT SET')}\")\nprint(f\"  SNOWFLAKE_PAT: {'SET' if os.environ.get('SNOWFLAKE_PAT') else 'NOT SET'}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf4f8e4-213b-40a0-9e54-1a97208b7646",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# View setup log if something went wrong\n# !tail -50 setup_r.log"
    }
  ]
}