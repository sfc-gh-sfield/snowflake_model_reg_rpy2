{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb3ecf4a-ef26-4a83-9670-2a23fdf5e64d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# R in Snowflake Workspace Notebooks\n\nThis notebook demonstrates how to use R within Snowflake Workspace Notebooks using rpy2.\n\n**Capabilities:**\n- Execute R code in `%%R` magic cells alongside Python\n- Transfer data bidirectionally between Python and R\n- Connect to Snowflake from R using ADBC, Reticulate, or DuckDB\n\n**Quick Start:**\n1. Run Section 1 (Installation)\n2. Run Section 3.1 (Session Setup)\n3. Choose your preferred data access method (Sections 3-7)\n\n**Sections:**\n1. [Installation & Configuration](#section-1-installation--configuration)\n2. [Python & R Interoperability](#section-2-python--r-interoperability)\n3. [Snowflake via ADBC](#section-3-snowflake-database-connectivity) (direct R connection)\n4. [Key Pair Authentication](#section-4-alternative-authentication---key-pair-jwt) (alternative to PAT)\n5. [Snowflake via Reticulate](#section-5-reticulate---access-snowpark-from-r) (easiest - no auth setup)\n6. [Data Visualization with ggplot2](#section-6-data-visualization-with-ggplot2)\n7. [DuckDB Integration](#section-7-duckdb-integration-experimental) (dplyr + dbplyr)\n8. [Iceberg Integration](#section-8-iceberg-integration) (experimental)"
    },
    {
      "cell_type": "markdown",
      "id": "da3e6d58-aaf8-43be-90c7-cf76455e23ee",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 1: Installation & Configuration\n\nThis section sets up R and rpy2 in the Workspace Notebook environment.\n\n## Overview\n\nSnowflake Workspace Notebooks run in containers with a managed Python kernel. To use R:\n\n1. **Install R** via micromamba (lightweight conda-compatible package manager)\n2. **Install rpy2** into the notebook's Python kernel\n3. **Register `%%R` magic** for R cell support\n\n## Customizing R Packages\n\nEdit `r_packages.yaml` to customize which R packages are installed:\n\n```yaml\n# Conda-forge packages (installed via micromamba)\nconda_packages:\n  - r-base           # Required: Base R\n  - r-tidyverse      # Data manipulation\n  - r-yourpackage    # Add packages here\n\n# CRAN packages (installed via install.packages)\ncran_packages:\n  - somepackage      # Packages not available on conda-forge\n```\n\n## Installation Options\n\n| Command | Description |\n|---------|-------------|\n| `bash setup_r_environment.sh` | Basic R installation |\n| `bash setup_r_environment.sh --adbc` | R + ADBC driver for Snowflake connectivity |\n| `bash setup_r_environment.sh --verbose` | Show detailed logging |\n| `bash setup_r_environment.sh --help` | Show all options |"
    },
    {
      "cell_type": "markdown",
      "id": "110f15a0-8ee6-4381-aef1-b107ce941a9a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.1 Install R Environment\n\nRun the setup script. Choose `--basic` for R only, or `--adbc` to include ADBC Snowflake driver.\n\n**Note:** This step takes 2-5 minutes on first run. The `--adbc` option takes longer as it compiles the Snowflake driver.\n\nThe script includes:\n- Pre-flight checks (disk space, network connectivity)\n- Automatic retry for network operations\n- Logging to `setup_r.log` for debugging"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c47b16da-93c7-4d39-a76b-8b97291c3f3c",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Choose ONE of the following:\n\n# Option A: Basic R installation (faster)\n# !bash setup_r_environment.sh --basic\n\n# Option B: R + ADBC for Snowflake connectivity (required for Section 3)\n!bash setup_r_environment.sh --adbc"
    },
    {
      "cell_type": "markdown",
      "id": "9b76d476-0f07-48f8-aa54-1774c1ea5d58",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.2 Configure Python Environment & Install rpy2\n\nThis cell uses the helper module to:\n1. Point Python to the R environment\n2. Install rpy2 into the notebook kernel\n3. Register the `%%R` magic\n4. Load output helper functions for cleaner display\n\n**Run this cell after the installation script completes.**\n\n**Output Helpers:** Workspace Notebooks add extra line breaks to R output. After setup, use these R functions for cleaner formatting:\n\n| Function | Usage | Description |\n|----------|-------|-------------|\n| `rprint(x)` | `rprint(df)` | Print any object cleanly |\n| `rview(df, n)` | `rview(iris, n=10)` | View data frame with optional row limit |\n| `rglimpse(df)` | `rglimpse(df)` | Glimpse data frame structure |"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa92858-c66e-4875-bbe0-0e8f9567e2d2",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Setup R helpers\nimport sys\nsys.path.insert(0, '.')  # Ensure current directory is in path\n\nfrom r_helpers import setup_r_environment\n\nresult = setup_r_environment()\n\nif result['success']:\n    print(\"✓ R environment configured successfully\")\n    print(f\"  R version: {result['r_version']}\")\n    print(f\"  rpy2 installed: {result['rpy2_installed']}\")\n    print(f\"  %%R magic registered: {result['magic_registered']}\")\nelse:\n    print(\"✗ Setup failed:\")\n    for error in result['errors']:\n        print(f\"  - {error}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6369cbe4-4fd4-4d0a-8678-db2a4bdcbca1",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Manual R configuration\n# Uncomment and run if Method 1 fails\n\n# import os\n# import sys\n# import subprocess\n\n# ENV_PREFIX = \"/root/.local/share/mamba/envs/r_env\"\n# os.environ[\"PATH\"] = f\"{ENV_PREFIX}/bin:\" + os.environ[\"PATH\"]\n# os.environ[\"R_HOME\"] = f\"{ENV_PREFIX}/lib/R\"\n\n# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rpy2\", \"-q\"], check=True)\n\n# from rpy2.ipython import rmagic\n# get_ipython().register_magics(rmagic.RMagics)\n# print(\"R environment configured\")"
    },
    {
      "cell_type": "markdown",
      "id": "6d34bdc6-d048-4c44-9ff6-36c3108708a4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.3 Verify R Installation\n\nTest that R is working correctly."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c5904b-aaee-4c87-83ec-c391ae8f47db",
      "metadata": {
        "language": "python",
        "name": "R__version",
        "title": "R__version"
      },
      "outputs": [],
      "source": "%%R\nR__version_check\n# Print R version (simple output works fine)\nR.version.string"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0687bf2-1bfc-479f-917b-300759dafdc1",
      "metadata": {
        "language": "python",
        "name": "R__installed_packages",
        "title": "R__installed_packages"
      },
      "outputs": [],
      "source": "%%R\nR__list_packages\n# List installed packages\n# Use rprint() for cleaner output in Workspace Notebooks\nip <- as.data.frame(installed.packages()[, c(1, 3:4)])\nip <- ip[is.na(ip$Priority), 1:2, drop = FALSE]\nrprint(ip)"
    },
    {
      "cell_type": "markdown",
      "id": "cd6e9057-a693-43a1-bb3c-60d64b8a9d3b",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.4 Run Diagnostics (Optional)\n\nRun comprehensive environment diagnostics to verify all components are working."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d5316e-ecac-490c-ae5f-bda9d1bdc534",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from r_helpers import check_environment, print_diagnostics\n\n# Run and display diagnostics\nprint_diagnostics()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.5 Installing Additional R Packages\n\nYou can install R packages in two ways:\n\n1. **Via `r_packages.yaml`** - Add packages before running the setup script (recommended for reproducibility)\n2. **From within a `%%R` cell** - Install packages interactively during your session\n\nThe examples below show how to install packages from within the notebook.",
      "id": "fff35cf4-94f2-4d52-b711-1ee60dcff48e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__install_packages\n# Install R packages into the micromamba environment\n# Set the library path to ensure packages go to the right location\nlib_path <- \"/root/.local/share/mamba/envs/r_env/lib/R/library\"\n.libPaths(lib_path)\n\n# Example: Install 'forecast' package if not already installed\nif (!require(\"forecast\", quietly = TRUE)) {\n    cat(\"Installing forecast package...\\n\")\n    install.packages(\"forecast\", repos = \"https://cloud.r-project.org/\", lib = lib_path)\n}\n\n# Verify installation\nlibrary(forecast)\ncat(\"forecast version:\", as.character(packageVersion(\"forecast\")), \"\\n\")",
      "id": "099fa673-52a0-4c68-a0e2-8a20e798eb32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__install_via_micromamba\n# Alternative: Use micromamba for packages with complex dependencies\n# This is better for compiled packages that need system libraries\n\n# Install via micromamba (runs in background)\nsystem(\"/root/.local/share/mamba/bin/micromamba install -n r_env -c conda-forge r-forecast -y\", \n       ignore.stdout = TRUE)\n\n# Reload library path and verify\n.libPaths(\"/root/.local/share/mamba/envs/r_env/lib/R/library\")\nlibrary(forecast)\ncat(\"forecast installed via micromamba\\n\")\ncat(\"Version:\", as.character(packageVersion(\"forecast\")), \"\\n\")",
      "id": "7f1acae2-3924-40d6-a7a0-d8df85b7ff69"
    },
    {
      "cell_type": "markdown",
      "id": "eeaa30a6-e61e-4a4d-a942-742b808aaa08",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 2: Python & R Interoperability\n\nThis section demonstrates how to work with data in both Python and R, including:\n- Using the `%%R` magic for R cells\n- Passing data from Python to R\n- Passing data from R to Python\n- Running R functions from Python"
    },
    {
      "cell_type": "markdown",
      "id": "8921eed3-bfee-423e-a3ea-3464d7644af4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.1 Using %%R Magic Cells\n\nThe `%%R` magic lets you write R code directly in a cell. The magic supports flags:\n\n| Flag | Description |\n|------|-------------|\n| `-i var` | Import Python variable `var` into R |\n| `-o var` | Export R variable `var` back to Python |\n| `-w WIDTH` | Set plot width |\n| `-h HEIGHT` | Set plot height |"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0b6790-046d-4395-9d7f-b3dbec39def9",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__basic_operations\n# Basic R operations\nx <- c(1, 2, 3, 4, 5)\nmean(x)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2ef3bb-538f-437d-9629-46c6977bd5af",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__tidyverse_demo\n# Using tidyverse\nlibrary(dplyr)\n\nrprint(\ndata.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  score = c(85, 92, 78)\n) %>%\n  mutate(grade = case_when(\n    score >= 90 ~ \"A\",\n    score >= 80 ~ \"B\",\n    TRUE ~ \"C\"\n  ))\n)  "
    },
    {
      "cell_type": "markdown",
      "id": "f4e22a3c-b996-418b-9dc7-3c9e51cdcb9a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.2 Passing Data: Python → R\n\nUse the `-i` flag to pass Python objects into R cells."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ae0e2d-6275-45c1-8878-498163bcc167",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Create a pandas DataFrame in Python\nimport pandas as pd\n\npython_df = pd.DataFrame({\n    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n    'population': [8336817, 3979576, 2693976, 2320268],\n    'area_sq_mi': [302.6, 468.7, 227.3, 670.6]\n})\n\nprint(\"Python DataFrame:\")\npython_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96305029-d546-4e7b-9f18-f4e5cae75341",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -i python_df\nR__python_to_r\n# The Python DataFrame is now available in R as 'python_df'\nlibrary(dplyr)\n\ncat(\"Received DataFrame in R:\\n\")\nrglimpse(python_df)  # Use rglimpse() for clean output\n\n# Perform R operations\nresult <- python_df %>%\n  mutate(density = population / area_sq_mi) %>%\n  arrange(desc(density))\n\nrprint(result)  # Use rprint() for clean output"
    },
    {
      "cell_type": "markdown",
      "id": "f80d038b-c7fa-44f4-b224-74e62a334d90",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.3 Passing Data: R → Python\n\nUse the `-o` flag to export R objects back to Python."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6624ecbd-01c1-4737-9581-7400053d80e2",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -o r_result\nR__r_to_python\n# Create a data frame in R\nr_result <- data.frame(\n  x = 1:10,\n  y = (1:10)^2,\n  label = paste0(\"Point_\", 1:10)\n)\n\ncat(\"Created R data.frame:\\n\")\nrprint(r_result)  # Use rprint() for clean output"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0196ae-2a28-4b7c-a644-2af75ab3e238",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# The R data.frame is now available in Python\nprint(\"R result in Python:\")\nprint(type(r_result))\nprint(r_result)"
    },
    {
      "cell_type": "markdown",
      "id": "3dba3b2c-8594-46a9-aff9-bc47f5864621",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.4 Using R from Python (without magic)\n\nFor more control, you can use rpy2's Python API directly."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91b7472-100e-4d4a-8cf3-d75359519ecc",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.conversion import localconverter\n\n# Import R packages\nbase = importr('base')\nstats = importr('stats')\n\n# Run R code and get results\nresult = ro.r('sum(1:100)')\nprint(f\"Sum of 1 to 100: {result[0]}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d9baa1-fc0a-4e7e-929c-2c023e841256",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Convert pandas DataFrame to R and run R functions on it\nimport pandas as pd\nimport rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\n\n# Create sample data\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [2.1, 3.9, 6.2, 7.8, 10.1]\n})\n\n# Convert to R and run linear regression\nwith (ro.default_converter + pandas2ri.converter).context():\n    r_df = ro.conversion.get_conversion().py2rpy(df)\n\n# Run linear regression in R\nlm_result = stats.lm('y ~ x', data=r_df)\nprint(\"Linear Regression Results:\")\nprint(base.summary(lm_result))"
    },
    {
      "cell_type": "markdown",
      "id": "6cea2255-06a5-4543-a815-62ab6ac34948",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2.5 Working with R's Built-in Datasets\n\nAccess R's built-in datasets and convert them to pandas."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90506701-31d8-401e-9b79-9dcebed176ae",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.conversion import localconverter\n\n# Load the iris dataset in R\nro.r(\"data(iris)\")\n\n# Get the R data.frame\niris_r = ro.r[\"iris\"]\n\n# Convert to pandas DataFrame\nwith localconverter(ro.default_converter + pandas2ri.converter):\n    iris_df = pandas2ri.rpy2py(iris_r)\n\nprint(\"Iris dataset (first 10 rows):\")\niris_df.head(10)"
    },
    {
      "cell_type": "markdown",
      "id": "f35bd66a-491d-4ee4-9e5e-fb66a5fb74fc",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 3: Snowflake Database Connectivity\n\nThis section demonstrates connecting to Snowflake from R using ADBC.\n\n**Prerequisites:**\n- Run the setup script with `--adbc` flag (Section 1.1)\n- Have appropriate Snowflake permissions\n\n## Authentication Options\n\n| Method | Status | Notes |\n|--------|--------|-------|\n| Python `get_active_session()` | ✅ Works | Use for Snowpark queries, bridge to R via rpy2 |\n| ADBC with PAT | ✅ Works | Direct R-to-Snowflake, requires PAT token |\n| SPCS OAuth Token | ❌ Blocked | Container token not authorized for ADBC |\n| Username/Password | ❌ Blocked | SPCS requires OAuth |\n\n## Connection Management\n\nThis notebook uses connection pooling - the ADBC connection is stored as `r_sf_con` in R's global environment and reused across cells. This avoids the overhead of creating new connections for each query."
    },
    {
      "cell_type": "markdown",
      "id": "d554f970-0734-4b7a-bac6-de8a4851afd3",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.1 Setup Python Session\n\nThis cell loads configuration and establishes the Snowflake session.\n\n**Configuration:**\n- Copy `notebook_config.yaml.template` to `notebook_config.yaml`\n- Edit with your account details (for Local IDE)\n- The config provides database, schema, warehouse, and query settings\n\n**Environments:**\n- **Workspace Notebook**: Uses `get_active_session()` (built-in OAuth)\n- **Local IDE (VSCode/Cursor)**: Uses config file for key-pair auth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7308601a-bba7-43b7-b962-eb71ccd7b3bf",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Setup Snowflake session and load configuration\nimport os\nimport sys\n\n# =============================================================================\n# Load Configuration File\n# =============================================================================\nCONFIG_FILE = 'notebook_config.yaml'\nCONFIG_TEMPLATE = 'notebook_config.yaml.template'\n\ndef load_config():\n    \"\"\"Load configuration from YAML file.\"\"\"\n    try:\n        import yaml\n    except ImportError:\n        print(\"Installing PyYAML...\")\n        import subprocess\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyyaml\", \"-q\"], check=True)\n        import yaml\n    \n    if os.path.exists(CONFIG_FILE):\n        with open(CONFIG_FILE) as f:\n            config = yaml.safe_load(f)\n        print(f\"✓ Loaded config from {CONFIG_FILE}\")\n        return config\n    elif os.path.exists(CONFIG_TEMPLATE):\n        print(f\"✗ Config not found!\")\n        print(f\"  Copy {CONFIG_TEMPLATE} to {CONFIG_FILE} and customize.\")\n        return {}\n    else:\n        print(\"✗ No config file found, using defaults\")\n        return {}\n\nCONFIG = load_config()\n\n# Extract config sections for easy access\nCONN_CONFIG = CONFIG.get('connection', {})\nDEFAULTS = CONFIG.get('defaults', {})\nQUERY_CONFIG = CONFIG.get('sample_queries', {})\nICEBERG_CONFIG = CONFIG.get('iceberg', {})\n\n# =============================================================================\n# Environment Detection and Session Setup\n# =============================================================================\ndef detect_environment():\n    \"\"\"\n    Detect if running in Snowflake Workspace Notebook or local IDE.\n    Returns: ('workspace', session) or ('local', config_dict)\n    \"\"\"\n    workspace_indicators = [\n        os.path.exists('/snowflake/session/token'),\n        'SNOWFLAKE_HOST' in os.environ,\n        '/home/udf' in os.getcwd(),\n    ]\n    \n    if any(workspace_indicators):\n        try:\n            from snowflake.snowpark.context import get_active_session\n            session = get_active_session()\n            return ('workspace', session)\n        except Exception as e:\n            return ('workspace_error', str(e))\n    else:\n        return ('local', None)\n\n# Detect environment\nENV_TYPE, ENV_RESULT = detect_environment()\n\nif ENV_TYPE == 'workspace':\n    session = ENV_RESULT\n    \n    # Get connection details from session, with config overrides\n    ACCOUNT = session.sql('SELECT CURRENT_ACCOUNT()').collect()[0][0]\n    USER = session.sql('SELECT CURRENT_USER()').collect()[0][0]\n    DATABASE = DEFAULTS.get('database') or session.get_current_database()\n    SCHEMA = DEFAULTS.get('schema') or session.get_current_schema()\n    WAREHOUSE = DEFAULTS.get('warehouse') or session.get_current_warehouse()\n    ROLE = session.get_current_role()\n    \n    # Build unified config\n    ENV_CONFIG = {\n        'account': ACCOUNT,\n        'user': USER,\n        'database': DATABASE,\n        'schema': SCHEMA,\n        'warehouse': WAREHOUSE,\n        'role': ROLE,\n    }\n    \n    print(f\"\\nEnvironment: Workspace Notebook\")\n    print(f\"  Account:   {ACCOUNT}\")\n    print(f\"  User:      {USER}\")\n    print(f\"  Role:      {ROLE}\")\n    print(f\"  Database:  {DATABASE}\")\n    print(f\"  Schema:    {SCHEMA}\")\n    print(f\"  Warehouse: {WAREHOUSE}\")\n    \nelif ENV_TYPE == 'local':\n    session = None\n    \n    # Use config file values for local IDE\n    ENV_CONFIG = {\n        'account': CONN_CONFIG.get('account', '<YOUR_ACCOUNT>'),\n        'user': CONN_CONFIG.get('user', '<YOUR_USER>'),\n        'database': DEFAULTS.get('database', 'SNOWFLAKE_SAMPLE_DATA'),\n        'schema': DEFAULTS.get('schema', 'TPCH_SF1'),\n        'warehouse': DEFAULTS.get('warehouse', '<YOUR_WAREHOUSE>'),\n        'role': DEFAULTS.get('role', 'PUBLIC'),\n        'private_key_path': CONN_CONFIG.get('private_key_path', '~/.ssh/snowflake_rsa_key.p8'),\n    }\n    \n    print(f\"\\nEnvironment: Local IDE\")\n    print(f\"  Account:   {ENV_CONFIG['account']}\")\n    print(f\"  User:      {ENV_CONFIG['user']}\")\n    print(f\"  Database:  {ENV_CONFIG['database']}\")\n    print(f\"  Warehouse: {ENV_CONFIG['warehouse']}\")\n    print(f\"  Key path:  {ENV_CONFIG['private_key_path']}\")\n    \n    if '<YOUR_' in str(ENV_CONFIG.values()):\n        print(\"\\n⚠️  Some config values need to be set!\")\n        print(f\"   Edit {CONFIG_FILE} with your values\")\nelse:\n    print(f\"Warning: Environment detection issue: {ENV_RESULT}\")\n    ENV_CONFIG = {}\n    session = None\n\n# Make query config easily accessible\nROW_LIMIT = QUERY_CONFIG.get('default_row_limit', 1000)\nLARGE_ROW_LIMIT = QUERY_CONFIG.get('large_row_limit', 10000)\nSAMPLE_START_DATE = QUERY_CONFIG.get('sample_start_date', '1995-01-01')\nTABLES = QUERY_CONFIG.get('tables', {'nation': 'NATION', 'customer': 'CUSTOMER', 'orders': 'ORDERS'})"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Authentication Methods Overview\n\nThis notebook supports multiple authentication methods for different connectivity approaches:\n\n| Section | Method | Auth Type | Status | Environment |\n|---------|--------|-----------|--------|-------------|\n| **3.1** | `get_active_session()` | Built-in OAuth | ✅ Working | Workspace |\n| **3.3-3.6** | ADBC + PAT | PAT Token | ✅ Working | Workspace |\n| **4.2** | ADBC + Key Pair | JWT | ✅ Working | Both |\n| **5** | Reticulate | Session OAuth | ✅ Working | Workspace |\n| **7** (DuckDB) | Key Pair | JWT | ✅ Working | Local IDE |\n| **7.3.1** | Python Bridge | Session OAuth | ✅ Working | Both |\n| **8** | Horizon Catalog | JWT | ✅ Working | Local IDE |\n\n**Recommended Path for Most Users:**\n1. Run **Section 3.1** (required - sets up session)\n2. Choose ONE of:\n   - **Section 5** (Reticulate) - Easiest, uses built-in auth, works everywhere\n   - **Section 7.3.1** (Python Bridge) - For dplyr workflows, works everywhere  \n   - **Section 7** (DuckDB Direct) - For Local IDE only (key-pair required)\n   - **Section 3** (ADBC) - For direct R-to-Snowflake in Workspace",
      "id": "b4129cb2-87dd-4ac0-9865-249bcc5fad1f"
    },
    {
      "cell_type": "markdown",
      "id": "009cd9dc-7edb-45c5-a9ac-4ef46cd7343f",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.2 Create Programmatic Access Token (PAT) (For ADBC - Optional)\n\n**Used by:** Section 3 (R-ADBC) only. Skip if using Section 5 (Reticulate) or Section 7 (DuckDB).\n\nPAT enables direct R-to-Snowflake ADBC connections. The session from Section 3.1 is used to create the token."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c5b82f-6b67-475f-ba93-82c671356582",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Create PAT for authentication (requires session from 3.1)\nfrom r_helpers import PATManager\n\n# Uses the 'session' variable from Section 3.1\nif session is None:\n    print(\"Please run Section 3.1 first!\")\nelse:\n    pat_manager = PATManager(session)\n    pat_result = pat_manager.create_pat()  # Creates PAT with 1 day expiry\n    \n    if pat_result['success']:\n        print(f\"✓ PAT created successfully\")\n        print(f\"  Token: {pat_result['token'][:20]}...\")\n        print(f\"  Expires: {pat_result['expires_at']}\")\n    else:\n        print(f\"✗ PAT creation failed: {pat_result.get('error', 'Unknown error')}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e60a83b-c4aa-4aa2-bb04-ea156b006f94",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Check PAT status at any time\nstatus = pat_mgr.get_status()\nprint(\"PAT Status:\")\nfor key, value in status.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "markdown",
      "id": "842178d4-957e-4c55-8c11-e0b39f1246be",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.3 Validate ADBC Prerequisites\n\nBefore connecting, validate that all ADBC prerequisites are met."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6864614e-d4dc-4470-af1d-8d7664faaa2b",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from r_helpers import validate_adbc_connection\n\nvalid, message = validate_adbc_connection()\nprint(message)"
    },
    {
      "cell_type": "markdown",
      "id": "6eff7ee7-16d2-411d-b76c-ee5773148b32",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.4 Initialize R Connection Management\n\nLoad the connection management functions into R. This provides:\n- `get_snowflake_connection()` - Get or create connection (stored as `r_sf_con`)\n- `close_snowflake_connection()` - Close and release connection\n- `is_snowflake_connected()` - Check connection status\n- `snowflake_connection_status()` - Get detailed status"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a8002d-624c-444d-80de-e99924526040",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from r_helpers import init_r_connection_management\n\nsuccess, msg = init_r_connection_management()\nprint(msg)"
    },
    {
      "cell_type": "markdown",
      "id": "160d5092-fac0-42e8-b661-c7f286bdd7c6",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.5 Connect to Snowflake from R (ADBC)\n\nUse `get_snowflake_connection()` to establish or reuse the ADBC connection.\n\nThe connection is stored as `r_sf_con` in R's global environment and is automatically reused in subsequent cells."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34f2297-86d4-4141-ade0-4e16e852a595",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__adbc_connect\n# Get or create the Snowflake connection\n# Connection is stored globally as r_sf_con\nr_sf_con <- get_snowflake_connection()\n\n# Show connection status (uses print_connection_status() for clean output)\nprint_connection_status()"
    },
    {
      "cell_type": "markdown",
      "id": "7253f50d-eec9-4108-a3bf-2f917bbddd8a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.6 Query Snowflake from R\n\nRun queries using the `r_sf_con` connection. The connection is automatically reused across cells."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4d1703-f628-4683-b543-fe90d645e996",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__adbc_test_query\n# Simple test query using r_sf_con\nr_sf_con |>\n  read_adbc(\"SELECT CURRENT_USER() AS USER, CURRENT_ROLE() AS ROLE, CURRENT_WAREHOUSE() AS WAREHOUSE\") |>\n  tibble::as_tibble()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb06f5a2-bd50-460c-835e-10336cf2ce26",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__adbc_query_nations\n# Query sample data from Snowflake\n# Using the shared SNOWFLAKE_SAMPLE_DATA database\nnations <- r_sf_con |>\n  read_adbc(\"\n    SELECT N_NATIONKEY, N_NAME, N_REGIONKEY \n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION \n    ORDER BY N_NATIONKEY\n    LIMIT 10\n  \") |>\n  tibble::as_tibble()\n\nnations"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7b830e-423c-44dc-b991-5f77041f4596",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__adbc_query_orders\n# More complex query with aggregation\nlibrary(dplyr)\n\norders_summary <- r_sf_con |>\n  read_adbc(\"\n    SELECT \n      O_ORDERSTATUS,\n      COUNT(*) as ORDER_COUNT,\n      SUM(O_TOTALPRICE) as TOTAL_VALUE,\n      AVG(O_TOTALPRICE) as AVG_VALUE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    GROUP BY O_ORDERSTATUS\n    ORDER BY ORDER_COUNT DESC\n  \") |>\n  tibble::as_tibble()\n\norders_summary"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9781d24e-c833-48cf-aad8-199ab9a66dda",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__adbc_verify_connection\n# Verify connection is being reused (not recreated)\ncat(\"Connection still valid:\", is_snowflake_connected(), \"\\n\")"
    },
    {
      "cell_type": "markdown",
      "id": "548ca051-cac6-49f9-96a5-96c671d6c933",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.7 Query from Python, Analyze in R\n\nAn alternative pattern: use Python's Snowpark session for querying, then pass data to R for analysis."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd3f588-93a1-4a0a-a758-e6def9a507aa",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Query Snowflake via Python\ncustomers_df = session.sql(f\"\"\"\n    SELECT \n        C_CUSTKEY,\n        C_NAME,\n        C_NATIONKEY,\n        C_ACCTBAL\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n    LIMIT 100\n\"\"\").to_pandas()\n\nprint(f\"Retrieved {len(customers_df)} rows\")\ncustomers_df.head()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97291af9-fb1a-44ba-bdd0-382c37bfe407",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -i customers_df\nR__analyze_customers\n# Analyze the data in R\nlibrary(dplyr)\n\ncat(\"Summary Statistics for Customer Account Balance:\\n\")\nrprint(summary(customers_df$C_ACCTBAL))\n\ncat(\"\\nCustomers by Nation (top 5):\\n\")\nresult <- customers_df %>%\n  group_by(C_NATIONKEY) %>%\n  summarise(\n    count = n(),\n    avg_balance = mean(C_ACCTBAL),\n    total_balance = sum(C_ACCTBAL)\n  ) %>%\n  arrange(desc(count)) %>%\n  head(5)\n\nrprint(result)  # Use rprint() for clean output"
    },
    {
      "cell_type": "markdown",
      "id": "fa4dd5c0-f9b0-4c2b-8173-5e72053d84cd",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.8 Check Connection Status\n\nYou can check the connection status from either Python or R."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65cbb49-52c4-4233-8e5f-0b49fb52b95d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Check status from Python\nfrom r_helpers import get_r_connection_status\n\nstatus = get_r_connection_status()\nprint(\"R Connection Status (from Python):\")\nfor key, value in status.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8276ca-a924-440f-9e58-0476efa92af5",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__check_connection\n# Get or create the Snowflake connection\n# Connection is stored globally as r_sf_con\nr_sf_con <- get_snowflake_connection()\n\n# Show connection status (uses print_connection_status() for clean output)\nprint_connection_status()"
    },
    {
      "cell_type": "markdown",
      "id": "93b418d9-faa8-435f-b2fd-5da6f377951c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3.9 Clean Up\n\nClose ADBC connection and optionally remove the PAT."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e5e418-d204-4d6f-b220-aab224bdaef3",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__close_connection\n# Close the Snowflake connection\nclose_snowflake_connection()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b970b88d-e34b-4a8e-b86b-48c60e2ca6ec",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Alternative: Close from Python\n# from r_helpers import close_r_connection\n# success, msg = close_r_connection()\n# print(msg)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c429c3-d3ff-4a73-bd38-acb15f244eb1",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Cleanup - remove PAT\n# pat_mgr.remove_pat()\n# print(\"PAT removed\")"
    },
    {
      "cell_type": "markdown",
      "id": "7c34f713-51a1-49b1-91ea-1a8d48c6b0fb",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 4: Alternative Authentication - Key Pair (JWT)\n\nThis section demonstrates Key Pair (JWT) authentication as an alternative to PAT.\n\n## Authentication Methods for R ADBC\n\n| Method | Status | Notes |\n|--------|--------|-------|\n| **PAT (Programmatic Access Token)** | ✅ Working | **Recommended** - easiest to set up (see Section 3) |\n| **Key Pair (JWT)** | ✅ Working | Alternative - no token expiry, shown below |\n| SPCS OAuth Token | ❌ Blocked | Container token restricted to specific connectors |\n| Username/Password | ❌ Blocked | SPCS enforces OAuth for internal connections |\n\n> **Note:** For tests of non-working methods, see `archive/auth_methods_not_working.ipynb`\n\n## Prerequisites\n\n- ADBC installed (`--adbc` flag during setup)\n- RSA key pair generated\n- Public key registered with your Snowflake user"
    },
    {
      "cell_type": "markdown",
      "id": "c881522f-bb36-4052-a258-f9453b36b5e6",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.1 Load Alternative Auth Test Functions\n\nLoad the R functions for testing different authentication methods."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f299b22-9a97-4fbc-9193-5a0f6ba41c7a",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from r_helpers import init_r_alt_auth\n\nsuccess, msg = init_r_alt_auth()\nprint(msg)"
    },
    {
      "cell_type": "markdown",
      "id": "f6b2a13e-1808-44df-bc82-f2e860e34329",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.2 Key Pair (JWT) Authentication (Alternative for ADBC)\n\n**Used by:** Section 3 (R-ADBC) as an alternative to PAT, and Section 8 (Iceberg) for Horizon Catalog API.\n\nKey pair authentication uses RSA keys instead of passwords/PAT. This method is MFA-compatible and doesn't expire like PAT tokens."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9574456f-8ab5-498f-b95e-c7941ed73936",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Key-pair authentication setup\nfrom r_helpers import KeyPairAuth\n\n# Initialize key pair auth helper\nkp_auth = KeyPairAuth()\n\n# Generate a new key pair (or use load_private_key() for existing key)\n# Note: Requires 'cryptography' package: pip install cryptography\nresult = kp_auth.generate_key_pair(\n    key_size=2048,\n    output_dir=\"/tmp\",\n    passphrase=None  # Set a passphrase for encrypted key\n)\n\nif result['success']:\n    print(\"✓ Key pair generated successfully\")\n    print(f\"  Private key: {result['private_key_path']}\")\n    print(f\"  Public key:  {result['public_key_path']}\")\n    print(f\"\\n  Public key for Snowflake registration:\")\n    print(f\"  {result['public_key_for_snowflake'][:50]}...\")\nelse:\n    print(f\"✗ Key generation failed: {result['error']}\")"
    },
    {
      "cell_type": "markdown",
      "id": "735ac1b2-caec-4807-9dda-acb7eeabe26f",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Step 2: Register Public Key with Snowflake\n\nRun this SQL to register the public key with your user (requires ACCOUNTADMIN or appropriate privileges)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093fc815-1bd0-4f97-899d-c60c1addd15a",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Generate key registration SQL\nif result['success']:\n    sql = kp_auth.register_public_key_sql(result['public_key_for_snowflake'])\n    print(\"Run this SQL to register the public key:\")\n    print(\"-\" * 60)\n    print(sql)\n    print(\"-\" * 60)\n    print(\"\\nOr run via Snowpark session:\")\n    print(\"  session.sql(sql).collect()\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28554e82-cf03-48f1-bf54-697101761076",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Register public key in Snowflake\nsession.sql(sql).collect()"
    },
    {
      "cell_type": "markdown",
      "id": "229490a2-9617-4de1-a555-564dce292bef",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### Step 3: Configure and Test Key Pair Auth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539bf7b1-8623-474c-ae55-8401f263f053",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Configure environment for key pair auth\nconfig = kp_auth.configure_for_adbc()\nprint(\"Key Pair Auth Configuration:\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e962629-43d8-49d8-84de-ab7506019bb3",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__keypair_auth_test\n# Test key pair authentication\n# Note: Public key must be registered with user first!\nresult <- test_keypair_auth()\nrprint(result)"
    },
    {
      "cell_type": "markdown",
      "id": "9d37aef7-8b00-41e1-9cba-f50380f8dcf9",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4.3 Authentication Summary\n\n### Working Methods\n\n| Method | Auth Type | Best For |\n|--------|-----------|----------|\n| **PAT** | `auth_pat` | Most use cases - easy programmatic setup |\n| **Key Pair** | `auth_jwt` | Long-lived credentials without expiry |\n\n### Non-Working Methods (Blocked by SPCS)\n\n| Method | Reason |\n|--------|--------|\n| SPCS OAuth Token | Restricted to specific Snowflake connectors |\n| Username/Password | SPCS enforces OAuth internally |\n\n> See `archive/auth_methods_not_working.ipynb` for test code if needed."
    },
    {
      "cell_type": "markdown",
      "id": "5e2f1f8f-eee0-4da1-b29a-071880cd4a71",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 5: Reticulate - Access Snowpark from R\n\nThis section demonstrates using **reticulate** to access the Python Snowpark session directly from R. This is an alternative to ADBC that leverages the notebook's built-in authentication.\n\n## Advantages of Reticulate Approach\n\n| Feature | Reticulate + Snowpark | ADBC |\n|---------|----------------------|------|\n| Authentication | Uses notebook's built-in auth | Requires PAT or Key Pair |\n| Setup | No additional auth setup | PAT creation or key registration |\n| Connection | Shares Python session | Separate R connection |\n| Best for | Quick queries, prototyping | Production R pipelines |\n\n## How It Works\n\n1. R accesses Python's Snowpark session via reticulate\n2. Execute SQL queries using `session$sql()`\n3. Convert results to pandas DataFrame with `.to_pandas()`\n4. Reticulate automatically converts pandas → R data.frame\n\n## Output Pattern\n\nFor best display in Notebooks, use `%%R -o variable` to export R data frames to Python, then display them in a subsequent Python cell. This lets the Notebook render the DataFrame with proper formatting."
    },
    {
      "cell_type": "markdown",
      "id": "59d0ea0c-5ae6-4efd-aa08-0da369432f46",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.1 Setup Reticulate\n\nConfigure reticulate to use the notebook's Python environment.\n\n> **Note:** You may see a warning about reticulate/rpy2 compatibility. This is safe to ignore if using reticulate >= 1.25 (installed by default). The issue was fixed in reticulate PR #1188."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233e9988-bb37-4569-9960-6b9d72f2048a",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__setup_reticulate\nlibrary(reticulate)\n\n# Use the same Python that's running the notebook kernel\n# This ensures we access the same Snowpark session\nuse_python(Sys.which(\"python3\"), required = TRUE)\n\n# Verify Python is accessible\npy_config()"
    },
    {
      "cell_type": "markdown",
      "id": "a744bd12-1ed8-4e64-9129-a25142bd0e1e",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.2 Access Snowpark Session from R\n\nImport the Snowpark module and get the active session. This uses the notebook's built-in authentication - no PAT required!"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4f44dd-af95-4cb4-8d75-70dc663cc358",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__access_snowpark\n# Import Snowpark module\nsnowpark <- import(\"snowflake.snowpark\")\n\n# Get the active session (uses notebook's built-in auth)\nsession <- snowpark$Session$builder$getOrCreate()\n\n# Verify connection\nrcat(\"Connected to Snowflake via Snowpark!\")\nrcat(\"Account: \", session$get_current_account())\nrcat(\"User: \", session$get_current_user())\nrcat(\"Database: \", session$get_current_database())\nrcat(\"Schema: \", session$get_current_schema())"
    },
    {
      "cell_type": "markdown",
      "id": "50bdd984-6ac7-4db3-a7fa-25807dfa7a9a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.3 Query Snowflake and Get R DataFrame\n\nExecute SQL queries and convert results to R data frames.\n\n**Output Pattern:** Use `%%R -o variable` to export results to Python, then display in the next cell for nice Notebook formatting."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ebfdab-d6b3-4056-b3fc-94dfe140a945",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -o nations_df\nR__query_nations\n# Execute a query and get Snowpark DataFrame\n# Use -o to export result to Python for nice display\nnations_df <- session$sql(\"\n    SELECT N_NATIONKEY, N_NAME, N_REGIONKEY \n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION \n    LIMIT 10\n\")$to_pandas()\n\n# Print data type (R sees this as a data.frame)\ncat(\"R data type:\", class(nations_df), \"\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd668a1b-6f73-4e27-ac85-f7f174233ce4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the exported DataFrame (nice Notebook rendering)\nnations_df"
    },
    {
      "cell_type": "markdown",
      "id": "e746a041-5ed3-4e76-928f-c4a0f5a4f98b",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.4 R Analysis on Snowflake Data\n\nPerform R analysis using dplyr on data retrieved via Snowpark. Use `-o` to export the result for display."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e853f863-b88b-44b1-8a3f-5179ab84c357",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -o customer_analysis\nR__customer_analysis\n# Query customer data with aggregation\ncustomers_df <- session$sql(\"\n    SELECT \n        C_MKTSEGMENT,\n        COUNT(*) as CUSTOMER_COUNT,\n        AVG(C_ACCTBAL) as AVG_BALANCE,\n        MIN(C_ACCTBAL) as MIN_BALANCE,\n        MAX(C_ACCTBAL) as MAX_BALANCE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n    GROUP BY C_MKTSEGMENT\n    ORDER BY AVG_BALANCE DESC\n\")$to_pandas()\n\n# Use dplyr for additional analysis\nlibrary(dplyr)\n\ncustomer_analysis <- customers_df %>%\n    mutate(\n        BALANCE_RANGE = MAX_BALANCE - MIN_BALANCE,\n        SEGMENT_SIZE = case_when(\n            CUSTOMER_COUNT > 30000 ~ \"Large\",\n            CUSTOMER_COUNT > 29000 ~ \"Medium\",\n            TRUE ~ \"Small\"\n        )\n    )\n\ncat(\"Analysis complete - result exported to Python\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e5910f-a6ad-4329-a355-80ec1eef5f0d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the R analysis result (exported via -o)\ncustomer_analysis"
    },
    {
      "cell_type": "markdown",
      "id": "4d900768-6b13-4381-9477-23d9cdc568b4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.5 Helper Function for Snowpark Queries\n\nCreate a convenience function to simplify querying. Use `-o` to export results for display."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86189b02-9ffd-4fa3-9b58-a52b12e67f08",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -o orders_summary\nR__snowpark_helper\n#' Query Snowflake via Snowpark and return R data.frame\n#' \n#' @param sql SQL query string\n#' @return R data.frame with query results\nsnowpark_query <- function(sql) {\n    session$sql(sql)$to_pandas()\n}\n\n# Example usage - export result with -o\norders_summary <- snowpark_query(\"\n    SELECT \n        O_ORDERSTATUS,\n        COUNT(*) as ORDER_COUNT,\n        SUM(O_TOTALPRICE) as TOTAL_VALUE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    GROUP BY O_ORDERSTATUS\n\")\n\ncat(\"Query complete - orders_summary exported to Python\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bc16d2-a25f-4493-bc59-867c0a381dc8",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the orders summary (exported via -o)\norders_summary"
    },
    {
      "cell_type": "markdown",
      "id": "4bf7b467-24bc-4eb3-b80d-e6fb1340fdaf",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5.6 Reticulate vs ADBC Comparison\n\n| Aspect | Reticulate + Snowpark | ADBC (Section 3 & 4) |\n|--------|----------------------|----------------------|\n| **Authentication** | Automatic (notebook's session) | PAT or Key Pair required |\n| **Setup complexity** | Minimal | Moderate |\n| **Data path** | Snowflake → Snowpark → pandas → R | Snowflake → Arrow → R |\n| **Performance** | Good for moderate data | Better for large data (Arrow) |\n| **R-native** | No (via Python) | Yes (native R driver) |\n| **Best for** | Quick analysis, prototyping | Production R workflows |\n\n### When to Use Each\n\n**Use Reticulate + Snowpark when:**\n- You need quick access without auth setup\n- Working interactively/prototyping\n- Data sizes are moderate (< 1M rows)\n- You're already using Python and R together\n\n**Use ADBC when:**\n- Building production R pipelines\n- Working with large datasets\n- Need pure R solution\n- Require connection pooling/management"
    },
    {
      "cell_type": "markdown",
      "id": "9b1abb74-0c73-4aef-8c4e-a7cde51101de",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 6: Data Visualization with ggplot2\n\nThis section demonstrates creating visualizations with **ggplot2** and displaying them in the Notebook.\n\n## Key Points\n\n- ggplot2 is included via `tidyverse` (installed by default)\n- Use `%%R -w WIDTH -h HEIGHT` to control plot dimensions (in pixels)\n- Call `print(p)` explicitly to render the plot\n- Plots render inline in the Notebook output\n\n## Plot Size Parameters\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `-w` | Width in pixels | `-w 800` |\n| `-h` | Height in pixels | `-h 500` |\n| `--type` | Graphics device | `--type=cairo` (optional, better quality) |"
    },
    {
      "cell_type": "markdown",
      "id": "b5c8ba29-8ffe-4915-945e-d177b138eb8b",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.1 Basic ggplot2 Example\n\nCreate a simple scatter plot using the built-in `mtcars` dataset."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e918ee36-3fc9-48c6-a9e2-36fff893f47d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -w 700 -h 450\nR__ggplot_basic\nlibrary(ggplot2)\n\n# Basic scatter plot with mtcars\np <- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n    geom_point(size = 3) +\n    labs(\n        title = \"Fuel Efficiency vs Weight\",\n        x = \"Weight (1000 lbs)\",\n        y = \"Miles per Gallon\",\n        color = \"Cylinders\"\n    ) +\n    theme_minimal()\n\nprint(p)"
    },
    {
      "cell_type": "markdown",
      "id": "7c0eb03e-cc12-4f6e-b06a-d45afe8c75b3",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.2 Visualize Snowflake Data\n\nQuery Snowflake data and create a bar chart. Bar charts work best when values have meaningful differences from zero.\n\n> **Tip:** Avoid bar charts when values are clustered in a narrow range (e.g., all ~$4,500). Use dot plots or adjust the visualization instead."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeec4aa1-3107-4b55-9f8e-f308ceb85a25",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -w 800 -h 500\nR__ggplot_snowflake\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Query Snowflake for order data by status\n# This data has more variance for a meaningful bar chart\norders <- session$sql(\"\n    SELECT \n        O_ORDERSTATUS,\n        COUNT(*) as ORDER_COUNT,\n        ROUND(SUM(O_TOTALPRICE) / 1e9, 2) as TOTAL_VALUE_BILLIONS\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    GROUP BY O_ORDERSTATUS\n    ORDER BY TOTAL_VALUE_BILLIONS DESC\n\")$to_pandas()\n\n# Create bar chart - good when values have meaningful differences\np <- ggplot(orders, aes(x = reorder(O_ORDERSTATUS, -TOTAL_VALUE_BILLIONS), \n                         y = TOTAL_VALUE_BILLIONS)) +\n    geom_col(aes(fill = ORDER_COUNT), width = 0.6) +\n    geom_text(aes(label = paste0(\"$\", TOTAL_VALUE_BILLIONS, \"B\")), \n              vjust = -0.5, size = 4) +\n    scale_fill_viridis_c(option = \"plasma\", labels = scales::comma) +\n    scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n    labs(\n        title = \"Total Order Value by Status\",\n        subtitle = \"Data from Snowflake TPC-H Sample\",\n        x = \"Order Status\",\n        y = \"Total Value ($ Billions)\",\n        fill = \"Order\\nCount\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n        plot.title = element_text(face = \"bold\")\n    )\n\nprint(p)"
    },
    {
      "cell_type": "markdown",
      "id": "48028ac8-fbab-4b4c-9dee-78a8e9a3563a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.3 Multi-Panel Visualization (Facets)\n\nCreate faceted plots to compare distributions across categories."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837d3a97-0de7-41a4-8625-0aab612915b4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -w 900 -h 600\nR__ggplot_facets\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Query order data by status and priority\norders <- session$sql(\"\n    SELECT \n        O_ORDERSTATUS,\n        O_ORDERPRIORITY,\n        O_TOTALPRICE\n    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n    LIMIT 5000\n\")$to_pandas()\n\n# Create faceted histogram\np <- ggplot(orders, aes(x = O_TOTALPRICE, fill = O_ORDERSTATUS)) +\n    geom_histogram(bins = 30, alpha = 0.7) +\n    facet_wrap(~O_ORDERPRIORITY, scales = \"free_y\", ncol = 3) +\n    scale_x_continuous(labels = scales::dollar_format(scale = 0.001, suffix = \"K\")) +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n        title = \"Order Value Distribution by Priority\",\n        subtitle = \"Colored by Order Status\",\n        x = \"Total Price\",\n        y = \"Count\",\n        fill = \"Status\"\n    ) +\n    theme_light(base_size = 11) +\n    theme(\n        plot.title = element_text(face = \"bold\"),\n        strip.text = element_text(face = \"bold\")\n    )\n\nprint(p)"
    },
    {
      "cell_type": "markdown",
      "id": "2846d831-5d58-41c5-961b-d85116fc0a71",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6.4 Saving and Loading Plots\n\nUse `ggsave()` to export plots to files, then display them from Python using `IPython.display.Image`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6eaf65-3ead-4788-9bdd-5fe21df38c76",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -w 700 -h 450\nR__ggplot_save\nlibrary(ggplot2)\n\n# Create a plot\np <- ggplot(mtcars, aes(x = hp, y = mpg)) +\n    geom_point(aes(color = factor(gear)), size = 3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkgray\") +\n    labs(\n        title = \"MPG vs Horsepower\",\n        x = \"Horsepower\",\n        y = \"Miles per Gallon\",\n        color = \"Gears\"\n    ) +\n    theme_bw()\n\n# Display the plot inline\nprint(p)\n\n# Save to file\nggsave(\"/tmp/mpg_vs_hp.png\", p, width = 8, height = 5, dpi = 150)\ncat(\"Plot saved to /tmp/mpg_vs_hp.png\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58651527-4632-4301-b079-fb47e5d6af79",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Display the saved PNG file in the notebook\nfrom IPython.display import Image, display\n\ndisplay(Image(filename=\"/tmp/mpg_vs_hp.png\"))"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-section-header",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n# Section 7: DuckDB Integration (Experimental)\n\nThis section demonstrates using **DuckDB** as an intermediary between R and Snowflake, enabling:\n- **dbplyr workflows** with Snowflake data via DuckDB's Snowflake extension\n- **Local caching** of Snowflake query results for iterative analysis\n- **Cross-environment compatibility** - works in both Workspace Notebooks and local IDEs (VSCode/Cursor)\n\n## Architecture\n\n```\nR (dplyr/dbplyr)\n    ↕ DBI\nDuckDB (in-process analytics)\n    ↕ Snowflake extension (ADBC)\nSnowflake (key-pair auth)\n```\n\n## Prerequisites\n\n1. Run setup with `--full` flag: `bash setup_r_environment.sh --full`\n2. For local IDE: Configure key-pair authentication\n3. DuckDB Snowflake extension will be installed automatically"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-env-detection",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.1 Prerequisites\n\n**Session Setup**: Environment detection and session setup is now handled in **Section 3.1**.\nMake sure you've run that cell before proceeding with DuckDB integration.\n\nThe `ENV_TYPE` variable tells you which environment you're in:\n- `'workspace'` - Workspace Notebook (uses OAuth)\n- `'local'` - Local IDE (uses key-pair auth)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-env-setup",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Verify session from Section 3.1\nif 'ENV_TYPE' not in dir():\n    print(\"Please run Section 3.1 first to set up the session!\")\nelse:\n    print(f\"Environment: {ENV_TYPE}\")\n    print(f\"Session: {'Available' if session else 'Not available (local mode)'}\")\n    if ENV_CONFIG:\n        print(f\"Account: {ENV_CONFIG.get('account', 'N/A')}\")"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-local-config",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.2 Configure Connection (Local IDE Only)\n\n**Skip this section if running in Workspace Notebook.**\n\nFor local IDEs, set these environment variables before starting your notebook:\n\n```bash\nexport SNOWFLAKE_ACCOUNT=\"your_account\"     # e.g., \"xy12345\"  \nexport SNOWFLAKE_USER=\"your_user\"\nexport SNOWFLAKE_DATABASE=\"SNOWFLAKE_SAMPLE_DATA\"\nexport SNOWFLAKE_WAREHOUSE=\"COMPUTE_WH\"\nexport SNOWFLAKE_PRIVATE_KEY_PATH=\"~/.ssh/snowflake_rsa_key.p8\"\n```\n\nThen restart your notebook and run Section 3.1 to detect the environment."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-local-config-cell",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Local IDE verification (optional)\n# Environment variables should be set before starting the notebook\n# This cell just displays the current configuration\n\nif ENV_TYPE == 'local':\n    print(\"Local IDE Configuration:\")\n    for key in ['account', 'user', 'database', 'warehouse', 'private_key_path']:\n        print(f\"  {key}: {ENV_CONFIG.get(key, 'N/A')}\")\n    \n    # Check if key file exists\n    key_path = os.path.expanduser(ENV_CONFIG.get('private_key_path', ''))\n    if os.path.exists(key_path):\n        print(f\"\\n✓ Private key file found\")\n    else:\n        print(f\"\\n✗ Private key file NOT found: {key_path}\")\nelse:\n    print(\"Running in Workspace - no local configuration needed\")"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-r-setup",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.3 DuckDB + Snowflake Setup in R\n\nThis cell configures DuckDB with the Snowflake extension and creates a connection.\n\n**Authentication:**\n- **Local IDE**: Uses key-pair authentication (✅ Tested, recommended)\n- **Workspace Notebooks**: OAuth has known issues with ADBC driver - **use Python Bridge (7.3.1) instead**\n\n**Note**: The DuckDB Snowflake extension's OAuth support has known issues with token validation. \nFor Workspace Notebooks, the **Python Bridge** approach in Section 7.3.1 is the recommended method."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-r-setup-cell",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__duckdb_setup\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(dbplyr)\n\ncat(\"Loading DuckDB with Snowflake extension...\\n\")\n\n# Connect to DuckDB (in-memory for speed, or file for persistence)\nduckdb_con <- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\n\n# Load the Snowflake extension\ntryCatch({\n    dbExecute(duckdb_con, \"INSTALL snowflake FROM community\")\n    dbExecute(duckdb_con, \"LOAD snowflake\")\n    cat(\"✓ Snowflake extension loaded\\n\")\n}, error = function(e) {\n    cat(\"✗ Error loading extension:\", conditionMessage(e), \"\\n\")\n})\n\ncat(\"DuckDB ready. Configure Snowflake secret in next cell.\\n\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-snowflake-secret",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Create DuckDB Snowflake secret (for Local IDE)\n# NOTE: OAuth has known issues with DuckDB's ADBC driver\n# For Workspace Notebooks, use the Python Bridge approach in Section 7.3.1 instead\n\nif ENV_TYPE == 'workspace':\n    print(\"=\" * 60)\n    print(\"WORKSPACE NOTEBOOK DETECTED\")\n    print(\"=\" * 60)\n    print(\"\\nDuckDB's direct Snowflake extension has OAuth issues in SPCS.\")\n    print(\"\\nRECOMMENDED: Use the Python Bridge approach in Section 7.3.1\")\n    print(\"This queries Snowflake via Python and transfers data to R/DuckDB.\")\n    print(\"\\nSkip the next R cell and proceed to Section 7.3.1.\")\n    # Use empty dict instead of None for R interop\n    duckdb_auth = {'method': 'none'}\n    \nelse:\n    # Local IDE: Use key-pair auth (tested and working)\n    key_path = os.path.expanduser(ENV_CONFIG.get('private_key_path', ''))\n    if os.path.exists(key_path):\n        with open(key_path, 'r') as f:\n            private_key = f.read()\n        \n        duckdb_auth = {\n            'method': 'keypair',\n            'account': ENV_CONFIG['account'],\n            'user': ENV_CONFIG['user'],\n            'database': ENV_CONFIG['database'],\n            'warehouse': ENV_CONFIG['warehouse'],\n            'private_key': private_key,\n        }\n        print(f\"✓ Private key loaded from {key_path}\")\n        print(f\"  Account: {ENV_CONFIG['account']}\")\n        print(\"\\nRun the next R cell to create the Snowflake secret\")\n    else:\n        print(f\"✗ Private key not found: {key_path}\")\n        print(\"  Set SNOWFLAKE_PRIVATE_KEY_PATH environment variable\")\n        duckdb_auth = {'method': 'none'}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-create-secret-r",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R -i duckdb_auth\nR__duckdb_secret\n# Create DuckDB Snowflake secret (Local IDE only)\n\nif (duckdb_auth$method == \"none\") {\n    cat(\"DuckDB direct Snowflake connection not configured.\\n\")\n    cat(\"\\nFor Workspace Notebooks: Use Python Bridge (Section 7.3.1)\\n\")\n    cat(\"For Local IDE: Configure key-pair auth in previous cell\\n\")\n} else if (duckdb_auth$method == \"keypair\") {\n    cat(\"Creating Snowflake secret with key-pair auth...\\n\")\n    cat(\"  Account:\", duckdb_auth$account, \"\\n\")\n    cat(\"  User:\", duckdb_auth$user, \"\\n\")\n    \n    secret_sql <- sprintf(\"\nCREATE OR REPLACE SECRET snowflake_secret (\n    TYPE snowflake,\n    ACCOUNT '%s',\n    USER '%s',\n    DATABASE '%s',\n    WAREHOUSE '%s',\n    AUTH_TYPE 'key_pair',\n    PRIVATE_KEY '%s'\n)\",\n        duckdb_auth$account,\n        duckdb_auth$user,\n        duckdb_auth$database,\n        duckdb_auth$warehouse,\n        gsub(\"'\", \"''\", duckdb_auth$private_key)\n    )\n    \n    tryCatch({\n        dbExecute(duckdb_con, secret_sql)\n        cat(\"✓ Key-pair secret created successfully\\n\")\n        cat(\"\\nRun next cell to attach Snowflake database\\n\")\n    }, error = function(e) {\n        cat(\"✗ Error:\", conditionMessage(e), \"\\n\")\n    })\n}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-attach-snowflake",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__duckdb_attach\n# Attach Snowflake as a catalog in DuckDB (Local IDE only)\n# For Workspace Notebooks, skip to Section 7.3.1 Python Bridge\n\nif (!exists(\"duckdb_auth\") || is.null(duckdb_auth)) {\n    cat(\"Skipping - use Python Bridge (Section 7.3.1) for Workspace Notebooks\\n\")\n} else {\n    cat(\"Attaching Snowflake database...\\n\")\n    \n    tryCatch({\n        dbExecute(duckdb_con, \"ATTACH '' AS sf (TYPE snowflake, SECRET snowflake_secret, READ_ONLY)\")\n        cat(\"✓ Snowflake attached as 'sf' catalog\\n\\n\")\n        \n        # List schemas\n        cat(\"Available schemas:\\n\")\n        schemas <- dbGetQuery(duckdb_con, \n            \"SELECT schema_name FROM sf.information_schema.schemata ORDER BY schema_name LIMIT 10\")\n        rprint(schemas)\n        \n    }, error = function(e) {\n        cat(\"✗ Error:\", conditionMessage(e), \"\\n\")\n        cat(\"\\nTroubleshooting:\\n\")\n        cat(\"  - Verify account name format (e.g., 'xy12345' not full URL)\\n\")\n        cat(\"  - Check private key is valid PKCS8 format\\n\")\n        cat(\"  - Ensure public key is registered in Snowflake\\n\")\n    })\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-workspace-alternative",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 7.3.1 Python Bridge (Recommended for Workspace Notebooks)\n\nThis approach queries Snowflake via Python Snowpark and transfers data to R/DuckDB for local analysis.\n\n**Why use this?**\n- Works reliably in both Workspace Notebooks and Local IDEs\n- Uses the existing Snowpark session authentication\n- No additional credential setup needed\n- Ideal for dplyr/dbplyr workflows on fetched data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-python-bridge",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Python Bridge: Query Snowflake, analyze with R/DuckDB\n# This example uses SNOWFLAKE_SAMPLE_DATA (available to all Snowflake accounts)\n\nif session is None:\n    print(\"No session available. Run Section 3.1 first!\")\nelse:\n    import rpy2.robjects as ro\n    from rpy2.robjects import pandas2ri\n    from rpy2.robjects.conversion import localconverter\n    \n    # Query TPCH sample data\n    # Note: Uses SNOWFLAKE_SAMPLE_DATA which is available to all accounts\n    query = \"\"\"\n        SELECT O_ORDERKEY, O_CUSTKEY, O_ORDERSTATUS, \n               O_TOTALPRICE::FLOAT as O_TOTALPRICE, \n               O_ORDERDATE\n        FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\n        WHERE O_ORDERDATE >= '1995-01-01'\n        LIMIT 10000\n    \"\"\"\n    \n    print(\"Querying SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS...\")\n    orders_df = session.sql(query).to_pandas()\n    \n    # Transfer to R environment\n    with localconverter(ro.default_converter + pandas2ri.converter):\n        r_orders = ro.conversion.py2rpy(orders_df)\n        ro.globalenv['sf_orders'] = r_orders\n    \n    print(f\"✓ Transferred {len(orders_df):,} rows to R as 'sf_orders'\")\n    print(f\"  Columns: {', '.join(orders_df.columns)}\")\n    print(\"\\nUse %%R cells to analyze with dplyr:\")\n    print(\"  library(dplyr)\")\n    print(\"  sf_orders %>% group_by(O_ORDERSTATUS) %>% summarize(n = n())\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-bridge-analysis",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__duckdb_analyze\n# Analyze data transferred via Python bridge\n# This cell works in Workspace Notebooks without DuckDB\n\nif (exists(\"sf_orders\")) {\n    library(dplyr)\n    \n    result <- sf_orders %>%\n        mutate(order_year = format(O_ORDERDATE, \"%Y\")) %>%\n        group_by(order_year, O_ORDERSTATUS) %>%\n        summarise(\n            orders = n(),\n            total_value = sum(O_TOTALPRICE, na.rm = TRUE),\n            .groups = \"drop\"\n        ) %>%\n        arrange(order_year, desc(orders))\n    \n    cat(\"Order analysis using dplyr (via Python bridge):\\n\")\n    rprint(result)\n} else {\n    cat(\"Note: sf_orders not found. Run the Python bridge cell above first.\\n\")\n    cat(\"Or use the DuckDB approach if in local IDE.\\n\")\n}"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-query-examples",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.4 Query Snowflake with dplyr\n\nThe recommended pattern for dplyr workflows:\n1. **Direct SQL** for fetching data from Snowflake\n2. **Cache locally** in DuckDB for iterative analysis\n3. **Use dplyr** on local cached tables\n\n**Important**: Use 2-part table names (`sf.schema.table`) when database is set in the secret."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-direct-sql",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__dplyr_direct_sql\n# Pattern 1: Direct SQL Query to Snowflake\n# Best for: simple aggregations, data exploration\n\ncat(\"Direct SQL query to Snowflake...\\n\\n\")\n\ntryCatch({\n    # Note: Use sf.schema.table format (database set in secret)\n    customers <- dbGetQuery(duckdb_con, \"\n        SELECT C_MKTSEGMENT, COUNT(*) as customers, ROUND(AVG(C_ACCTBAL), 2) as avg_balance\n        FROM sf.tpch_sf1.customer\n        GROUP BY C_MKTSEGMENT\n        ORDER BY customers DESC\n    \")\n    \n    cat(\"Customer analysis by market segment:\\n\")\n    rprint(customers)\n    \n}, error = function(e) {\n    cat(\"Error:\", conditionMessage(e), \"\\n\")\n    cat(\"Note: Ensure database is set in secret (e.g., SNOWFLAKE_SAMPLE_DATA)\\n\")\n})"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-dplyr-query",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__dplyr_cache_local\n# Pattern 2: Cache locally, then use dplyr\n# Best for: complex analysis, joins, window functions\n\ncat(\"Caching Snowflake data locally for dplyr analysis...\\n\\n\")\n\ntryCatch({\n    # Cache data from Snowflake into local DuckDB table\n    dbExecute(duckdb_con, \"\n        CREATE OR REPLACE TABLE orders_local AS \n        SELECT O_ORDERKEY, O_CUSTKEY, O_ORDERSTATUS, O_TOTALPRICE, O_ORDERDATE, O_ORDERPRIORITY\n        FROM sf.tpch_sf1.orders\n        LIMIT 50000\n    \")\n    cat(\"✓ Cached 50,000 orders locally\\n\\n\")\n    \n    # Now use dplyr on the local table - fast and featureful!\n    analysis <- tbl(duckdb_con, \"orders_local\") %>%\n        mutate(\n            order_year = year(O_ORDERDATE),\n            priority = case_when(\n                O_ORDERPRIORITY %in% c(\"1-URGENT\", \"2-HIGH\") ~ \"High\",\n                TRUE ~ \"Normal\"\n            )\n        ) %>%\n        group_by(order_year, O_ORDERSTATUS, priority) %>%\n        summarise(\n            orders = n(),\n            total_value = sum(O_TOTALPRICE, na.rm = TRUE),\n            .groups = 'drop'\n        ) %>%\n        arrange(order_year, O_ORDERSTATUS) %>%\n        collect()\n    \n    cat(\"Order analysis with dplyr:\\n\")\n    rprint(head(analysis, 10))\n    \n}, error = function(e) {\n    cat(\"Error:\", conditionMessage(e), \"\\n\")\n})"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-cache-pattern",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.5 Advanced Patterns\n\nAdditional patterns for DuckDB + Snowflake workflows."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-cache-example",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__dplyr_join_tables\n# Pattern 3: Join local cached tables\n# Best for: combining reference data with transactional data\n\ncat(\"Caching reference tables for joins...\\n\\n\")\n\ntryCatch({\n    # Cache reference tables\n    dbExecute(duckdb_con, \"CREATE OR REPLACE TABLE nations AS SELECT * FROM sf.tpch_sf1.nation\")\n    dbExecute(duckdb_con, \"CREATE OR REPLACE TABLE regions AS SELECT * FROM sf.tpch_sf1.region\")\n    cat(\"✓ Reference tables cached\\n\\n\")\n    \n    # Join using dplyr\n    result <- tbl(duckdb_con, \"nations\") %>%\n        inner_join(tbl(duckdb_con, \"regions\"), by = c(\"N_REGIONKEY\" = \"R_REGIONKEY\")) %>%\n        select(nation = N_NAME, region = R_NAME) %>%\n        arrange(region, nation) %>%\n        collect()\n    \n    cat(\"Nations by Region:\\n\")\n    rprint(result)\n    \n}, error = function(e) {\n    cat(\"Error:\", conditionMessage(e), \"\\n\")\n})"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-window-functions",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__dplyr_window_funcs\n# Pattern 4: Window functions with dplyr\n# Best for: rankings, running totals, lead/lag analysis\n\ncat(\"Window functions on cached data...\\n\\n\")\n\ntryCatch({\n    # Ensure orders_local exists from previous cell\n    if (!dbExistsTable(duckdb_con, \"orders_local\")) {\n        dbExecute(duckdb_con, \"\n            CREATE TABLE orders_local AS \n            SELECT O_ORDERKEY, O_CUSTKEY, O_ORDERSTATUS, O_TOTALPRICE, O_ORDERDATE\n            FROM sf.tpch_sf1.orders\n            LIMIT 50000\n        \")\n    }\n    \n    # Top customers by total order value\n    top_customers <- tbl(duckdb_con, \"orders_local\") %>%\n        group_by(O_CUSTKEY) %>%\n        summarise(\n            orders = n(),\n            total_value = sum(O_TOTALPRICE, na.rm = TRUE),\n            avg_order = mean(O_TOTALPRICE, na.rm = TRUE),\n            .groups = 'drop'\n        ) %>%\n        arrange(desc(total_value)) %>%\n        head(10) %>%\n        collect()\n    \n    cat(\"Top 10 customers by total order value:\\n\")\n    rprint(top_customers)\n    \n}, error = function(e) {\n    cat(\"Error:\", conditionMessage(e), \"\\n\")\n})"
    },
    {
      "cell_type": "markdown",
      "id": "duckdb-cleanup",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7.6 Cleanup\n\nClose the DuckDB connection when done."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duckdb-cleanup-cell",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__duckdb_cleanup\n# Cleanup: Disconnect from DuckDB\n# Uncomment to close connection\n\n# dbDisconnect(duckdb_con)\n# cat(\"DuckDB connection closed\\n\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n## 8. Iceberg Integration via Horizon Catalog (Experimental)\n\nThis section covers accessing Snowflake-managed Iceberg tables from external query engines using the Horizon Catalog REST API.\n\n**Status**: 🔬 Experimental - Some features may require additional configuration.\n\n### Key Concepts\n\n- **Horizon Catalog**: Snowflake's implementation of the Apache Iceberg REST API\n- **Vended Credentials**: Temporary S3/Azure/GCS credentials provided by the catalog\n- **Authentication**: JWT/OAuth flow using the same key-pair as Snowflake\n\n### What Works Now\n- ✅ JWT generation and token exchange\n- ✅ Catalog metadata queries (list namespaces, tables)\n- ✅ Table metadata retrieval (schema, partition specs, snapshots)\n- ✅ DuckDB iceberg extension ATTACH\n\n### In Progress\n- ⚠️ Full DuckDB query support (requires vended credentials)\n- ⚠️ PyIceberg integration",
      "id": "03aca72e-6b2d-4426-8c31-aedc0993cfb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8.1 Create an Iceberg Table (One-Time Setup)\n\nFirst, let's create a Snowflake-managed Iceberg table from the TPCH Nation data for testing.\n\n**Prerequisites:**\n- An external volume configured with S3/Azure/GCS storage\n- `CREATE ICEBERG TABLE` privilege\n\n**Configuration:** Update `iceberg.external_volume` in `notebook_config.yaml`",
      "id": "dd6d1586-c995-4256-8c32-9476c19fde2a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "# Create Iceberg table from TPCH Nation data\n# Uses ICEBERG_CONFIG from notebook_config.yaml\n\nif session is None:\n    print(\"No session available. Run Section 3.1 first!\")\nelse:\n    iceberg_table = ICEBERG_CONFIG.get('test_table_name', 'NATION_ICEBERG')\n    external_vol = ICEBERG_CONFIG.get('external_volume', '<YOUR_EXTERNAL_VOLUME>')\n    target_db = ENV_CONFIG.get('database', 'SIMON')\n    target_schema = ENV_CONFIG.get('schema', 'PUBLIC')\n    \n    print(f\"Creating Iceberg table: {target_db}.{target_schema}.{iceberg_table}\")\n    print(f\"External volume: {external_vol}\")\n    \n    if '<YOUR_' in external_vol:\n        print(\"\\n⚠️  Configure iceberg.external_volume in notebook_config.yaml first!\")\n    else:\n        create_sql = f\"\"\"\n        CREATE OR REPLACE ICEBERG TABLE {target_db}.{target_schema}.{iceberg_table}\n            EXTERNAL_VOLUME = '{external_vol}'\n            CATALOG = 'SNOWFLAKE'\n            BASE_LOCATION = '{iceberg_table.lower()}/'\n        AS\n        SELECT \n            N_NATIONKEY::NUMBER(38,0) as N_NATIONKEY,\n            N_NAME::STRING as N_NAME,\n            N_REGIONKEY::NUMBER(38,0) as N_REGIONKEY,\n            N_COMMENT::STRING as N_COMMENT\n        FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION\n        \"\"\"\n        \n        try:\n            session.sql(create_sql).collect()\n            print(f\"✓ Iceberg table created: {target_db}.{target_schema}.{iceberg_table}\")\n            \n            # Verify\n            count = session.sql(f\"SELECT COUNT(*) FROM {target_db}.{target_schema}.{iceberg_table}\").collect()[0][0]\n            print(f\"  Row count: {count}\")\n        except Exception as e:\n            print(f\"✗ Error: {e}\")",
      "outputs": [],
      "execution_count": null,
      "id": "b8faf8af-39e9-4e59-88ce-8b0f6e893f5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8.2 Horizon Catalog Authentication\n\nGenerate a JWT and exchange it for an access token to authenticate with the Horizon Catalog REST API.\n\n**Note**: This uses key-pair authentication. You need a private key configured (see Section 4.2).\n\nThe `ENV_CONFIG` from Section 3.1 provides the account and user details.",
      "id": "864680f1-fc49-4ec5-8ba1-63b8e83551ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Horizon Catalog auth helper (uses ENV_CONFIG from Section 3.1)\nimport jwt\nimport time\nimport hashlib\nimport base64\nimport requests\nfrom datetime import datetime\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_horizon_access_token(account, user, private_key_path, role=None):\n    \"\"\"Generate JWT and exchange for access token.\"\"\"\n    import os\n    \n    # Load private key\n    key_path = os.path.expanduser(private_key_path)\n    with open(key_path, 'rb') as f:\n        private_key = serialization.load_pem_private_key(f.read(), password=None)\n    \n    # Get public key fingerprint\n    public_key = private_key.public_key()\n    public_key_bytes = public_key.public_bytes(\n        encoding=serialization.Encoding.DER,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    fingerprint = hashlib.sha256(public_key_bytes).digest()\n    fingerprint_b64 = base64.b64encode(fingerprint).decode()\n    \n    # Build JWT\n    qualified_user = f\"{account.upper()}.{user.upper()}\"\n    now = int(time.time())\n    payload = {\n        \"iss\": f\"{qualified_user}.SHA256:{fingerprint_b64}\",\n        \"sub\": qualified_user,\n        \"iat\": now,\n        \"exp\": now + 3600,\n    }\n    \n    token = jwt.encode(payload, private_key, algorithm=\"RS256\")\n    \n    # Exchange JWT for access token\n    url = f\"https://{account}.snowflakecomputing.com/polaris/api/catalog/v1/oauth/tokens\"\n    \n    data = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n        \"assertion\": token,\n    }\n    if role:\n        data[\"scope\"] = f\"PRINCIPAL_ROLE:{role}\"\n    \n    resp = requests.post(url, data=data, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"})\n    \n    if resp.status_code == 200:\n        return resp.json().get(\"access_token\")\n    else:\n        raise Exception(f\"Token exchange failed: {resp.status_code} - {resp.text}\")\n\n# Get access token using settings from ENV_CONFIG\nif ENV_TYPE == 'local':\n    key_path = ENV_CONFIG.get('private_key_path', '~/.ssh/snowflake_rsa_key.p8')\n    try:\n        ACCESS_TOKEN = get_horizon_access_token(\n            account=ENV_CONFIG['account'],\n            user=ENV_CONFIG['user'],\n            private_key_path=key_path\n        )\n        print(f\"✓ Access token obtained (first 20 chars): {ACCESS_TOKEN[:20]}...\")\n    except Exception as e:\n        print(f\"✗ Error getting access token: {e}\")\n        ACCESS_TOKEN = None\nelse:\n    print(\"Note: Horizon Catalog auth requires key-pair (local IDE mode)\")\n    print(\"In Workspace, use the DuckDB Snowflake extension approach from Section 7\")\n    ACCESS_TOKEN = None",
      "id": "6db07766-776c-4435-a857-40aa62aecfda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8.3 Query Horizon Catalog Metadata\n\nUse the REST API to list namespaces and tables in the Iceberg catalog.",
      "id": "45cdca4c-2c80-439d-bcd7-f8a7c5768b3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Query Horizon Catalog API\nimport requests\nimport json\nimport os\n\n# Configuration - update these for your environment\nHORIZON_CONFIG = {\n    'account': os.environ.get('SNOWFLAKE_ACCOUNT', '<YOUR_ORG>-<YOUR_ACCOUNT>'),\n    'user': os.environ.get('SNOWFLAKE_USER', '<YOUR_USER>'),\n    'role': 'SYSADMIN',\n    'database': '<YOUR_DATABASE>',  # Database with Iceberg tables\n    'private_key_path': os.environ.get(\n        'SNOWFLAKE_PRIVATE_KEY_PATH',\n        '~/.snowflake/keys/rsa_key.p8'\n    )\n}\n\ndef query_horizon_catalog(endpoint, access_token):\n    \"\"\"Query the Horizon Catalog REST API.\"\"\"\n    account = HORIZON_CONFIG['account']\n    base_url = f\"https://{account}.snowflakecomputing.com/polaris/api/catalog/v1\"\n    \n    response = requests.get(\n        f\"{base_url}/{endpoint}\",\n        headers={\n            'Authorization': f'Bearer {access_token}',\n            'Content-Type': 'application/json'\n        }\n    )\n    \n    if response.status_code == 200:\n        return response.json()\n    else:\n        return {'error': response.status_code, 'message': response.text}\n\n# Example usage (uncomment when configured):\n\"\"\"\n# Get access token\naccess_token = get_horizon_access_token(\n    HORIZON_CONFIG['account'],\n    HORIZON_CONFIG['user'],\n    HORIZON_CONFIG['role'],\n    HORIZON_CONFIG['private_key_path']\n)\n\n# List namespaces (schemas)\ndatabase = HORIZON_CONFIG['database']\nnamespaces = query_horizon_catalog(f\"{database}/namespaces\", access_token)\nprint(\"Namespaces:\", json.dumps(namespaces, indent=2))\n\n# List tables in PUBLIC schema\ntables = query_horizon_catalog(f\"{database}/namespaces/PUBLIC/tables\", access_token)\nprint(\"Tables:\", json.dumps(tables, indent=2))\n\n# Get table metadata\ntable_meta = query_horizon_catalog(\n    f\"{database}/namespaces/PUBLIC/tables/NATION_ICEBERG\",\n    access_token\n)\nprint(\"Table metadata:\", json.dumps(table_meta, indent=2)[:500])\n\"\"\"\n\nprint(\"Horizon Catalog query functions defined.\")\nprint(\"Configure HORIZON_CONFIG and uncomment example code to test.\")",
      "id": "0e6d4d3a-c73e-4d82-b1e2-e6de4cc02e71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8.4 DuckDB Iceberg Extension (Experimental)\n\nDuckDB's iceberg extension can connect to REST catalogs including Snowflake Horizon.\n\n**Current Status**: Catalog attachment works, but data queries may fail due to vended credentials limitations. See design document for workarounds.",
      "id": "2f208a8d-d7be-4e06-bdf9-37a7eeb1044b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__iceberg_duckdb\n# DuckDB Iceberg Integration (Experimental)\n# \n# NOTE: This demonstrates attaching to Horizon Catalog.\n# Full query support may require vended credentials configuration.\n\nlibrary(DBI)\nlibrary(duckdb)\n\n# Connect to DuckDB\niceberg_con <- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\n\n# Install and load iceberg extension\ndbExecute(iceberg_con, \"INSTALL iceberg\")\ndbExecute(iceberg_con, \"LOAD iceberg\")\n\ncat(\"Iceberg extension loaded\\n\")\n\n# Configuration - update for your environment\n# Uncomment and configure when you have an access token\n\"\"\"\nACCOUNT <- 'MYORG-MYACCOUNT'\nDATABASE <- 'MY_DATABASE'\nACCESS_TOKEN <- '<your_access_token_from_section_8.2>'\n\n# Attach to Horizon Catalog\nattach_sql <- sprintf(\n    \\\"ATTACH '%s' AS horizon (\n        TYPE ICEBERG,\n        ENDPOINT 'https://%s.snowflakecomputing.com/polaris/api/catalog',\n        TOKEN '%s'\n    )\\\",\n    DATABASE,\n    ACCOUNT,\n    ACCESS_TOKEN\n)\n\ndbExecute(iceberg_con, attach_sql)\ncat('Horizon Catalog attached\\\\n')\n\n# List tables (this works!)\ntables <- dbGetQuery(iceberg_con, \n    \\\"SELECT * FROM duckdb_tables() WHERE database_name = 'horizon'\\\")\nprint(tables)\n\n# Query table (may fail with current vended credentials limitations)\n# tryCatch({\n#     result <- dbGetQuery(iceberg_con, 'SELECT * FROM horizon.PUBLIC.NATION_ICEBERG')\n#     print(result)\n# }, error = function(e) {\n#     cat('Query failed - see design doc for workarounds:\\\\n')\n#     cat(conditionMessage(e), '\\\\n')\n# })\n\"\"\"\n\ncat(\"\\nDuckDB Iceberg demo configured.\\n\")\ncat(\"Configure variables and uncomment code to test.\\n\")",
      "id": "f7c10a85-51bb-488e-812f-2ee00e84ea52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8.5 Recommended Alternative: Snowflake + DuckDB Hybrid\n\nUntil full Iceberg REST catalog support is available, use the working DuckDB Snowflake extension approach from Section 7:\n\n1. **Query Snowflake via ADBC** (using DuckDB Snowflake extension)\n2. **Cache results locally** in DuckDB\n3. **Use dplyr/dbplyr** on the local cache\n\nThis provides the same benefits (local processing, R ecosystem) with full support today.",
      "id": "63661b21-0a26-48ac-b343-13d430484784"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "%%R\nR__iceberg_hybrid\n# Hybrid Approach: Best of Both Worlds\n# Use the working DuckDB + Snowflake pattern for Iceberg-like benefits\n\n# Assuming DuckDB connection from Section 7 is active (duckdb_con)\n# If not, re-run Section 7.3\n\n# Example: Query Snowflake Iceberg table, cache locally\n# (Even though it's an Iceberg table in Snowflake, query via SQL works!)\n\n\"\"\"\n# Query the Iceberg table via standard Snowflake SQL\ndbExecute(duckdb_con, \\\"\n    CREATE OR REPLACE TABLE nation_iceberg_local AS \n    SELECT * FROM sf.PUBLIC.NATION_ICEBERG\n\\\")\n\n# Now use dplyr on the local cache\nlibrary(dplyr)\nlibrary(dbplyr)\n\ntbl(duckdb_con, 'nation_iceberg_local') %>%\n    group_by(N_REGIONKEY) %>%\n    summarise(\n        nations = n(),\n        sample_name = first(N_NAME)\n    ) %>%\n    collect() %>%\n    print()\n\"\"\"\n\ncat(\"Hybrid pattern example ready.\\n\")\ncat(\"Uncomment code after running Section 7 DuckDB setup.\\n\")",
      "id": "33ff8779-28b0-4f0e-9abe-70401f1844a0"
    },
    {
      "cell_type": "markdown",
      "id": "e96125a8-ea1b-4e54-bacb-f106bf2cee79",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| `ModuleNotFoundError: No module named 'rpy2'` | Run Section 1.2 to install rpy2 |\n| `R.version.string` returns error | Verify PATH and R_HOME are set correctly |\n| ADBC `auth_pat` error | Ensure PAT was created and stored in `SNOWFLAKE_PAT` |\n| Network policy error | PAT may need `MINS_TO_BYPASS_NETWORK_POLICY_REQUIREMENT` |\n| `adbcsnowflake` not found | Ensure setup script ran with `--adbc` flag |\n| Setup script fails | Check `setup_r.log` for detailed error messages |\n| `r_sf_con` not found | Run `get_snowflake_connection()` to create connection |\n\n### Run Full Diagnostics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f31ce02-b7b8-4cc0-a2fb-38e3b45bfedd",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Comprehensive diagnostic check\nfrom r_helpers import print_diagnostics\nprint_diagnostics()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdef786-f0f5-474c-8faa-b6af52a56ef1",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Environment diagnostics\nimport os\nimport shutil\n\nprint(\"Quick Environment Check:\")\nprint(f\"  R_HOME: {os.environ.get('R_HOME', 'NOT SET')}\")\nprint(f\"  R binary: {shutil.which('R') or 'NOT FOUND'}\")\nprint(f\"  SNOWFLAKE_ACCOUNT: {os.environ.get('SNOWFLAKE_ACCOUNT', 'NOT SET')}\")\nprint(f\"  SNOWFLAKE_PAT: {'SET' if os.environ.get('SNOWFLAKE_PAT') else 'NOT SET'}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf4f8e4-213b-40a0-9e54-1a97208b7646",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# View setup log if something went wrong\n# !tail -50 setup_r.log"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}