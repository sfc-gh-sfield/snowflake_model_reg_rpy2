{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# Snowpark Scala in Workspace Notebooks (Prototype)\n",
        "\n",
        "This notebook demonstrates running **Scala** and **Snowpark Scala** within a\n",
        "Snowflake Workspace Notebook using a `%%scala` cell magic powered by JPype.\n",
        "\n",
        "**Architecture:** Python kernel → JPype (JNI) → JVM (in-process) → Scala REPL → Snowpark\n",
        "\n",
        "---\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. Installation & Configuration\n",
        "2. Basic Scala Execution\n",
        "3. Python <-> Scala Interop\n",
        "4. Snowpark Scala Session\n",
        "   - 4.6 Snowpark DataFrame Interop (SQL Plan Transfer)\n",
        "5. Diagnostics\n",
        "6. Spark Connect for Scala (opt-in)"
      ],
      "id": "0f3beaaa-2458-4fd6-8e39-5673811180d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "---\n",
        "## 1. Installation & Configuration\n",
        "\n",
        "### 1.1 Install JDK, Scala, and Snowpark JAR\n",
        "\n",
        "Run the setup script. This takes ~2-4 minutes on first run (installs\n",
        "OpenJDK 17, Scala 2.12, Ammonite, Snowpark JAR via micromamba + coursier).\n",
        "\n",
        "On subsequent runs it detects what is already installed and skips those steps."
      ],
      "id": "8840d2db-7fd9-499f-b4e7-be0a14ccd6a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "!bash setup_scala_environment.sh"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e9ef0223-78de-41e9-8dcf-1b0345acaa11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 1.2 Configure Python Environment & Register Scala Magics\n",
        "\n",
        "This cell:\n",
        "1. Sets `JAVA_HOME` and `PATH`\n",
        "2. Installs JPype1 into the kernel venv (if needed)\n",
        "3. Starts the JVM in-process with the Scala + Snowpark classpath\n",
        "4. Initialises the Scala REPL (Ammonite-lite or IMain)\n",
        "5. Registers `%%scala` (cell) and `%scala` (line) magics"
      ],
      "id": "fcc56566-6bf3-4042-8c6d-6a33189d7ddd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import setup_scala_environment\n",
        "\n",
        "result = setup_scala_environment()\n",
        "\n",
        "print(f\"Success:          {result['success']}\")\n",
        "print(f\"Java version:     {result['java_version']}\")\n",
        "print(f\"Scala version:    {result['scala_version']}\")\n",
        "print(f\"Interpreter type: {result['interpreter_type']}\")\n",
        "print(f\"JVM started:      {result['jvm_started']}\")\n",
        "print(f\"Magic registered: {result['magic_registered']}\")\n",
        "if result.get('jvm_options'):\n",
        "    print(f\"JVM options:      {result['jvm_options']}\")\n",
        "\n",
        "if result['errors']:\n",
        "    print(f\"\\nErrors:\")\n",
        "    for err in result['errors']:\n",
        "        print(f\"  - {err}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3ac75c11-867e-459c-afd0-671d922d531b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 1.3 Verify Scala Execution"
      ],
      "id": "88520cd8-94f5-428f-a126-ebaa8000f9c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "println(s\"Hello from Scala ${util.Properties.versionString}\")\n",
        "println(s\"Java: ${System.getProperty(\"java.version\")}\")\n",
        "println(s\"OS: ${System.getProperty(\"os.name\")}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b0384195-33ac-4287-ad54-9ec088e5ed4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 1.4 Single-line Scala (`%scala`)\n",
        "\n",
        "The `%scala` line magic runs a single Scala expression inline — handy\n",
        "for quick checks without a full `%%scala` cell.\n",
        "\n",
        "**Note:** IPython expands `${expr}` in line magic arguments before Scala\n",
        "sees them. Use `$varName` (no braces) or string concatenation for `%scala`.\n",
        "For `s\"${...}\"` interpolation, use `%%scala` cells instead."
      ],
      "id": "196910f6-b275-4256-ba45-811b3d1e143c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%scala println(\"Quick check: 2 + 2 = \" + (2 + 2))\n",
        "%scala val v = util.Properties.versionString; println(s\"Scala $v\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c31cd85b-b383-431d-b2f3-3a3580080e86"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "---\n",
        "## 2. Basic Scala Execution\n",
        "\n",
        "State persists across `%%scala` cells — vals, defs, imports, and classes\n",
        "defined in one cell are available in the next."
      ],
      "id": "66d06ef6-0520-4717-9c32-4e2a090ce4a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Define a value\n",
        "val greeting = \"Hello from Snowflake Workspace Notebook!\"\n",
        "println(greeting)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "406153c2-452c-472c-a0fc-c6abc5c0d8c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Previous cell's 'greeting' is still in scope\n",
        "println(s\"Greeting length: ${greeting.length}\")\n",
        "\n",
        "// Define a function\n",
        "def factorial(n: Int): BigInt = if (n <= 1) 1 else n * factorial(n - 1)\n",
        "\n",
        "println(s\"10! = ${factorial(10)}\")\n",
        "println(s\"20! = ${factorial(20)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0cf1e12f-0352-49c0-9bcf-5aefee64cec3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Collections and functional programming\n",
        "val numbers = (1 to 10).toList\n",
        "val squares = numbers.map(n => n * n)\n",
        "val evenSquares = squares.filter(_ % 2 == 0)\n",
        "\n",
        "println(s\"Numbers:      $numbers\")\n",
        "println(s\"Squares:      $squares\")\n",
        "println(s\"Even squares: $evenSquares\")\n",
        "println(s\"Sum:          ${evenSquares.sum}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "321193c5-15aa-41c6-8835-ae369b9503c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Case classes and pattern matching\n",
        "case class Employee(name: String, department: String, salary: Double)\n",
        "\n",
        "val employees = List(\n",
        "  Employee(\"Alice\", \"Engineering\", 120000),\n",
        "  Employee(\"Bob\", \"Engineering\", 115000),\n",
        "  Employee(\"Carol\", \"Data Science\", 130000),\n",
        "  Employee(\"Dave\", \"Data Science\", 125000),\n",
        "  Employee(\"Eve\", \"Product\", 110000)\n",
        ")\n",
        "\n",
        "val byDept = employees.groupBy(_.department).map {\n",
        "  case (dept, emps) => (dept, emps.map(_.salary).sum / emps.size)\n",
        "}\n",
        "\n",
        "byDept.toList.sortBy(-_._2).foreach {\n",
        "  case (dept, avgSalary) =>\n",
        "    println(f\"  $dept%-20s $$${avgSalary}%,.0f\")\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7011b6fc-2150-4bbb-aa73-56184f6ff345"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "---\n",
        "## 3. Python ↔ Scala Interoperability\n",
        "\n",
        "### 3.1 Push values from Python to Scala"
      ],
      "id": "c1f90058-6c28-4859-9318-9e44eaec5ace"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import push_to_scala\n",
        "\n",
        "# Push a string and number from Python into the Scala interpreter\n",
        "push_to_scala(\"pythonMessage\", \"Hello from Python!\")\n",
        "push_to_scala(\"pythonNumber\", 42)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a0a24cad-f979-4d8e-b2a9-bd44ce9b147e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Access the variables pushed from Python\n",
        "println(s\"From Python: $pythonMessage\")\n",
        "println(s\"Number: $pythonNumber\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "47ea01ef-d796-4d84-9f15-478fda57f254"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 3.2 Pull values from Scala to Python"
      ],
      "id": "0eb1cf56-c921-473b-bbae-269d889cb5b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "val scalaResult = (1 to 100).sum\n",
        "println(s\"Sum 1..100 = $scalaResult\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9def987a-f78b-4072-9a10-f867c6d7db4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import pull_from_scala\n",
        "\n",
        "value = pull_from_scala(\"scalaResult\")\n",
        "print(f\"Pulled from Scala: {value} (type: {type(value).__name__})\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e9fa108a-97ad-4163-ba6e-7a69d8e3362c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 3.3 Magic flags: `-i` and `-o` (like rpy2's `%%R`)\n",
        "\n",
        "Instead of calling `push_to_scala()` / `pull_from_scala()` explicitly,\n",
        "you can use **`-i`** and **`-o`** flags directly on the `%%scala` line —\n",
        "the same pattern as rpy2's `%%R -i` / `%%R -o`."
      ],
      "id": "95bc9e84-8ed4-4e38-bb71-70c0ba6524bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Define Python variables to push into Scala\n",
        "py_limit = 50\n",
        "py_label = \"first N numbers\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "20e12bd0-bf95-4646-8830-b4b5178e78af"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala -i py_limit,py_label -o scala_sum --time\n",
        "// py_limit and py_label were pushed from Python automatically\n",
        "val n = py_limit.asInstanceOf[Int]\n",
        "val scala_sum = (1 to n).sum\n",
        "println(s\"Sum of $py_label (1 to $n) = $scala_sum\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6fc52b4f-116b-4f3b-b7f8-b2126e591842"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# scala_sum was pulled back into Python automatically via -o\n",
        "print(f\"Back in Python: scala_sum = {scala_sum} (type: {type(scala_sum).__name__})\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "68d38245-3467-4ad2-9b24-57c3c25f71ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "---\n",
        "## 4. Snowpark Scala Session\n",
        "\n",
        "### 4.1 Inject credentials\n",
        "\n",
        "Extract credentials from the Python session and the SPCS container token,\n",
        "then set them as Java System properties for Scala.\n",
        "\n",
        "Inside a Workspace Notebook, the SPCS OAuth token at `/snowflake/session/token`\n",
        "is used automatically. No PAT is needed."
      ],
      "id": "c25246f3-b3d1-443e-b752-b57c6de254c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from snowflake.snowpark.context import get_active_session\n",
        "from scala_helpers import inject_session_credentials\n",
        "\n",
        "session = get_active_session()\n",
        "creds = inject_session_credentials(session)\n",
        "\n",
        "print(\"Credentials injected as Java System properties:\")\n",
        "for k, v in creds.items():\n",
        "    if 'TOKEN' in k:\n",
        "        print(f\"  {k}: {'SET (' + str(len(v)) + ' chars)' if v else 'NOT SET'}\")\n",
        "    else:\n",
        "        print(f\"  {k}: {v}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7c4b34bc-e7f2-4fd5-9d80-5893569e661e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 4.2 Preview Session Code"
      ],
      "id": "2b3707f3-59b0-46a0-8f3d-fda8cf4267b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import create_snowpark_scala_session_code\n",
        "\n",
        "code = create_snowpark_scala_session_code()\n",
        "print(code)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2b48f15f-bebe-4a29-ad55-9ac1d5d9c2ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 4.3 Create Snowpark Scala Session"
      ],
      "id": "95cb0de0-a4da-452b-b0c9-721becf4296c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "import com.snowflake.snowpark._\n",
        "import com.snowflake.snowpark.functions._\n",
        "\n",
        "def prop(k: String): String = {\n",
        "  val v = System.getProperty(k)\n",
        "  require(v != null, s\"System property '$k' not set. Run inject_session_credentials() first.\")\n",
        "  v\n",
        "}\n",
        "\n",
        "val sfSession = Session.builder.configs(Map(\n",
        "  \"URL\"           -> prop(\"SNOWFLAKE_URL\"),\n",
        "  \"USER\"          -> prop(\"SNOWFLAKE_USER\"),\n",
        "  \"ROLE\"          -> prop(\"SNOWFLAKE_ROLE\"),\n",
        "  \"DB\"            -> prop(\"SNOWFLAKE_DATABASE\"),\n",
        "  \"SCHEMA\"        -> prop(\"SNOWFLAKE_SCHEMA\"),\n",
        "  \"WAREHOUSE\"     -> prop(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "  \"TOKEN\"         -> prop(\"SNOWFLAKE_TOKEN\"),\n",
        "  \"AUTHENTICATOR\" -> prop(\"SNOWFLAKE_AUTH_TYPE\")\n",
        ")).create\n",
        "\n",
        "println(\"Snowpark Scala session created!\")\n",
        "val _user = sfSession.sql(\"SELECT CURRENT_USER()\").collect()(0).getString(0)\n",
        "val _role = sfSession.sql(\"SELECT CURRENT_ROLE()\").collect()(0).getString(0)\n",
        "val _db = sfSession.sql(\"SELECT CURRENT_DATABASE()\").collect()(0).getString(0)\n",
        "println(s\"  User:      ${_user}\")\n",
        "println(s\"  Role:      ${_role}\")\n",
        "println(s\"  Database:  ${_db}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7fc41089-b943-4417-b64f-6e24b16181f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 4.4 Query Snowflake from Scala"
      ],
      "id": "19685f01-5e64-45dc-b911-78861be7e253"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Basic query\n",
        "sfSession.sql(\"SELECT CURRENT_USER() AS user, CURRENT_ROLE() AS role, CURRENT_WAREHOUSE() AS warehouse\").show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "97037545-381b-463a-8207-e4607b412946"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// DataFrame operations\n",
        "val df = sfSession.sql(\"SELECT 'Scala' AS language, 'Snowpark' AS framework, CURRENT_TIMESTAMP() AS ts\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d935526a-c4be-423d-8e0b-49bdae5a2a98"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Show available tables\n",
        "sfSession.sql(\"SHOW TABLES LIMIT 5\").show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ef5b3cda-da5e-4c2d-b190-cd3486497c58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "### 4.5 Cross-language Data Sharing\n",
        "\n",
        "The Python and Scala Snowpark sessions are **separate connections**, so\n",
        "`TEMPORARY TABLE`s (which are session-scoped) are not visible across them.\n",
        "Use a `TRANSIENT TABLE` instead, and drop it when done."
      ],
      "id": "f7fc8ea1-0420-4087-959f-1c91cd01887f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Python: create a transient table (visible across sessions, unlike TEMPORARY)\n",
        "session.sql(\"\"\"\n",
        "    CREATE OR REPLACE TRANSIENT TABLE scala_demo (\n",
        "        id INT, name STRING, value DOUBLE\n",
        "    ) AS\n",
        "    SELECT column1, column2, column3 FROM VALUES\n",
        "        (1, 'alpha', 10.5),\n",
        "        (2, 'beta', 20.3),\n",
        "        (3, 'gamma', 30.7)\n",
        "\"\"\").collect()\n",
        "print(\"Transient table 'scala_demo' created from Python\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2068eae3-b27c-4227-a24e-a0100419097f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Scala: read the temp table created by Python\n",
        "val demo = sfSession.table(\"scala_demo\")\n",
        "demo.show()\n",
        "\n",
        "// Compute something\n",
        "val total = demo.select(sum(col(\"VALUE\"))).collect()(0).getDouble(0)\n",
        "println(s\"Total value: $total\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a2ee403b-f30b-4d2f-be0a-71949dfe44d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 4.6 Snowpark DataFrame Interop (SQL Plan Transfer)\n",
        "\n",
        "When `-i` or `-o` reference a **Snowpark DataFrame**, the magic\n",
        "auto-detects it and transfers the underlying **SQL query plan** instead\n",
        "of materialising data through temp tables.\n",
        "\n",
        "- **Python → Scala (`-i`):** extracts `df.queries['queries'][-1]` and\n",
        "  creates a Scala `sfSession.sql(...)` DataFrame.\n",
        "- **Scala → Python (`-o`):** extracts the SQL from the Scala DataFrame\n",
        "  and creates a Python `session.sql(...)` DataFrame.\n",
        "\n",
        "No data is copied — only the SQL string crosses the bridge."
      ],
      "id": "060daa20-1586-4efe-a576-af09b9894f37"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Python → Scala: push a Snowpark Python DataFrame into Scala\n",
        "py_df = session.sql(\"\"\"\n",
        "    SELECT column1 AS id, column2 AS name, column3 AS score\n",
        "    FROM VALUES (1, 'Alice', 95.0), (2, 'Bob', 87.5), (3, 'Carol', 92.0)\n",
        "\"\"\")\n",
        "print(f\"Python DataFrame SQL plan:\\n  {py_df.queries['queries'][-1][:80]}...\")\n",
        "py_df.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "78ccd6a2-8b79-46f8-a9ba-db8c85b26efa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala -i py_df -o result_df --time\n",
        "// py_df is now a Scala Snowpark DataFrame (created from the SQL plan)\n",
        "// We can apply Scala Snowpark transformations on it\n",
        "val result_df = py_df.filter(col(\"SCORE\") > 90.0).select(col(\"NAME\"), col(\"SCORE\"))\n",
        "result_df.show()\n",
        "println(\"Filtered to scores > 90 — this DataFrame will be pulled back to Python via -o\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "18844587-e46c-4b40-a352-03f3a2db13ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Scala → Python: result_df was pulled back automatically via -o\n",
        "# It's now a Snowpark Python DataFrame — use it with Python Snowpark API\n",
        "print(f\"Type: {type(result_df).__name__}\")\n",
        "result_df.show()\n",
        "\n",
        "# Convert to Pandas if needed\n",
        "pandas_df = result_df.to_pandas()\n",
        "print(f\"\\nAs Pandas:\\n{pandas_df}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "58e0cf85-3cdd-4301-b343-303d1d4a517a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cleanup any interop views (if the view-based fallback was used)\n",
        "from scala_helpers import cleanup_interop_views\n",
        "dropped = cleanup_interop_views()\n",
        "if dropped:\n",
        "    print(f\"Cleaned up {dropped} interop view(s)\")\n",
        "else:\n",
        "    print(\"No interop views to clean up (SQL plan transfer was used)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2d487949-ace2-4b44-87f6-2e2d4312a221"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cleanup: drop the transient demo table\n",
        "session.sql(\"DROP TABLE IF EXISTS scala_demo\").collect()\n",
        "print(\"Table 'scala_demo' dropped\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a800e719-39b9-4096-b3e9-5ed7c75e4de5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "---\n",
        "## 5. Diagnostics\n",
        "\n",
        "Run the diagnostics check to verify the JVM, Scala interpreter,\n",
        "Snowpark classpath, credentials, and disk space are all healthy."
      ],
      "id": "73107849-00bf-46bb-9862-3961486647c3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import print_diagnostics\n",
        "print_diagnostics()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d93b3e11-4e48-4cb2-ab97-157b54d228a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "## 6. Spark Connect for Scala (opt-in)\n",
        "\n",
        "When `spark_connect.enabled: true` is set in `scala_packages.yaml`, the\n",
        "installer also sets up Snowpark Connect for Scala. This starts a local\n",
        "Spark Connect gRPC server (Python proxy consuming SPCS OAuth) and makes\n",
        "a Scala `SparkSession` available in `%%scala` cells as `spark`.\n",
        "\n",
        "**Two APIs, one notebook:**\n",
        "- `sfSession.sql(...)` — Snowpark Scala (direct JDBC, full Snowpark API)\n",
        "- `spark.sql(...)` — Spark SQL via Spark Connect (Spark DataFrame API)\n",
        "\n",
        "Both use the same JVM. No PAT needed — auth flows through the Python proxy."
      ],
      "id": "e403aabb-85e9-49fb-a800-fb2f9ac0ac87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 6.1 Start Spark Connect Server + Scala SparkSession"
      ],
      "id": "a038a410-1f78-4a71-b391-3ce24ad2a070"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from scala_helpers import setup_spark_connect\n",
        "\n",
        "sc_result = setup_spark_connect()\n",
        "print(f\"Spark Connect: {'ready' if sc_result['success'] else 'FAILED'}\")\n",
        "print(f\"Server port:   {sc_result['server_port']}\")\n",
        "if sc_result.get('pyspark_version'):\n",
        "    print(f\"PySpark:       {sc_result['pyspark_version']}\")\n",
        "if sc_result['errors']:\n",
        "    for err in sc_result['errors']:\n",
        "        print(f\"  ERROR: {err}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7d6fda87-0e10-44e5-9fe7-80ab901cab8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 6.2 Scala Spark SQL via %%scala"
      ],
      "id": "5cf42aa4-5a90-4d0e-9080-9740bff1ef34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Spark SQL via the local Spark Connect proxy\n",
        "val df = spark.sql(\"SELECT 1 AS id, 'hello from Scala Spark' AS msg\")\n",
        "df.show()\n",
        "println(\"Spark Connect: working\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d15e1156-9206-41ff-840d-6de8ffbc14be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 6.3 Query Snowflake Tables via Spark SQL"
      ],
      "id": "dc4d15a9-b7a8-4e1d-9dff-294dfe1b6765"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Query Snowflake metadata via Spark SQL (goes through the gRPC proxy)\n",
        "// Note: CURRENT_ROLE() is not supported by Spark Connect proxy\n",
        "spark.sql(\"SELECT CURRENT_USER() AS user\").show()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "  SELECT TABLE_SCHEMA, TABLE_NAME, ROW_COUNT\n",
        "  FROM INFORMATION_SCHEMA.TABLES\n",
        "  WHERE ROW_COUNT IS NOT NULL\n",
        "  ORDER BY ROW_COUNT DESC\n",
        "  LIMIT 5\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "01206c05-10a8-46be-b0af-3943c535eba7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 6.4 Side-by-side: Snowpark Scala vs Spark SQL\n",
        "\n",
        "Both APIs work in the same notebook. Use whichever fits your use case:\n",
        "- `sfSession` for native Snowpark Scala operations (DataFrame API, UDFs, stored procs)\n",
        "- `spark` for Spark SQL syntax (familiar to Spark users, broader SQL dialect)"
      ],
      "id": "177a7479-6cb9-4e0a-af53-95f4b175730c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Snowpark Scala: native Snowpark API\n",
        "println(\"=== Snowpark Scala (sfSession) ===\")\n",
        "sfSession.sql(\"SELECT 'snowpark' AS api, CURRENT_USER() AS user\").show()\n",
        "\n",
        "// Spark SQL: via gRPC proxy\n",
        "println(\"=== Spark SQL (spark) ===\")\n",
        "spark.sql(\"SELECT 'spark_connect' AS api, CURRENT_USER() AS user\").show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "438fcd59-36bb-4265-99a2-406a146cad22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "### 6.5 Interop: Snowpark Python writes, Scala Spark reads\n",
        "\n",
        "This demonstrates cross-language data sharing. Python writes a transient\n",
        "table via Snowpark, and Scala reads it via Spark SQL through the local proxy."
      ],
      "id": "62ce0f6b-7880-47e6-8651-004f34ed6c5e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Python: write a transient table\n",
        "session.sql(\"\"\"\n",
        "    CREATE OR REPLACE TRANSIENT TABLE _SPARK_CONNECT_DEMO AS\n",
        "    SELECT column1 AS id, column2 AS source FROM VALUES\n",
        "        (1, 'from_snowpark_python'),\n",
        "        (2, 'from_snowpark_python')\n",
        "\"\"\").collect()\n",
        "print(\"Snowpark Python: wrote _SPARK_CONNECT_DEMO\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "10df331b-b003-4b7b-ba51-56cc33bc4f36"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "%%scala\n",
        "// Scala Spark reads the table written by Python\n",
        "val interop_df = spark.sql(\"SELECT * FROM _SPARK_CONNECT_DEMO\")\n",
        "interop_df.show()\n",
        "println(\"Scala Spark: read table written by Snowpark Python\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0e1f47d1-fc11-4cf6-b3a4-c7f3fcdcd67f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cleanup\n",
        "session.sql(\"DROP TABLE IF EXISTS _SPARK_CONNECT_DEMO\").collect()\n",
        "print(\"Cleanup: _SPARK_CONNECT_DEMO dropped\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7d9775da-8ef3-4356-bc99-28b7bcefe3cc"
    }
  ]
}
