{
  "metadata": {
    "kernelspec": {
      "name": "jupyter",
      "display_name": "Jupyter Notebook"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f3beaaa-2458-4fd6-8e39-5673811180d8",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Snowpark Scala in Workspace Notebooks (Prototype)\n\nThis notebook demonstrates running **Scala** and **Snowpark Scala** within a\nSnowflake Workspace Notebook using a `%%scala` cell magic powered by JPype.\n\n**Architecture:** Python kernel → JPype (JNI) → JVM (in-process) → Scala REPL → Snowpark\n\n---\n\n## Contents\n\n1. Installation & Configuration\n2. Basic Scala Execution\n3. Python <-> Scala Interop\n4. Snowpark Scala Session\n   - 4.6 Snowpark DataFrame Interop (SQL Plan Transfer)\n5. Diagnostics\n6. Spark Connect for Scala (opt-in)"
    },
    {
      "cell_type": "markdown",
      "id": "8840d2db-7fd9-499f-b4e7-be0a14ccd6a6",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "---\n## 1. Installation & Configuration\n\n### 1.1 Install JDK, Scala, and Snowpark JAR\n\nRun the setup script. This takes ~2-4 minutes on first run (installs\nOpenJDK 17, Scala 2.12, Ammonite, Snowpark JAR via micromamba + coursier).\n\nOn subsequent runs it detects what is already installed and skips those steps."
    },
    {
      "cell_type": "code",
      "id": "e9ef0223-78de-41e9-8dcf-1b0345acaa11",
      "metadata": {
        "language": "python"
      },
      "source": "!bash setup_scala_environment.sh",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "fcc56566-6bf3-4042-8c6d-6a33189d7ddd",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.2 Configure Python Environment & Register Scala Magics\n\nThis cell:\n1. Sets `JAVA_HOME` and `PATH`\n2. Installs JPype1 into the kernel venv (if needed)\n3. Starts the JVM in-process with the Scala + Snowpark classpath\n4. Initialises the Scala REPL (Ammonite-lite or IMain)\n5. Registers `%%scala` (cell) and `%scala` (line) magics"
    },
    {
      "cell_type": "code",
      "id": "3ac75c11-867e-459c-afd0-671d922d531b",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import setup_scala_environment\n\nresult = setup_scala_environment()\n\nprint(f\"Success:          {result['success']}\")\nprint(f\"Java version:     {result['java_version']}\")\nprint(f\"Scala version:    {result['scala_version']}\")\nprint(f\"Interpreter type: {result['interpreter_type']}\")\nprint(f\"JVM started:      {result['jvm_started']}\")\nprint(f\"Magic registered: {result['magic_registered']}\")\nif result.get('jvm_options'):\n    print(f\"JVM options:      {result['jvm_options']}\")\n\nif result['errors']:\n    print(f\"\\nErrors:\")\n    for err in result['errors']:\n        print(f\"  - {err}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "88520cd8-94f5-428f-a126-ebaa8000f9c6",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.3 Verify Scala Execution"
    },
    {
      "cell_type": "code",
      "id": "b0384195-33ac-4287-ad54-9ec088e5ed4d",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\nprintln(s\"Hello from Scala ${util.Properties.versionString}\")\nprintln(s\"Java: ${System.getProperty(\"java.version\")}\")\nprintln(s\"OS: ${System.getProperty(\"os.name\")}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "196910f6-b275-4256-ba45-811b3d1e143c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 1.4 Single-line Scala (`%scala`)\n\nThe `%scala` line magic runs a single Scala expression inline — handy\nfor quick checks without a full `%%scala` cell.\n\n**Note:** IPython expands `${expr}` in line magic arguments before Scala\nsees them. Use `$varName` (no braces) or string concatenation for `%scala`.\nFor `s\"${...}\"` interpolation, use `%%scala` cells instead."
    },
    {
      "cell_type": "code",
      "id": "c31cd85b-b383-431d-b2f3-3a3580080e86",
      "metadata": {
        "language": "python"
      },
      "source": "%scala println(\"Quick check: 2 + 2 = \" + (2 + 2))\n%scala val v = util.Properties.versionString; println(s\"Scala $v\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "66d06ef6-0520-4717-9c32-4e2a090ce4a1",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "---\n## 2. Basic Scala Execution\n\nState persists across `%%scala` cells — vals, defs, imports, and classes\ndefined in one cell are available in the next."
    },
    {
      "cell_type": "code",
      "id": "406153c2-452c-472c-a0fc-c6abc5c0d8c1",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Define a value\nval greeting = \"Hello from Snowflake Workspace Notebook!\"\nprintln(greeting)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "0cf1e12f-0352-49c0-9bcf-5aefee64cec3",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Previous cell's 'greeting' is still in scope\nprintln(s\"Greeting length: ${greeting.length}\")\n\n// Define a function\ndef factorial(n: Int): BigInt = if (n <= 1) 1 else n * factorial(n - 1)\n\nprintln(s\"10! = ${factorial(10)}\")\nprintln(s\"20! = ${factorial(20)}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "321193c5-15aa-41c6-8835-ae369b9503c6",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Collections and functional programming\nval numbers = (1 to 10).toList\nval squares = numbers.map(n => n * n)\nval evenSquares = squares.filter(_ % 2 == 0)\n\nprintln(s\"Numbers:      $numbers\")\nprintln(s\"Squares:      $squares\")\nprintln(s\"Even squares: $evenSquares\")\nprintln(s\"Sum:          ${evenSquares.sum}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7011b6fc-2150-4bbb-aa73-56184f6ff345",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Case classes and pattern matching\ncase class Employee(name: String, department: String, salary: Double)\n\nval employees = List(\n  Employee(\"Alice\", \"Engineering\", 120000),\n  Employee(\"Bob\", \"Engineering\", 115000),\n  Employee(\"Carol\", \"Data Science\", 130000),\n  Employee(\"Dave\", \"Data Science\", 125000),\n  Employee(\"Eve\", \"Product\", 110000)\n)\n\nval byDept = employees.groupBy(_.department).map {\n  case (dept, emps) => (dept, emps.map(_.salary).sum / emps.size)\n}\n\nbyDept.toList.sortBy(-_._2).foreach {\n  case (dept, avgSalary) =>\n    println(f\"  $dept%-20s $$${avgSalary}%,.0f\")\n}",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c1f90058-6c28-4859-9318-9e44eaec5ace",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "---\n## 3. Python ↔ Scala Interoperability\n\n### 3.1 Push values from Python to Scala"
    },
    {
      "cell_type": "code",
      "id": "a0a24cad-f979-4d8e-b2a9-bd44ce9b147e",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import push_to_scala\n\n# Push a string and number from Python into the Scala interpreter\npush_to_scala(\"pythonMessage\", \"Hello from Python!\")\npush_to_scala(\"pythonNumber\", 42)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "47ea01ef-d796-4d84-9f15-478fda57f254",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Access the variables pushed from Python\nprintln(s\"From Python: $pythonMessage\")\nprintln(s\"Number: $pythonNumber\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "0eb1cf56-c921-473b-bbae-269d889cb5b9",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 3.2 Pull values from Scala to Python"
    },
    {
      "cell_type": "code",
      "id": "9def987a-f78b-4072-9a10-f867c6d7db4c",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\nval scalaResult = (1 to 100).sum\nprintln(s\"Sum 1..100 = $scalaResult\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e9fa108a-97ad-4163-ba6e-7a69d8e3362c",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import pull_from_scala\n\nvalue = pull_from_scala(\"scalaResult\")\nprint(f\"Pulled from Scala: {value} (type: {type(value).__name__})\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "95bc9e84-8ed4-4e38-bb71-70c0ba6524bc",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 3.3 Magic flags: `-i` and `-o` (like rpy2's `%%R`)\n\nInstead of calling `push_to_scala()` / `pull_from_scala()` explicitly,\nyou can use **`-i`** and **`-o`** flags directly on the `%%scala` line —\nthe same pattern as rpy2's `%%R -i` / `%%R -o`."
    },
    {
      "cell_type": "code",
      "id": "20e12bd0-bf95-4646-8830-b4b5178e78af",
      "metadata": {
        "language": "python"
      },
      "source": "# Define Python variables to push into Scala\npy_limit = 50\npy_label = \"first N numbers\"",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "6fc52b4f-116b-4f3b-b7f8-b2126e591842",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala -i py_limit,py_label -o scala_sum --time\n// py_limit and py_label were pushed from Python automatically\nval n = py_limit.asInstanceOf[Int]\nval scala_sum = (1 to n).sum\nprintln(s\"Sum of $py_label (1 to $n) = $scala_sum\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "68d38245-3467-4ad2-9b24-57c3c25f71ba",
      "metadata": {
        "language": "python"
      },
      "source": "# scala_sum was pulled back into Python automatically via -o\nprint(f\"Back in Python: scala_sum = {scala_sum} (type: {type(scala_sum).__name__})\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c25246f3-b3d1-443e-b752-b57c6de254c6",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "---\n## 4. Snowpark Scala Session\n\n### 4.1 Inject credentials\n\nExtract credentials from the Python session and the SPCS container token,\nthen set them as Java System properties for Scala.\n\nInside a Workspace Notebook, the SPCS OAuth token at `/snowflake/session/token`\nis used automatically. No PAT is needed."
    },
    {
      "cell_type": "code",
      "id": "7c4b34bc-e7f2-4fd5-9d80-5893569e661e",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.context import get_active_session\nfrom scala_helpers import inject_session_credentials\n\nsession = get_active_session()\ncreds = inject_session_credentials(session)\n\nprint(\"Credentials injected as Java System properties:\")\nfor k, v in creds.items():\n    if 'TOKEN' in k:\n        print(f\"  {k}: {'SET (' + str(len(v)) + ' chars)' if v else 'NOT SET'}\")\n    else:\n        print(f\"  {k}: {v}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "2b3707f3-59b0-46a0-8f3d-fda8cf4267b7",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4.2 Preview Session Code"
    },
    {
      "cell_type": "code",
      "id": "2b48f15f-bebe-4a29-ad55-9ac1d5d9c2ae",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import create_snowpark_scala_session_code\n\ncode = create_snowpark_scala_session_code()\nprint(code)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "95cb0de0-a4da-452b-b0c9-721becf4296c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4.3 Create Snowpark Scala Session"
    },
    {
      "cell_type": "code",
      "id": "7fc41089-b943-4417-b64f-6e24b16181f7",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\nimport com.snowflake.snowpark._\nimport com.snowflake.snowpark.functions._\n\ndef prop(k: String): String = {\n  val v = System.getProperty(k)\n  require(v != null, s\"System property '$k' not set. Run inject_session_credentials() first.\")\n  v\n}\n\nval sfSession = Session.builder.configs(Map(\n  \"URL\"           -> prop(\"SNOWFLAKE_URL\"),\n  \"USER\"          -> prop(\"SNOWFLAKE_USER\"),\n  \"ROLE\"          -> prop(\"SNOWFLAKE_ROLE\"),\n  \"DB\"            -> prop(\"SNOWFLAKE_DATABASE\"),\n  \"SCHEMA\"        -> prop(\"SNOWFLAKE_SCHEMA\"),\n  \"WAREHOUSE\"     -> prop(\"SNOWFLAKE_WAREHOUSE\"),\n  \"TOKEN\"         -> prop(\"SNOWFLAKE_TOKEN\"),\n  \"AUTHENTICATOR\" -> prop(\"SNOWFLAKE_AUTH_TYPE\")\n)).create\n\nprintln(\"Snowpark Scala session created!\")\nval _user = sfSession.sql(\"SELECT CURRENT_USER()\").collect()(0).getString(0)\nval _role = sfSession.sql(\"SELECT CURRENT_ROLE()\").collect()(0).getString(0)\nval _db = sfSession.sql(\"SELECT CURRENT_DATABASE()\").collect()(0).getString(0)\nprintln(s\"  User:      ${_user}\")\nprintln(s\"  Role:      ${_role}\")\nprintln(s\"  Database:  ${_db}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "19685f01-5e64-45dc-b911-78861be7e253",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4.4 Query Snowflake from Scala"
    },
    {
      "cell_type": "code",
      "id": "97037545-381b-463a-8207-e4607b412946",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Basic query\nsfSession.sql(\"SELECT CURRENT_USER() AS user, CURRENT_ROLE() AS role, CURRENT_WAREHOUSE() AS warehouse\").show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "d935526a-c4be-423d-8e0b-49bdae5a2a98",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// DataFrame operations\nval df = sfSession.sql(\"SELECT 'Scala' AS language, 'Snowpark' AS framework, CURRENT_TIMESTAMP() AS ts\")\ndf.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "ef5b3cda-da5e-4c2d-b190-cd3486497c58",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Show available tables\nsfSession.sql(\"SHOW TABLES LIMIT 5\").show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f7fc8ea1-0420-4087-959f-1c91cd01887f",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### 4.5 Cross-language Data Sharing\n\nThe Python and Scala Snowpark sessions are **separate connections**, so\n`TEMPORARY TABLE`s (which are session-scoped) are not visible across them.\nUse a `TRANSIENT TABLE` instead, and drop it when done."
    },
    {
      "cell_type": "code",
      "id": "2068eae3-b27c-4227-a24e-a0100419097f",
      "metadata": {
        "language": "python"
      },
      "source": "# Python: create a transient table (visible across sessions, unlike TEMPORARY)\nsession.sql(\"\"\"\n    CREATE OR REPLACE TRANSIENT TABLE scala_demo (\n        id INT, name STRING, value DOUBLE\n    ) AS\n    SELECT column1, column2, column3 FROM VALUES\n        (1, 'alpha', 10.5),\n        (2, 'beta', 20.3),\n        (3, 'gamma', 30.7)\n\"\"\").collect()\nprint(\"Transient table 'scala_demo' created from Python\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "a2ee403b-f30b-4d2f-be0a-71949dfe44d0",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Scala: read the temp table created by Python\nval demo = sfSession.table(\"scala_demo\")\ndemo.show()\n\n// Compute something\nval total = demo.select(sum(col(\"VALUE\"))).collect()(0).getDouble(0)\nprintln(s\"Total value: $total\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "060daa20-1586-4efe-a576-af09b9894f37",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 4.6 Snowpark DataFrame Interop (SQL Plan Transfer)\n\nWhen `-i` or `-o` reference a **Snowpark DataFrame**, the magic\nauto-detects it and transfers the underlying **SQL query plan** instead\nof materialising data through temp tables.\n\n- **Python → Scala (`-i`):** extracts `df.queries['queries'][-1]` and\n  creates a Scala `sfSession.sql(...)` DataFrame.\n- **Scala → Python (`-o`):** extracts the SQL from the Scala DataFrame\n  and creates a Python `session.sql(...)` DataFrame.\n\nNo data is copied — only the SQL string crosses the bridge."
    },
    {
      "cell_type": "code",
      "id": "78ccd6a2-8b79-46f8-a9ba-db8c85b26efa",
      "metadata": {
        "language": "python"
      },
      "source": "# Python → Scala: push a Snowpark Python DataFrame into Scala\npy_df = session.sql(\"\"\"\n    SELECT column1 AS id, column2 AS name, column3 AS score\n    FROM VALUES (1, 'Alice', 95.0), (2, 'Bob', 87.5), (3, 'Carol', 92.0)\n\"\"\")\nprint(f\"Python DataFrame SQL plan:\\n  {py_df.queries['queries'][-1][:80]}...\")\npy_df.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "18844587-e46c-4b40-a352-03f3a2db13ea",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala -i py_df -o result_df --time\n// py_df is now a Scala Snowpark DataFrame (created from the SQL plan)\n// We can apply Scala Snowpark transformations on it\nval result_df = py_df.filter(col(\"SCORE\") > 90.0).select(col(\"NAME\"), col(\"SCORE\"))\nresult_df.show()\nprintln(\"Filtered to scores > 90 — this DataFrame will be pulled back to Python via -o\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "58e0cf85-3cdd-4301-b343-303d1d4a517a",
      "metadata": {
        "language": "python"
      },
      "source": "# Scala → Python: result_df was pulled back automatically via -o\n# It's now a Snowpark Python DataFrame — use it with Python Snowpark API\nprint(f\"Type: {type(result_df).__name__}\")\nresult_df.show()\n\n# Convert to Pandas if needed\npandas_df = result_df.to_pandas()\nprint(f\"\\nAs Pandas:\\n{pandas_df}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "2d487949-ace2-4b44-87f6-2e2d4312a221",
      "metadata": {
        "language": "python"
      },
      "source": "# Cleanup any interop views (if the view-based fallback was used)\nfrom scala_helpers import cleanup_interop_views\ndropped = cleanup_interop_views()\nif dropped:\n    print(f\"Cleaned up {dropped} interop view(s)\")\nelse:\n    print(\"No interop views to clean up (SQL plan transfer was used)\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "a800e719-39b9-4096-b3e9-5ed7c75e4de5",
      "metadata": {
        "language": "python"
      },
      "source": "# Cleanup: drop the transient demo table\nsession.sql(\"DROP TABLE IF EXISTS scala_demo\").collect()\nprint(\"Table 'scala_demo' dropped\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "73107849-00bf-46bb-9862-3961486647c3",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "---\n## 5. Diagnostics\n\nRun the diagnostics check to verify the JVM, Scala interpreter,\nSnowpark classpath, credentials, and disk space are all healthy."
    },
    {
      "cell_type": "code",
      "id": "d93b3e11-4e48-4cb2-ab97-157b54d228a7",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import print_diagnostics\nprint_diagnostics()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e403aabb-85e9-49fb-a800-fb2f9ac0ac87",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Spark Connect for Scala (opt-in)\n\nWhen `spark_connect.enabled: true` is set in `scala_packages.yaml`, the\ninstaller also sets up Snowpark Connect for Scala. This starts a local\nSpark Connect gRPC server (Python proxy consuming SPCS OAuth) and makes\na Scala `SparkSession` available in `%%scala` cells as `spark`.\n\n**Two APIs, one notebook:**\n- `sfSession.sql(...)` — Snowpark Scala (direct JDBC, full Snowpark API)\n- `spark.sql(...)` — Spark SQL via Spark Connect (Spark DataFrame API)\n\nBoth use the same JVM. No PAT needed — auth flows through the Python proxy."
    },
    {
      "cell_type": "markdown",
      "id": "a038a410-1f78-4a71-b391-3ce24ad2a070",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 6.1 Start Spark Connect Server + Scala SparkSession"
    },
    {
      "cell_type": "code",
      "id": "7d6fda87-0e10-44e5-9fe7-80ab901cab8f",
      "metadata": {
        "language": "python"
      },
      "source": "from scala_helpers import setup_spark_connect\n\nsc_result = setup_spark_connect()\nprint(f\"Spark Connect: {'ready' if sc_result['success'] else 'FAILED'}\")\nprint(f\"Server port:   {sc_result['server_port']}\")\nif sc_result.get('pyspark_version'):\n    print(f\"PySpark:       {sc_result['pyspark_version']}\")\nif sc_result['errors']:\n    for err in sc_result['errors']:\n        print(f\"  ERROR: {err}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "5cf42aa4-5a90-4d0e-9080-9740bff1ef34",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 6.2 Scala Spark SQL via %%scala"
    },
    {
      "cell_type": "code",
      "id": "d15e1156-9206-41ff-840d-6de8ffbc14be",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Spark SQL via the local Spark Connect proxy\nval df = spark.sql(\"SELECT 1 AS id, 'hello from Scala Spark' AS msg\")\ndf.show()\nprintln(\"Spark Connect: working\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "dc4d15a9-b7a8-4e1d-9dff-294dfe1b6765",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 6.3 Query Snowflake Tables via Spark SQL"
    },
    {
      "cell_type": "code",
      "id": "01206c05-10a8-46be-b0af-3943c535eba7",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Query Snowflake metadata via Spark SQL (goes through the gRPC proxy)\n// Note: CURRENT_ROLE() is not supported by Spark Connect proxy\nspark.sql(\"SELECT CURRENT_USER() AS user\").show()\n\nspark.sql(\"\"\"\n  SELECT TABLE_SCHEMA, TABLE_NAME, ROW_COUNT\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE ROW_COUNT IS NOT NULL\n  ORDER BY ROW_COUNT DESC\n  LIMIT 5\n\"\"\").show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "177a7479-6cb9-4e0a-af53-95f4b175730c",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 6.4 Side-by-side: Snowpark Scala vs Spark SQL\n\nBoth APIs work in the same notebook. Use whichever fits your use case:\n- `sfSession` for native Snowpark Scala operations (DataFrame API, UDFs, stored procs)\n- `spark` for Spark SQL syntax (familiar to Spark users, broader SQL dialect)"
    },
    {
      "cell_type": "code",
      "id": "438fcd59-36bb-4265-99a2-406a146cad22",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Snowpark Scala: native Snowpark API\nprintln(\"=== Snowpark Scala (sfSession) ===\")\nsfSession.sql(\"SELECT 'snowpark' AS api, CURRENT_USER() AS user\").show()\n\n// Spark SQL: via gRPC proxy\nprintln(\"=== Spark SQL (spark) ===\")\nspark.sql(\"SELECT 'spark_connect' AS api, CURRENT_USER() AS user\").show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "62ce0f6b-7880-47e6-8651-004f34ed6c5e",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "### 6.5 Interop: Snowpark Python writes, Scala Spark reads\n\nThis demonstrates cross-language data sharing. Python writes a transient\ntable via Snowpark, and Scala reads it via Spark SQL through the local proxy."
    },
    {
      "cell_type": "code",
      "id": "10df331b-b003-4b7b-ba51-56cc33bc4f36",
      "metadata": {
        "language": "python"
      },
      "source": "# Python: write a transient table\nsession.sql(\"\"\"\n    CREATE OR REPLACE TRANSIENT TABLE _SPARK_CONNECT_DEMO AS\n    SELECT column1 AS id, column2 AS source FROM VALUES\n        (1, 'from_snowpark_python'),\n        (2, 'from_snowpark_python')\n\"\"\").collect()\nprint(\"Snowpark Python: wrote _SPARK_CONNECT_DEMO\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "0e1f47d1-fc11-4cf6-b3a4-c7f3fcdcd67f",
      "metadata": {
        "language": "python"
      },
      "source": "%%scala\n// Scala Spark reads the table written by Python\nval interop_df = spark.sql(\"SELECT * FROM _SPARK_CONNECT_DEMO\")\ninterop_df.show()\nprintln(\"Scala Spark: read table written by Snowpark Python\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7d9775da-8ef3-4356-bc99-28b7bcefe3cc",
      "metadata": {
        "language": "python"
      },
      "source": "# Cleanup\nsession.sql(\"DROP TABLE IF EXISTS _SPARK_CONNECT_DEMO\").collect()\nprint(\"Cleanup: _SPARK_CONNECT_DEMO dropped\")",
      "outputs": [],
      "execution_count": null
    }
  ]
}