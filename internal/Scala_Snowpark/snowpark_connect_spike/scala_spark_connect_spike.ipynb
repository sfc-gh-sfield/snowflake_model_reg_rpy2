{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# Scala Spark Connect via Local Python Proxy (Path B)\n",
        "\n",
        "**Goal:** Run Scala Spark code in a Workspace Notebook by:\n",
        "1. Starting a local Spark Connect server in Python (consumes SPCS OAuth token)\n",
        "2. Connecting a Scala Spark client to `sc://localhost:15002` via JPype\n",
        "\n",
        "This avoids PAT — the Python proxy handles auth using the container's\n",
        "Snowpark session, and Scala just talks to localhost.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Python: start_session(is_daemon=False, remote_url=\"sc://localhost:15002\")\n",
        "   ↓  (consumes SPCS OAuth token via Snowpark session)\n",
        "Scala (JPype) → sc://localhost:15002 → Python proxy → Snowflake warehouse\n",
        "```\n",
        "\n",
        "**Self-contained:** This notebook installs everything it needs. It can run\n",
        "on a fresh container or one where other notebooks have already been executed.\n",
        "All installs are idempotent.\n",
        "\n",
        "---\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. [Setup: Install dependencies](#1)\n",
        "2. [Start local Spark Connect server](#2)\n",
        "3. [Verify PySpark via local server](#3)\n",
        "4. [Setup JVM + Scala Spark Connect client](#4)\n",
        "5. [Connect Scala to local server](#5)\n",
        "6. [Scala Spark SQL tests](#6)\n",
        "7. [Findings](#7)"
      ],
      "id": "b61bf119-a052-476c-8ef7-306241ee0c59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"1\"></a>\n",
        "## 1. Setup: Install dependencies\n",
        "\n",
        "All dependencies are installed from scratch. Each install is idempotent —\n",
        "safe to re-run if already installed from a prior notebook or previous run.\n",
        "\n",
        "We need:\n",
        "- `snowpark-connect[jdk]` + `pyspark` (PySpark + bundled JDK)\n",
        "- `JPype1` (Python-JVM bridge)\n",
        "- `coursier` (JVM dependency resolver, for fetching Spark Connect client JARs)\n",
        "- `spark-connect-client-jvm` Scala JAR (fetched via coursier)"
      ],
      "id": "14165fbd-4aa7-4fd6-97ce-eccebd6656a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import subprocess, sys, os, time, shutil, pathlib, socket\n",
        "\n",
        "def _pip_install(package, label=None):\n",
        "    \"\"\"Install a pip package if not already importable. Idempotent.\"\"\"\n",
        "    label = label or package\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n",
        "        capture_output=True, text=True, timeout=300\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(f\"  {label}: OK\")\n",
        "    else:\n",
        "        print(f\"  {label}: FAILED — {result.stderr[-200:]}\")\n",
        "\n",
        "print(\"=== Installing Python packages (idempotent) ===\")\n",
        "_pip_install(\"snowpark-connect[jdk]\", \"snowpark-connect\")\n",
        "_pip_install(\"pyspark==3.5.6\", \"pyspark\")\n",
        "_pip_install(\"JPype1\", \"JPype1\")\n",
        "\n",
        "import snowflake.snowpark_connect\n",
        "import pyspark\n",
        "import jpype\n",
        "print(f\"\\nsnowpark-connect: imported OK\")\n",
        "print(f\"PySpark version:  {pyspark.__version__}\")\n",
        "print(f\"JPype1 version:   {jpype.__version__}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f79494d8-d760-439f-ab5f-1f613bc6594b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# --- Find or install Java (idempotent) ---\n",
        "# Search order: already on PATH, snowpark-connect[jdk] bundled, micromamba\n",
        "java_search_paths = [\n",
        "    (\"PATH\", shutil.which(\"java\")),\n",
        "    (\"snowpark-connect[jdk]\", pathlib.Path(sys.prefix) / \"jdk\"),\n",
        "    (\"micromamba (Scala prototype)\", pathlib.Path.home() / \"micromamba\" / \"envs\" / \"jvm_env\"),\n",
        "]\n",
        "\n",
        "java_found = False\n",
        "for label, path in java_search_paths:\n",
        "    if path and (isinstance(path, str) or path.exists()):\n",
        "        if isinstance(path, pathlib.Path):\n",
        "            os.environ[\"JAVA_HOME\"] = str(path)\n",
        "            os.environ[\"PATH\"] = f\"{path}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
        "        elif not os.environ.get(\"JAVA_HOME\"):\n",
        "            os.environ[\"JAVA_HOME\"] = str(pathlib.Path(path).resolve().parent.parent)\n",
        "        java_found = True\n",
        "        print(f\"Java source: {label}\")\n",
        "        break\n",
        "\n",
        "if not java_found:\n",
        "    print(\"ERROR: No Java found. The [jdk] pip extra should have provided one.\")\n",
        "    print(\"Attempting micromamba install as fallback...\")\n",
        "    # Minimal JDK install via micromamba (same approach as Scala prototype)\n",
        "    subprocess.run(\n",
        "        \"curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj -C ~ bin/micromamba 2>/dev/null; \"\n",
        "        \"~/bin/micromamba create -y -n jvm_env -c conda-forge openjdk=17 -q\",\n",
        "        shell=True, timeout=120\n",
        "    )\n",
        "    mm_jdk = pathlib.Path.home() / \"micromamba\" / \"envs\" / \"jvm_env\"\n",
        "    if mm_jdk.exists():\n",
        "        os.environ[\"JAVA_HOME\"] = str(mm_jdk)\n",
        "        os.environ[\"PATH\"] = f\"{mm_jdk}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
        "        print(f\"Installed JDK via micromamba: {mm_jdk}\")\n",
        "    else:\n",
        "        print(\"FATAL: Could not install Java\")\n",
        "\n",
        "java_path = shutil.which(\"java\")\n",
        "print(f\"Java binary:  {java_path or 'NOT FOUND'}\")\n",
        "if java_path:\n",
        "    jv = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
        "    print(f\"Java version: {jv.stderr.splitlines()[0]}\")\n",
        "print(f\"JAVA_HOME:    {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n",
        "\n",
        "# --- Install coursier (idempotent) ---\n",
        "CS_DIR = os.path.expanduser(\"~/scala_jars\")\n",
        "os.makedirs(CS_DIR, exist_ok=True)\n",
        "cs_path = shutil.which(\"cs\") or os.path.join(CS_DIR, \"cs\")\n",
        "\n",
        "if not os.path.isfile(cs_path):\n",
        "    print(\"\\nInstalling coursier...\")\n",
        "    subprocess.run(\n",
        "        f\"curl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz \"\n",
        "        f\"| gzip -d > {cs_path} && chmod +x {cs_path}\",\n",
        "        shell=True, check=True, timeout=60\n",
        "    )\n",
        "    print(f\"Coursier installed: {cs_path}\")\n",
        "else:\n",
        "    print(f\"\\nCoursier already at: {cs_path}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "34713092-9227-4c24-bc79-0f4a86d6979e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"2\"></a>\n",
        "## 2. Start local Spark Connect server\n",
        "\n",
        "Use the `snowpark_connect` API to start a local Spark Connect gRPC\n",
        "server that listens on `localhost:15002`. This server consumes the\n",
        "Workspace's Snowpark session (SPCS OAuth) and proxies requests to\n",
        "Snowflake.\n",
        "\n",
        "The server runs in a background thread so the notebook stays interactive."
      ],
      "id": "36cae1e5-6ff7-4b5f-afe3-1097088d09e7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# First, write the config.toml that the server needs\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "def _safe(fn):\n",
        "    try:\n",
        "        v = fn()\n",
        "        return v.strip('\"') if v else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "conn_info = {\n",
        "    \"account\": _safe(lambda: session.sql(\"SELECT CURRENT_ACCOUNT()\").collect()[0][0]),\n",
        "    \"user\": _safe(lambda: session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]),\n",
        "    \"role\": _safe(session.get_current_role),\n",
        "    \"database\": _safe(session.get_current_database),\n",
        "    \"schema\": _safe(session.get_current_schema),\n",
        "    \"warehouse\": _safe(session.get_current_warehouse),\n",
        "    \"host\": os.environ.get(\"SNOWFLAKE_HOST\", \"\"),\n",
        "}\n",
        "\n",
        "spcs_token = \"\"\n",
        "if os.path.isfile(\"/snowflake/session/token\"):\n",
        "    with open(\"/snowflake/session/token\") as f:\n",
        "        spcs_token = f.read().strip()\n",
        "\n",
        "config_dir = pathlib.Path.home() / \".snowflake\"\n",
        "config_dir.mkdir(parents=True, exist_ok=True)\n",
        "config_file = config_dir / \"config.toml\"\n",
        "\n",
        "toml_content = f'''[connections.spark-connect]\n",
        "host = \"{conn_info['host']}\"\n",
        "account = \"{conn_info['account']}\"\n",
        "user = \"{conn_info['user']}\"\n",
        "token = \"{spcs_token}\"\n",
        "authenticator = \"oauth\"\n",
        "warehouse = \"{conn_info['warehouse']}\"\n",
        "database = \"{conn_info['database']}\"\n",
        "schema = \"{conn_info['schema']}\"\n",
        "role = \"{conn_info['role']}\"\n",
        "'''\n",
        "\n",
        "config_file.write_text(toml_content)\n",
        "config_file.chmod(0o600)\n",
        "print(f\"Config written to {config_file}\")\n",
        "print(f\"Account: {conn_info['account']}, User: {conn_info['user']}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a4ec9a18-ae9f-4f4a-bf20-8ca67ea7a05c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Explore the snowpark_connect API to find the right way to start a local server\n",
        "import snowflake.snowpark_connect as spc\n",
        "\n",
        "print(\"=== snowpark_connect top-level attributes ===\")\n",
        "for attr in sorted(dir(spc)):\n",
        "    if not attr.startswith(\"_\"):\n",
        "        obj = getattr(spc, attr)\n",
        "        print(f\"  {attr}: {type(obj).__name__}\")\n",
        "\n",
        "# Check for server/start_session APIs\n",
        "if hasattr(spc, 'server'):\n",
        "    print(\"\\n=== snowpark_connect.server attributes ===\")\n",
        "    for attr in sorted(dir(spc.server)):\n",
        "        if not attr.startswith(\"_\"):\n",
        "            print(f\"  {attr}: {type(getattr(spc.server, attr)).__name__}\")\n",
        "\n",
        "if hasattr(spc, 'client'):\n",
        "    print(\"\\n=== snowpark_connect.client attributes ===\")\n",
        "    for attr in sorted(dir(spc.client)):\n",
        "        if not attr.startswith(\"_\"):\n",
        "            print(f\"  {attr}: {type(getattr(spc.client, attr)).__name__}\")\n",
        "\n",
        "# Check for start_session specifically\n",
        "for mod_name in ['', '.server', '.client']:\n",
        "    try:\n",
        "        mod = eval(f'spc{mod_name}')\n",
        "        if hasattr(mod, 'start_session'):\n",
        "            import inspect\n",
        "            sig = inspect.signature(mod.start_session)\n",
        "            print(f\"\\n=== {f'spc{mod_name}'}.start_session signature ===\")\n",
        "            print(f\"  {sig}\")\n",
        "    except Exception:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "432a7b02-dbbc-4fb8-ac7b-599cdfd17dec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Start the local Spark Connect server\n",
        "# The exact API depends on what we found above. Try the known patterns:\n",
        "import threading\n",
        "\n",
        "SERVER_PORT = 15002\n",
        "server_started = threading.Event()\n",
        "server_error = [None]\n",
        "\n",
        "def start_local_server():\n",
        "    \"\"\"Start the Spark Connect server in a background thread.\"\"\"\n",
        "    try:\n",
        "        # Pattern 1: start_session with is_daemon=False\n",
        "        if hasattr(spc, 'start_session'):\n",
        "            spc.start_session(\n",
        "                is_daemon=False,\n",
        "                remote_url=f\"sc://localhost:{SERVER_PORT}\"\n",
        "            )\n",
        "        elif hasattr(spc, 'server') and hasattr(spc.server, 'start_session'):\n",
        "            spc.server.start_session(\n",
        "                is_daemon=False,\n",
        "                remote_url=f\"sc://localhost:{SERVER_PORT}\"\n",
        "            )\n",
        "        elif hasattr(spc, 'client') and hasattr(spc.client, 'start_session'):\n",
        "            spc.client.start_session(\n",
        "                remote_url=f\"sc://localhost:{SERVER_PORT}\"\n",
        "            )\n",
        "        else:\n",
        "            server_error[0] = \"No start_session API found\"\n",
        "    except Exception as e:\n",
        "        server_error[0] = str(e)\n",
        "\n",
        "# Start in background thread (server blocks)\n",
        "server_thread = threading.Thread(target=start_local_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for the server to start listening\n",
        "print(f\"Waiting for local Spark Connect server on localhost:{SERVER_PORT}...\")\n",
        "for attempt in range(30):\n",
        "    time.sleep(1)\n",
        "    if server_error[0]:\n",
        "        print(f\"Server failed: {server_error[0]}\")\n",
        "        break\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        s.settimeout(1)\n",
        "        s.connect((\"127.0.0.1\", SERVER_PORT))\n",
        "        s.close()\n",
        "        print(f\"Server is LISTENING on localhost:{SERVER_PORT} (took {attempt+1}s)\")\n",
        "        server_started.set()\n",
        "        break\n",
        "    except (socket.error, OSError):\n",
        "        if attempt % 5 == 4:\n",
        "            print(f\"  Still waiting ({attempt+1}s)...\")\n",
        "else:\n",
        "    if not server_started.is_set():\n",
        "        print(\"Server did not start within 30s\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "573add80-73fa-4356-bbdb-a34bc3b8a874"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"3\"></a>\n",
        "## 3. Verify PySpark via local server\n",
        "\n",
        "Before trying Scala, verify that a PySpark client can connect to the\n",
        "local server (not the remote endpoint)."
      ],
      "id": "7e50d982-230f-41a5-bade-b0be20622558"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Connect PySpark to the LOCAL server\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_local = SparkSession.builder.remote(f\"sc://localhost:{SERVER_PORT}\").getOrCreate()\n",
        "print(f\"PySpark connected to local server: {type(spark_local).__name__}\")\n",
        "print(f\"Spark version: {spark_local.version}\")\n",
        "\n",
        "# Basic test\n",
        "spark_local.sql(\"SELECT 1 AS test, CURRENT_USER() AS user\").show()\n",
        "print(\"PySpark via local server: WORKING\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fd3e2325-cb6b-450c-97d1-446d657b4575"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"4\"></a>\n",
        "## 4. Setup JVM + Scala Spark Connect client JARs\n",
        "\n",
        "Fetch the `spark-connect-client-jvm` JAR and its dependencies using\n",
        "coursier (installed in step 1). This JAR lets a Scala client speak\n",
        "the Spark Connect gRPC protocol. Idempotent — skips if already fetched."
      ],
      "id": "1d24c9ad-4a5e-44c7-88dc-dc5cefac53bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import pyspark\n",
        "SPARK_VERSION = pyspark.__version__  # e.g. \"3.5.6\"\n",
        "SCALA_VERSION = \"2.12\"\n",
        "JAR_DIR = os.path.expanduser(\"~/spark_connect_jars\")\n",
        "CP_FILE = os.path.join(JAR_DIR, \"spark_connect_classpath.txt\")\n",
        "os.makedirs(JAR_DIR, exist_ok=True)\n",
        "\n",
        "# cs_path was set in the setup cell above\n",
        "artifact = f\"org.apache.spark:spark-connect-client-jvm_{SCALA_VERSION}:{SPARK_VERSION}\"\n",
        "\n",
        "# Idempotent: skip if classpath already resolved\n",
        "if os.path.isfile(CP_FILE):\n",
        "    with open(CP_FILE) as f:\n",
        "        jars = [j for j in f.read().strip().split(\":\") if j and os.path.exists(j)]\n",
        "    if jars:\n",
        "        print(f\"Spark Connect client JARs already resolved: {len(jars)} JARs\")\n",
        "    else:\n",
        "        os.remove(CP_FILE)\n",
        "        jars = []\n",
        "\n",
        "if not os.path.isfile(CP_FILE):\n",
        "    print(f\"Fetching {artifact} via coursier...\")\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        [cs_path, \"fetch\", artifact, \"--classpath\"],\n",
        "        capture_output=True, text=True, timeout=300\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        classpath = result.stdout.strip()\n",
        "        jars = [j for j in classpath.split(\":\") if j]\n",
        "        with open(CP_FILE, \"w\") as f:\n",
        "            f.write(classpath)\n",
        "        print(f\"Resolved {len(jars)} JARs in {elapsed:.1f}s\")\n",
        "    else:\n",
        "        print(f\"Coursier failed: {result.stderr[-300:]}\")\n",
        "        jars = []"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bea36376-ec1f-46b5-acb3-188908d9d4b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"5\"></a>\n",
        "## 5. Connect Scala to local Spark Connect server\n",
        "\n",
        "Start a JVM via JPype with the `spark-connect-client-jvm` JARs on the\n",
        "classpath, then create a Scala SparkSession connected to `sc://localhost:15002`."
      ],
      "id": "a9931c3e-ae39-4ec9-949d-7fad126b08d7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import jpype\n",
        "\n",
        "if not jpype.isJVMStarted():\n",
        "    cp_file = os.path.expanduser(\"~/spark_connect_jars/spark_connect_classpath.txt\")\n",
        "    if os.path.isfile(cp_file):\n",
        "        with open(cp_file) as f:\n",
        "            classpath = [j for j in f.read().strip().split(\":\") if j]\n",
        "    else:\n",
        "        raise RuntimeError(\"No classpath file found. Run the JAR fetch cell first.\")\n",
        "    \n",
        "    jvm_options = [\n",
        "        \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n",
        "        \"-Xmx1g\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"Starting JVM with {len(classpath)} JARs...\")\n",
        "    jpype.startJVM(\n",
        "        jpype.getDefaultJVMPath(),\n",
        "        *jvm_options,\n",
        "        classpath=classpath,\n",
        "        convertStrings=True,\n",
        "    )\n",
        "    import jpype.imports\n",
        "    print(f\"JVM started: {jpype.getDefaultJVMPath()}\")\n",
        "else:\n",
        "    print(f\"JVM already running: {jpype.getDefaultJVMPath()}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "77a2ef55-63f6-4bb2-9f19-fb57c0a1024a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Create a Scala SparkSession connected to the local Spark Connect server\n",
        "try:\n",
        "    SparkSession = jpype.JClass(\"org.apache.spark.sql.SparkSession\")\n",
        "    \n",
        "    scala_spark = (\n",
        "        SparkSession.builder()\n",
        "        .remote(f\"sc://localhost:{SERVER_PORT}\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    \n",
        "    print(f\"Scala SparkSession created: {type(scala_spark)}\")\n",
        "    print(f\"Spark version: {scala_spark.version()}\")\n",
        "    print(\"\\n*** Scala Spark Connect via local proxy: WORKING ***\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to create Scala SparkSession: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fcf1da95-0718-450a-9442-b6e5c2322039"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"6\"></a>\n",
        "## 6. Scala Spark SQL tests\n",
        "\n",
        "If the Scala SparkSession is live, test running SQL on Snowflake."
      ],
      "id": "050287c4-3c47-46b7-9732-a6c5f601bcdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test: Run SQL from Scala through the local proxy to Snowflake\n",
        "try:\n",
        "    result = scala_spark.sql(\"SELECT 1 AS id, 'hello from scala' AS msg\")\n",
        "    result.show()\n",
        "    print(\"Scala Spark SQL: WORKING\")\n",
        "except Exception as e:\n",
        "    print(f\"Scala SQL failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4cb41207-c2d2-4319-9cf5-0f5e7d1ec708"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test: Query Snowflake data from Scala\n",
        "try:\n",
        "    result = scala_spark.sql(\"SELECT CURRENT_USER() AS user\")\n",
        "    result.show()\n",
        "except Exception as e:\n",
        "    print(f\"CURRENT_USER query failed: {e}\")\n",
        "\n",
        "try:\n",
        "    result = scala_spark.sql(\n",
        "        \"SELECT TABLE_NAME, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES LIMIT 3\"\n",
        "    )\n",
        "    result.show()\n",
        "except Exception as e:\n",
        "    print(f\"INFORMATION_SCHEMA query failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0526db23-1cc7-4d0f-a068-8fbdc7231819"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test: Interop — Snowpark Python writes, Scala Spark reads\n",
        "try:\n",
        "    session.sql(\"\"\"\n",
        "        CREATE OR REPLACE TRANSIENT TABLE _SCALA_CONNECT_TEST AS\n",
        "        SELECT 1 AS id, 'from_snowpark_python' AS source\n",
        "    \"\"\").collect()\n",
        "    print(\"Snowpark Python: wrote _SCALA_CONNECT_TEST\")\n",
        "    \n",
        "    scala_df = scala_spark.sql(\"SELECT * FROM _SCALA_CONNECT_TEST\")\n",
        "    scala_df.show()\n",
        "    print(\"Scala Spark read it via local proxy!\")\n",
        "    \n",
        "    session.sql(\"DROP TABLE IF EXISTS _SCALA_CONNECT_TEST\").collect()\n",
        "    print(\"Cleanup done\")\n",
        "except Exception as e:\n",
        "    print(f\"Interop test failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "964bec8f-ada3-4001-9056-e1fc9cc93f70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"7\"></a>\n",
        "## 7. Findings\n",
        "\n",
        "| Question | Result | Notes |\n",
        "|----------|--------|-------|\n",
        "| Local server starts? | | `start_session` on localhost:15002 |\n",
        "| PySpark via local server? | | Separate from remote endpoint test |\n",
        "| JVM + spark-connect JARs? | | coursier fetch |\n",
        "| Scala SparkSession created? | | `SparkSession.builder().remote(...)` |\n",
        "| Scala SQL to Snowflake? | | Through local proxy |\n",
        "| Scala reads Snowflake tables? | | INFORMATION_SCHEMA etc |\n",
        "| Snowpark ↔ Scala Spark interop? | | Via transient tables |\n",
        "| Auth: no PAT needed? | | SPCS token via Python proxy |"
      ],
      "id": "a12b5ae5-e5ed-47fc-bd47-5eae45b9f282"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}