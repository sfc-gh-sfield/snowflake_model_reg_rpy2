{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b61bf119-a052-476c-8ef7-306241ee0c59",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Scala Spark Connect via Local Python Proxy (Path B)\n\n**Goal:** Run Scala Spark code in a Workspace Notebook by:\n1. Starting a local Spark Connect server in Python (consumes SPCS OAuth token)\n2. Connecting a Scala Spark client to `sc://localhost:15002` via JPype\n\nThis avoids PAT — the Python proxy handles auth using the container's\nSnowpark session, and Scala just talks to localhost.\n\n**Architecture:**\n```\nPython: start_session(is_daemon=False, remote_url=\"sc://localhost:15002\")\n   ↓  (consumes SPCS OAuth token via Snowpark session)\nScala (JPype) → sc://localhost:15002 → Python proxy → Snowflake warehouse\n```\n\n**Self-contained:** This notebook installs everything it needs. It can run\non a fresh container or one where other notebooks have already been executed.\nAll installs are idempotent.\n\n## Contents\n\n1. [Setup: Install dependencies](#1)\n2. [Start local Spark Connect server](#2)\n3. [Verify PySpark via local server](#3)\n4. [Setup JVM + Scala Spark Connect client](#4)\n5. [Connect Scala to local server](#5)\n6. [Scala Spark SQL tests](#6)\n7. [Findings](#7)"
    },
    {
      "cell_type": "markdown",
      "id": "14165fbd-4aa7-4fd6-97ce-eccebd6656a6",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 1. Setup: Install dependencies\n\nAll dependencies are installed from scratch. Each install is idempotent —\nsafe to re-run if already installed from a prior notebook or previous run.\n\nWe need:\n- `snowpark-connect[jdk]` + `pyspark` (PySpark + bundled JDK)\n- `JPype1` (Python-JVM bridge)\n- `coursier` (JVM dependency resolver, for fetching Spark Connect client JARs)\n- `spark-connect-client-jvm` Scala JAR (fetched via coursier)"
    },
    {
      "cell_type": "code",
      "id": "f79494d8-d760-439f-ab5f-1f613bc6594b",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 1\nimport subprocess, sys, os, time, shutil, pathlib, socket\n\ndef _pip_install(package, label=None):\n    \"\"\"Install a pip package if not already importable. Idempotent.\"\"\"\n    label = label or package\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n        capture_output=True, text=True, timeout=300\n    )\n    if result.returncode == 0:\n        print(f\"  {label}: OK\")\n    else:\n        print(f\"  {label}: FAILED — {result.stderr[-200:]}\")\n\nprint(\"=== Installing Python packages (idempotent) ===\")\n_pip_install(\"snowpark-connect[jdk]\", \"snowpark-connect\")\n_pip_install(\"pyspark==3.5.6\", \"pyspark\")\n_pip_install(\"JPype1\", \"JPype1\")\n\nimport snowflake.snowpark_connect\nimport pyspark\nimport jpype\nprint(f\"\\nsnowpark-connect: imported OK\")\nprint(f\"PySpark version:  {pyspark.__version__}\")\nprint(f\"JPype1 version:   {jpype.__version__}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "34713092-9227-4c24-bc79-0f4a86d6979e",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 2 — Find or install Java (idempotent)\nMICROMAMBA_ROOT = pathlib.Path.home() / \"micromamba\"\nMM_ENV = MICROMAMBA_ROOT / \"envs\" / \"jvm_env\"\n\ndef _set_java_home(java_home_path):\n    \"\"\"Set JAVA_HOME and update PATH.\"\"\"\n    os.environ[\"JAVA_HOME\"] = str(java_home_path)\n    bin_dir = pathlib.Path(java_home_path) / \"bin\"\n    if str(bin_dir) not in os.environ.get(\"PATH\", \"\"):\n        os.environ[\"PATH\"] = f\"{bin_dir}:\" + os.environ.get(\"PATH\", \"\")\n\njava_search_paths = [\n    (\"PATH (pre-existing)\",          lambda: shutil.which(\"java\")),\n    (\"snowpark-connect[jdk] bundle\", lambda: pathlib.Path(sys.prefix) / \"jdk\"),\n    (\"micromamba jvm_env\",           lambda: MM_ENV),\n]\n\njava_found = False\nfor label, path_fn in java_search_paths:\n    path = path_fn()\n    if path and isinstance(path, str):\n        _set_java_home(pathlib.Path(path).resolve().parent.parent)\n        java_found = True\n        print(f\"Java source: {label}\")\n        break\n    elif path and isinstance(path, pathlib.Path) and path.exists():\n        _set_java_home(path)\n        java_found = True\n        print(f\"Java source: {label}\")\n        break\n\nif not java_found:\n    print(\"No Java found — installing OpenJDK 17 via micromamba...\")\n    os.environ[\"MAMBA_ROOT_PREFIX\"] = str(MICROMAMBA_ROOT)\n    mm_bin = MICROMAMBA_ROOT / \"bin\" / \"micromamba\"\n\n    if not mm_bin.exists():\n        print(\"  Downloading micromamba...\")\n        (MICROMAMBA_ROOT / \"bin\").mkdir(parents=True, exist_ok=True)\n        subprocess.run(\n            \"cd /tmp && \"\n            \"curl -Ls --retry 3 --retry-delay 2 \"\n            \"https://micro.mamba.pm/api/micromamba/linux-64/latest \"\n            \"| tar -xvj bin/micromamba 2>/dev/null && \"\n            f\"mv bin/micromamba {mm_bin} && rmdir bin 2>/dev/null || true\",\n            shell=True, check=True, timeout=60\n        )\n        print(f\"  micromamba installed: {mm_bin}\")\n\n    os.environ[\"PATH\"] = f\"{MICROMAMBA_ROOT / 'bin'}:\" + os.environ.get(\"PATH\", \"\")\n\n    if not MM_ENV.exists():\n        print(\"  Creating jvm_env with OpenJDK 17...\")\n        subprocess.run(\n            f\"{mm_bin} create -y -n jvm_env -c conda-forge openjdk=17 -q\",\n            shell=True, check=True, timeout=120,\n            env={**os.environ, \"MAMBA_ROOT_PREFIX\": str(MICROMAMBA_ROOT)}\n        )\n\n    if MM_ENV.exists():\n        _set_java_home(MM_ENV)\n        print(f\"  JDK ready: {MM_ENV}\")\n    else:\n        raise RuntimeError(\"FATAL: Could not install Java via micromamba\")\n\njava_path = shutil.which(\"java\")\nprint(f\"\\nJava binary:  {java_path or 'NOT FOUND'}\")\nif java_path:\n    jv = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n    print(f\"Java version: {jv.stderr.splitlines()[0]}\")\nprint(f\"JAVA_HOME:    {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n\n# --- Install coursier (idempotent) ---\nCS_DIR = os.path.expanduser(\"~/scala_jars\")\nos.makedirs(CS_DIR, exist_ok=True)\ncs_path = shutil.which(\"cs\") or os.path.join(CS_DIR, \"cs\")\n\nif not os.path.isfile(cs_path):\n    print(\"\\nInstalling coursier...\")\n    subprocess.run(\n        f\"curl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz \"\n        f\"| gzip -d > {cs_path} && chmod +x {cs_path}\",\n        shell=True, check=True, timeout=60\n    )\n    print(f\"Coursier installed: {cs_path}\")\nelse:\n    print(f\"\\nCoursier already at: {cs_path}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "36cae1e5-6ff7-4b5f-afe3-1097088d09e7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 2. Start local Spark Connect server\n\nUse the `snowpark_connect` API to start a local Spark Connect gRPC\nserver that listens on `localhost:15002`. This server consumes the\nWorkspace's Snowpark session (SPCS OAuth) and proxies requests to\nSnowflake.\n\nThe server runs in a background thread so the notebook stays interactive."
    },
    {
      "cell_type": "code",
      "id": "a4ec9a18-ae9f-4f4a-bf20-8ca67ea7a05c",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 3 — Write config.toml for the Spark Connect server\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\ndef _safe(fn):\n    try:\n        v = fn()\n        return v.strip('\"') if v else \"\"\n    except Exception:\n        return \"\"\n\nconn_info = {\n    \"account\": _safe(lambda: session.sql(\"SELECT CURRENT_ACCOUNT()\").collect()[0][0]),\n    \"user\": _safe(lambda: session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]),\n    \"role\": _safe(session.get_current_role),\n    \"database\": _safe(session.get_current_database),\n    \"schema\": _safe(session.get_current_schema),\n    \"warehouse\": _safe(session.get_current_warehouse),\n    \"host\": os.environ.get(\"SNOWFLAKE_HOST\", \"\"),\n}\n\nspcs_token = \"\"\nif os.path.isfile(\"/snowflake/session/token\"):\n    with open(\"/snowflake/session/token\") as f:\n        spcs_token = f.read().strip()\n\nconfig_dir = pathlib.Path.home() / \".snowflake\"\nconfig_dir.mkdir(parents=True, exist_ok=True)\nconfig_file = config_dir / \"config.toml\"\n\ntoml_content = f'''[connections.spark-connect]\nhost = \"{conn_info['host']}\"\naccount = \"{conn_info['account']}\"\nuser = \"{conn_info['user']}\"\ntoken = \"{spcs_token}\"\nauthenticator = \"oauth\"\nwarehouse = \"{conn_info['warehouse']}\"\ndatabase = \"{conn_info['database']}\"\nschema = \"{conn_info['schema']}\"\nrole = \"{conn_info['role']}\"\n'''\n\nconfig_file.write_text(toml_content)\nconfig_file.chmod(0o600)\nprint(f\"Config written to {config_file}\")\nprint(f\"Account: {conn_info['account']}, User: {conn_info['user']}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "432a7b02-dbbc-4fb8-ac7b-599cdfd17dec",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 4 — Explore snowpark_connect API\nimport snowflake.snowpark_connect as spc\n\nprint(\"=== snowpark_connect top-level attributes ===\")\nfor attr in sorted(dir(spc)):\n    if not attr.startswith(\"_\"):\n        obj = getattr(spc, attr)\n        print(f\"  {attr}: {type(obj).__name__}\")\n\n# Check for server/start_session APIs\nif hasattr(spc, 'server'):\n    print(\"\\n=== snowpark_connect.server attributes ===\")\n    for attr in sorted(dir(spc.server)):\n        if not attr.startswith(\"_\"):\n            print(f\"  {attr}: {type(getattr(spc.server, attr)).__name__}\")\n\nif hasattr(spc, 'client'):\n    print(\"\\n=== snowpark_connect.client attributes ===\")\n    for attr in sorted(dir(spc.client)):\n        if not attr.startswith(\"_\"):\n            print(f\"  {attr}: {type(getattr(spc.client, attr)).__name__}\")\n\n# Check for start_session specifically\nfor mod_name in ['', '.server', '.client']:\n    try:\n        mod = eval(f'spc{mod_name}')\n        if hasattr(mod, 'start_session'):\n            import inspect\n            sig = inspect.signature(mod.start_session)\n            print(f\"\\n=== {f'spc{mod_name}'}.start_session signature ===\")\n            print(f\"  {sig}\")\n    except Exception:\n        pass",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "573add80-73fa-4356-bbdb-a34bc3b8a874",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 5 — Start local Spark Connect server\nimport threading\n\nSERVER_PORT = 15002\n\n# Check if server is already listening (idempotent — e.g. re-run without restart)\ndef _port_open(port):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(1)\n        s.connect((\"127.0.0.1\", port))\n        s.close()\n        return True\n    except (socket.error, OSError):\n        return False\n\nif _port_open(SERVER_PORT):\n    print(f\"Spark Connect server already listening on localhost:{SERVER_PORT}\")\nelse:\n    server_error = [None]\n\n    def _start_server():\n        try:\n            spc.start_session(\n                is_daemon=False,\n                remote_url=f\"sc://localhost:{SERVER_PORT}\",\n                snowpark_session=session,\n            )\n        except Exception as e:\n            server_error[0] = str(e)\n\n    server_thread = threading.Thread(target=_start_server, daemon=True)\n    server_thread.start()\n\n    print(f\"Waiting for Spark Connect server on localhost:{SERVER_PORT}...\")\n    for attempt in range(30):\n        time.sleep(1)\n        if server_error[0]:\n            print(f\"Server failed: {server_error[0]}\")\n            break\n        if _port_open(SERVER_PORT):\n            print(f\"Server LISTENING on localhost:{SERVER_PORT} (took {attempt+1}s)\")\n            break\n        if attempt % 5 == 4:\n            print(f\"  Still waiting ({attempt+1}s)...\")\n    else:\n        print(\"Server did not start within 30s\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "7e50d982-230f-41a5-bade-b0be20622558",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 3. Verify PySpark via local server\n\nBefore trying Scala, verify that a PySpark client can connect to the\nlocal server (not the remote endpoint)."
    },
    {
      "cell_type": "code",
      "id": "fd3e2325-cb6b-450c-97d1-446d657b4575",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 6 — Verify PySpark via local server\nfrom pyspark.sql import SparkSession\n\nspark_local = SparkSession.builder.remote(f\"sc://localhost:{SERVER_PORT}\").getOrCreate()\nprint(f\"PySpark connected to local server: {type(spark_local).__name__}\")\nprint(f\"Spark version: {spark_local.version}\")\n\n# Basic test\nspark_local.sql(\"SELECT 1 AS test, CURRENT_USER() AS user\").show()\nprint(\"PySpark via local server: WORKING\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "1d24c9ad-4a5e-44c7-88dc-dc5cefac53bc",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 4. Setup JVM + Scala Spark Connect client JARs\n\nFetch the `spark-connect-client-jvm` JAR and its dependencies using\ncoursier (installed in step 1). This JAR lets a Scala client speak\nthe Spark Connect gRPC protocol. Idempotent — skips if already fetched."
    },
    {
      "cell_type": "code",
      "id": "bea36376-ec1f-46b5-acb3-188908d9d4b7",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 7 — Fetch Spark Connect client JARs via coursier\nimport pyspark\nSPARK_VERSION = pyspark.__version__  # e.g. \"3.5.6\"\nSCALA_VERSION = \"2.12\"\nJAR_DIR = os.path.expanduser(\"~/spark_connect_jars\")\nCP_FILE = os.path.join(JAR_DIR, \"spark_connect_classpath.txt\")\nos.makedirs(JAR_DIR, exist_ok=True)\n\n# cs_path was set in the setup cell above\nartifact = f\"org.apache.spark:spark-connect-client-jvm_{SCALA_VERSION}:{SPARK_VERSION}\"\n\n# Idempotent: skip if classpath already resolved\nif os.path.isfile(CP_FILE):\n    with open(CP_FILE) as f:\n        jars = [j for j in f.read().strip().split(\":\") if j and os.path.exists(j)]\n    if jars:\n        print(f\"Spark Connect client JARs already resolved: {len(jars)} JARs\")\n    else:\n        os.remove(CP_FILE)\n        jars = []\n\nif not os.path.isfile(CP_FILE):\n    print(f\"Fetching {artifact} via coursier...\")\n    t0 = time.time()\n    result = subprocess.run(\n        [cs_path, \"fetch\", artifact, \"--classpath\"],\n        capture_output=True, text=True, timeout=300\n    )\n    elapsed = time.time() - t0\n    \n    if result.returncode == 0:\n        classpath = result.stdout.strip()\n        jars = [j for j in classpath.split(\":\") if j]\n        with open(CP_FILE, \"w\") as f:\n            f.write(classpath)\n        print(f\"Resolved {len(jars)} JARs in {elapsed:.1f}s\")\n    else:\n        print(f\"Coursier failed: {result.stderr[-300:]}\")\n        jars = []",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "a9931c3e-ae39-4ec9-949d-7fad126b08d7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 5. Connect Scala to local Spark Connect server\n\nStart a JVM via JPype with the `spark-connect-client-jvm` JARs on the\nclasspath, then create a Scala SparkSession connected to `sc://localhost:15002`."
    },
    {
      "cell_type": "code",
      "id": "77a2ef55-63f6-4bb2-9f19-fb57c0a1024a",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 8 — Start JVM with Spark Connect JARs\n\nif not jpype.isJVMStarted():\n    cp_file = os.path.expanduser(\"~/spark_connect_jars/spark_connect_classpath.txt\")\n    if os.path.isfile(cp_file):\n        with open(cp_file) as f:\n            classpath = [j for j in f.read().strip().split(\":\") if j]\n    else:\n        raise RuntimeError(\"No classpath file found. Run the JAR fetch cell first.\")\n    \n    jvm_options = [\n        \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n        \"-Xmx1g\",\n    ]\n    \n    print(f\"Starting JVM with {len(classpath)} JARs...\")\n    jpype.startJVM(\n        jpype.getDefaultJVMPath(),\n        *jvm_options,\n        classpath=classpath,\n        convertStrings=True,\n    )\n    import jpype.imports\n    print(f\"JVM started: {jpype.getDefaultJVMPath()}\")\nelse:\n    print(f\"JVM already running: {jpype.getDefaultJVMPath()}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "fcf1da95-0718-450a-9442-b6e5c2322039",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 9 — Create Scala SparkSession via Spark Connect\ntry:\n    SparkSession = jpype.JClass(\"org.apache.spark.sql.SparkSession\")\n    \n    scala_spark = (\n        SparkSession.builder()\n        .remote(f\"sc://localhost:{SERVER_PORT}\")\n        .getOrCreate()\n    )\n    \n    print(f\"Scala SparkSession created: {type(scala_spark)}\")\n    print(f\"Spark version: {scala_spark.version()}\")\n    print(\"\\n*** Scala Spark Connect via local proxy: WORKING ***\")\n    \nexcept Exception as e:\n    print(f\"Failed to create Scala SparkSession: {type(e).__name__}: {e}\")\n    import traceback\n    traceback.print_exc()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "050287c4-3c47-46b7-9732-a6c5f601bcdf",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 6. Scala Spark SQL tests\n\nIf the Scala SparkSession is live, test running SQL on Snowflake."
    },
    {
      "cell_type": "code",
      "id": "4cb41207-c2d2-4319-9cf5-0f5e7d1ec708",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 10 — Scala Spark SQL basic test\ntry:\n    result = scala_spark.sql(\"SELECT 1 AS id, 'hello from scala' AS msg\")\n    result.show()\n    print(\"Scala Spark SQL: WORKING\")\nexcept Exception as e:\n    print(f\"Scala SQL failed: {e}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "0526db23-1cc7-4d0f-a068-8fbdc7231819",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 11 — Scala queries against Snowflake\ntry:\n    result = scala_spark.sql(\"SELECT CURRENT_USER() AS user\")\n    result.show()\nexcept Exception as e:\n    print(f\"CURRENT_USER query failed: {e}\")\n\ntry:\n    result = scala_spark.sql(\n        \"SELECT TABLE_NAME, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES LIMIT 3\"\n    )\n    result.show()\nexcept Exception as e:\n    print(f\"INFORMATION_SCHEMA query failed: {e}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "964bec8f-ada3-4001-9056-e1fc9cc93f70",
      "metadata": {
        "language": "python"
      },
      "source": "# Cell 12 — Interop: Snowpark Python writes, Scala Spark reads\ntry:\n    session.sql(\"\"\"\n        CREATE OR REPLACE TRANSIENT TABLE _SCALA_CONNECT_TEST AS\n        SELECT 1 AS id, 'from_snowpark_python' AS source\n    \"\"\").collect()\n    print(\"Snowpark Python: wrote _SCALA_CONNECT_TEST\")\n    \n    scala_df = scala_spark.sql(\"SELECT * FROM _SCALA_CONNECT_TEST\")\n    scala_df.show()\n    print(\"Scala Spark read it via local proxy!\")\n    \n    session.sql(\"DROP TABLE IF EXISTS _SCALA_CONNECT_TEST\").collect()\n    print(\"Cleanup done\")\nexcept Exception as e:\n    print(f\"Interop test failed: {e}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "a12b5ae5-e5ed-47fc-bd47-5eae45b9f282",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 7. Findings\n\n| Question | Result | Notes |\n|----------|--------|-------|\n| Local server starts? | | `start_session` on localhost:15002 |\n| PySpark via local server? | | Separate from remote endpoint test |\n| JVM + spark-connect JARs? | | coursier fetch |\n| Scala SparkSession created? | | `SparkSession.builder().remote(...)` |\n| Scala SQL to Snowflake? | | Through local proxy |\n| Scala reads Snowflake tables? | | INFORMATION_SCHEMA etc |\n| Snowpark ↔ Scala Spark interop? | | Via transient tables |\n| Auth: no PAT needed? | | SPCS token via Python proxy |"
    }
  ]
}