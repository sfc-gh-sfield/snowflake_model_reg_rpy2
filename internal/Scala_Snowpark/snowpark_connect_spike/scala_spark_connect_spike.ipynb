{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# Scala Spark Connect via Local Python Proxy (Path B)\n",
        "\n",
        "**Goal:** Run Scala Spark code in a Workspace Notebook by:\n",
        "1. Starting a local Spark Connect server in Python (consumes SPCS OAuth token)\n",
        "2. Connecting a Scala Spark client to `sc://localhost:15002` via JPype\n",
        "\n",
        "This avoids PAT — the Python proxy handles auth using the container's\n",
        "Snowpark session, and Scala just talks to localhost.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Python: start_session(is_daemon=False, remote_url=\"sc://localhost:15002\")\n",
        "   ↓  (consumes SPCS OAuth token via Snowpark session)\n",
        "Scala (JPype) → sc://localhost:15002 → Python proxy → Snowflake warehouse\n",
        "```\n",
        "\n",
        "**Self-contained:** This notebook installs everything it needs. It can run\n",
        "on a fresh container or one where other notebooks have already been executed.\n",
        "All installs are idempotent.\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. Setup: Install dependencies (Cells 1–2)\n",
        "2. Start local Spark Connect server (Cells 3–5)\n",
        "3. Verify PySpark via local server (Cell 6)\n",
        "4. Setup JVM + Scala Spark Connect client (Cell 7)\n",
        "5. Connect Scala to local server (Cells 8–9)\n",
        "6. Scala Spark SQL tests (Cells 10–12)\n",
        "7. Findings"
      ],
      "id": "b61bf119-a052-476c-8ef7-306241ee0c59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 1. Setup: Install dependencies\n",
        "\n",
        "All dependencies are installed from scratch. Each install is idempotent —\n",
        "safe to re-run if already installed from a prior notebook or previous run.\n",
        "\n",
        "We need:\n",
        "- `snowpark-connect[jdk]` + `pyspark` (PySpark + bundled JDK)\n",
        "- `JPype1` (Python-JVM bridge)\n",
        "- `coursier` (JVM dependency resolver, for fetching Spark Connect client JARs)\n",
        "- `spark-connect-client-jvm` Scala JAR (fetched via coursier)"
      ],
      "id": "14165fbd-4aa7-4fd6-97ce-eccebd6656a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 1\n",
        "import subprocess, sys, os, time, shutil, pathlib, socket\n",
        "\n",
        "def _pip_install(package, label=None):\n",
        "    \"\"\"Install a pip package if not already importable. Idempotent.\"\"\"\n",
        "    label = label or package\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n",
        "        capture_output=True, text=True, timeout=300\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(f\"  {label}: OK\")\n",
        "    else:\n",
        "        print(f\"  {label}: FAILED — {result.stderr[-200:]}\")\n",
        "\n",
        "print(\"=== Installing Python packages (idempotent) ===\")\n",
        "_pip_install(\"snowpark-connect[jdk]\", \"snowpark-connect\")\n",
        "_pip_install(\"pyspark==3.5.6\", \"pyspark\")\n",
        "_pip_install(\"JPype1\", \"JPype1\")\n",
        "_pip_install(\"opentelemetry-exporter-otlp\", \"opentelemetry-exporter-otlp\")\n",
        "\n",
        "import snowflake.snowpark_connect\n",
        "import pyspark\n",
        "import jpype\n",
        "print(f\"\\nsnowpark-connect: imported OK\")\n",
        "print(f\"PySpark version:  {pyspark.__version__}\")\n",
        "print(f\"JPype1 version:   {jpype.__version__}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f79494d8-d760-439f-ab5f-1f613bc6594b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 2 — Find or install Java (idempotent)\n",
        "MICROMAMBA_ROOT = pathlib.Path.home() / \"micromamba\"\n",
        "MM_ENV = MICROMAMBA_ROOT / \"envs\" / \"jvm_env\"\n",
        "\n",
        "def _set_java_home(java_home_path):\n",
        "    \"\"\"Set JAVA_HOME and update PATH.\"\"\"\n",
        "    os.environ[\"JAVA_HOME\"] = str(java_home_path)\n",
        "    bin_dir = pathlib.Path(java_home_path) / \"bin\"\n",
        "    if str(bin_dir) not in os.environ.get(\"PATH\", \"\"):\n",
        "        os.environ[\"PATH\"] = f\"{bin_dir}:\" + os.environ.get(\"PATH\", \"\")\n",
        "\n",
        "java_search_paths = [\n",
        "    (\"PATH (pre-existing)\",          lambda: shutil.which(\"java\")),\n",
        "    (\"snowpark-connect[jdk] bundle\", lambda: pathlib.Path(sys.prefix) / \"jdk\"),\n",
        "    (\"micromamba jvm_env\",           lambda: MM_ENV),\n",
        "]\n",
        "\n",
        "java_found = False\n",
        "for label, path_fn in java_search_paths:\n",
        "    path = path_fn()\n",
        "    if path and isinstance(path, str):\n",
        "        _set_java_home(pathlib.Path(path).resolve().parent.parent)\n",
        "        java_found = True\n",
        "        print(f\"Java source: {label}\")\n",
        "        break\n",
        "    elif path and isinstance(path, pathlib.Path) and path.exists():\n",
        "        _set_java_home(path)\n",
        "        java_found = True\n",
        "        print(f\"Java source: {label}\")\n",
        "        break\n",
        "\n",
        "if not java_found:\n",
        "    print(\"No Java found — installing OpenJDK 17 via micromamba...\")\n",
        "    os.environ[\"MAMBA_ROOT_PREFIX\"] = str(MICROMAMBA_ROOT)\n",
        "    mm_bin = MICROMAMBA_ROOT / \"bin\" / \"micromamba\"\n",
        "\n",
        "    if not mm_bin.exists():\n",
        "        print(\"  Downloading micromamba...\")\n",
        "        (MICROMAMBA_ROOT / \"bin\").mkdir(parents=True, exist_ok=True)\n",
        "        subprocess.run(\n",
        "            \"cd /tmp && \"\n",
        "            \"curl -Ls --retry 3 --retry-delay 2 \"\n",
        "            \"https://micro.mamba.pm/api/micromamba/linux-64/latest \"\n",
        "            \"| tar -xvj bin/micromamba 2>/dev/null && \"\n",
        "            f\"mv bin/micromamba {mm_bin} && rmdir bin 2>/dev/null || true\",\n",
        "            shell=True, check=True, timeout=60\n",
        "        )\n",
        "        print(f\"  micromamba installed: {mm_bin}\")\n",
        "\n",
        "    os.environ[\"PATH\"] = f\"{MICROMAMBA_ROOT / 'bin'}:\" + os.environ.get(\"PATH\", \"\")\n",
        "\n",
        "    if not MM_ENV.exists():\n",
        "        print(\"  Creating jvm_env with OpenJDK 17...\")\n",
        "        subprocess.run(\n",
        "            f\"{mm_bin} create -y -n jvm_env -c conda-forge openjdk=17 -q\",\n",
        "            shell=True, check=True, timeout=120,\n",
        "            env={**os.environ, \"MAMBA_ROOT_PREFIX\": str(MICROMAMBA_ROOT)}\n",
        "        )\n",
        "\n",
        "    if MM_ENV.exists():\n",
        "        _set_java_home(MM_ENV)\n",
        "        print(f\"  JDK ready: {MM_ENV}\")\n",
        "    else:\n",
        "        raise RuntimeError(\"FATAL: Could not install Java via micromamba\")\n",
        "\n",
        "java_path = shutil.which(\"java\")\n",
        "print(f\"\\nJava binary:  {java_path or 'NOT FOUND'}\")\n",
        "if java_path:\n",
        "    jv = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
        "    print(f\"Java version: {jv.stderr.splitlines()[0]}\")\n",
        "print(f\"JAVA_HOME:    {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n",
        "\n",
        "# JVM flags applied via env var so they take effect regardless of who starts\n",
        "# the JVM (snowpark_connect.start_session starts it before our Cell 8).\n",
        "os.environ[\"JAVA_TOOL_OPTIONS\"] = \" \".join([\n",
        "    \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n",
        "    \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\",\n",
        "    \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n",
        "    \"--add-opens=java.base/java.util=ALL-UNNAMED\",\n",
        "])\n",
        "print(f\"JAVA_TOOL_OPTIONS set for Arrow/Java 17 compatibility\")\n",
        "\n",
        "# --- Install coursier JAR launcher (idempotent) ---\n",
        "# Using the JAR launcher instead of the native binary because the GraalVM\n",
        "# native image crashes in containers (PhysicalMemory.size detection failure).\n",
        "CS_DIR = os.path.expanduser(\"~/scala_jars\")\n",
        "os.makedirs(CS_DIR, exist_ok=True)\n",
        "cs_jar = os.path.join(CS_DIR, \"coursier.jar\")\n",
        "\n",
        "if not os.path.isfile(cs_jar):\n",
        "    print(\"\\nDownloading coursier JAR launcher...\")\n",
        "    subprocess.run(\n",
        "        f\"curl -fL -o {cs_jar} \"\n",
        "        f\"https://github.com/coursier/coursier/releases/latest/download/coursier\",\n",
        "        shell=True, check=True, timeout=60\n",
        "    )\n",
        "    print(f\"Coursier JAR: {cs_jar}\")\n",
        "else:\n",
        "    print(f\"\\nCoursier JAR already at: {cs_jar}\")\n",
        "\n",
        "def cs_fetch(artifact):\n",
        "    \"\"\"Run coursier fetch using the JAR launcher, returning the classpath string.\"\"\"\n",
        "    env = {k: v for k, v in os.environ.items() if k != \"JAVA_TOOL_OPTIONS\"}\n",
        "    result = subprocess.run(\n",
        "        [\"java\", \"-jar\", cs_jar, \"fetch\", artifact, \"--classpath\"],\n",
        "        capture_output=True, text=True, timeout=300, env=env,\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(f\"coursier fetch failed: {result.stderr[-300:]}\")\n",
        "    return result.stdout.strip()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "34713092-9227-4c24-bc79-0f4a86d6979e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 2. Start local Spark Connect server\n",
        "\n",
        "Use the `snowpark_connect` API to start a local Spark Connect gRPC\n",
        "server that listens on `localhost:15002`. This server consumes the\n",
        "Workspace's Snowpark session (SPCS OAuth) and proxies requests to\n",
        "Snowflake.\n",
        "\n",
        "The server runs in a background thread so the notebook stays interactive."
      ],
      "id": "36cae1e5-6ff7-4b5f-afe3-1097088d09e7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 3 — Write config.toml for the Spark Connect server\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "def _safe(fn):\n",
        "    try:\n",
        "        v = fn()\n",
        "        return v.strip('\"') if v else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "conn_info = {\n",
        "    \"account\": _safe(lambda: session.sql(\"SELECT CURRENT_ACCOUNT()\").collect()[0][0]),\n",
        "    \"user\": _safe(lambda: session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]),\n",
        "    \"role\": _safe(session.get_current_role),\n",
        "    \"database\": _safe(session.get_current_database),\n",
        "    \"schema\": _safe(session.get_current_schema),\n",
        "    \"warehouse\": _safe(session.get_current_warehouse),\n",
        "    \"host\": os.environ.get(\"SNOWFLAKE_HOST\", \"\"),\n",
        "}\n",
        "\n",
        "spcs_token = \"\"\n",
        "if os.path.isfile(\"/snowflake/session/token\"):\n",
        "    with open(\"/snowflake/session/token\") as f:\n",
        "        spcs_token = f.read().strip()\n",
        "\n",
        "config_dir = pathlib.Path.home() / \".snowflake\"\n",
        "config_dir.mkdir(parents=True, exist_ok=True)\n",
        "config_file = config_dir / \"config.toml\"\n",
        "\n",
        "toml_content = f'''[connections.spark-connect]\n",
        "host = \"{conn_info['host']}\"\n",
        "account = \"{conn_info['account']}\"\n",
        "user = \"{conn_info['user']}\"\n",
        "token = \"{spcs_token}\"\n",
        "authenticator = \"oauth\"\n",
        "warehouse = \"{conn_info['warehouse']}\"\n",
        "database = \"{conn_info['database']}\"\n",
        "schema = \"{conn_info['schema']}\"\n",
        "role = \"{conn_info['role']}\"\n",
        "'''\n",
        "\n",
        "config_file.write_text(toml_content)\n",
        "config_file.chmod(0o600)\n",
        "print(f\"Config written to {config_file}\")\n",
        "print(f\"Account: {conn_info['account']}, User: {conn_info['user']}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a4ec9a18-ae9f-4f4a-bf20-8ca67ea7a05c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 4 — Explore snowpark_connect API\n",
        "import snowflake.snowpark_connect as spc\n",
        "\n",
        "print(\"=== snowpark_connect top-level attributes ===\")\n",
        "for attr in sorted(dir(spc)):\n",
        "    if not attr.startswith(\"_\"):\n",
        "        obj = getattr(spc, attr)\n",
        "        print(f\"  {attr}: {type(obj).__name__}\")\n",
        "\n",
        "# Check for server/start_session APIs\n",
        "if hasattr(spc, 'server'):\n",
        "    print(\"\\n=== snowpark_connect.server attributes ===\")\n",
        "    for attr in sorted(dir(spc.server)):\n",
        "        if not attr.startswith(\"_\"):\n",
        "            print(f\"  {attr}: {type(getattr(spc.server, attr)).__name__}\")\n",
        "\n",
        "if hasattr(spc, 'client'):\n",
        "    print(\"\\n=== snowpark_connect.client attributes ===\")\n",
        "    for attr in sorted(dir(spc.client)):\n",
        "        if not attr.startswith(\"_\"):\n",
        "            print(f\"  {attr}: {type(getattr(spc.client, attr)).__name__}\")\n",
        "\n",
        "# Check for start_session specifically\n",
        "for mod_name in ['', '.server', '.client']:\n",
        "    try:\n",
        "        mod = eval(f'spc{mod_name}')\n",
        "        if hasattr(mod, 'start_session'):\n",
        "            import inspect\n",
        "            sig = inspect.signature(mod.start_session)\n",
        "            print(f\"\\n=== {f'spc{mod_name}'}.start_session signature ===\")\n",
        "            print(f\"  {sig}\")\n",
        "    except Exception:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "432a7b02-dbbc-4fb8-ac7b-599cdfd17dec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 5 — Start local Spark Connect server\n",
        "import threading\n",
        "\n",
        "SERVER_PORT = 15002\n",
        "\n",
        "# Check if server is already listening (idempotent — e.g. re-run without restart)\n",
        "def _port_open(port):\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        s.settimeout(1)\n",
        "        s.connect((\"127.0.0.1\", port))\n",
        "        s.close()\n",
        "        return True\n",
        "    except (socket.error, OSError):\n",
        "        return False\n",
        "\n",
        "if _port_open(SERVER_PORT):\n",
        "    print(f\"Spark Connect server already listening on localhost:{SERVER_PORT}\")\n",
        "else:\n",
        "    server_error = [None]\n",
        "\n",
        "    def _start_server():\n",
        "        try:\n",
        "            spc.start_session(\n",
        "                is_daemon=False,\n",
        "                remote_url=f\"sc://localhost:{SERVER_PORT}\",\n",
        "                snowpark_session=session,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            server_error[0] = str(e)\n",
        "\n",
        "    server_thread = threading.Thread(target=_start_server, daemon=True)\n",
        "    server_thread.start()\n",
        "\n",
        "    print(f\"Waiting for Spark Connect server on localhost:{SERVER_PORT}...\")\n",
        "    for attempt in range(30):\n",
        "        time.sleep(1)\n",
        "        if server_error[0]:\n",
        "            print(f\"Server failed: {server_error[0]}\")\n",
        "            break\n",
        "        if _port_open(SERVER_PORT):\n",
        "            print(f\"Server LISTENING on localhost:{SERVER_PORT} (took {attempt+1}s)\")\n",
        "            break\n",
        "        if attempt % 5 == 4:\n",
        "            print(f\"  Still waiting ({attempt+1}s)...\")\n",
        "    else:\n",
        "        print(\"Server did not start within 30s\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "573add80-73fa-4356-bbdb-a34bc3b8a874"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 3. Verify PySpark via local server\n",
        "\n",
        "Before trying Scala, verify that a PySpark client can connect to the\n",
        "local server (not the remote endpoint)."
      ],
      "id": "7e50d982-230f-41a5-bade-b0be20622558"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 6 — Verify PySpark via local server\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_local = SparkSession.builder.remote(f\"sc://localhost:{SERVER_PORT}\").getOrCreate()\n",
        "print(f\"PySpark connected to local server: {type(spark_local).__name__}\")\n",
        "print(f\"Spark version: {spark_local.version}\")\n",
        "\n",
        "# Basic test\n",
        "spark_local.sql(\"SELECT 1 AS test, CURRENT_USER() AS user\").show()\n",
        "print(\"PySpark via local server: WORKING\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fd3e2325-cb6b-450c-97d1-446d657b4575"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 4. Setup JVM + Scala Spark Connect client JARs\n",
        "\n",
        "Fetch the `spark-connect-client-jvm` JAR and its dependencies using\n",
        "coursier (installed in step 1). This JAR lets a Scala client speak\n",
        "the Spark Connect gRPC protocol. Idempotent — skips if already fetched."
      ],
      "id": "1d24c9ad-4a5e-44c7-88dc-dc5cefac53bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 7 — Fetch Spark Connect client JARs via coursier\n",
        "import pyspark\n",
        "SPARK_VERSION = pyspark.__version__  # e.g. \"3.5.6\"\n",
        "SCALA_VERSION = \"2.12\"\n",
        "JAR_DIR = os.path.expanduser(\"~/spark_connect_jars\")\n",
        "CP_FILE = os.path.join(JAR_DIR, \"spark_connect_classpath.txt\")\n",
        "os.makedirs(JAR_DIR, exist_ok=True)\n",
        "\n",
        "artifact = f\"org.apache.spark:spark-connect-client-jvm_{SCALA_VERSION}:{SPARK_VERSION}\"\n",
        "\n",
        "# Idempotent: skip if classpath already resolved with valid JARs\n",
        "if os.path.isfile(CP_FILE):\n",
        "    with open(CP_FILE) as f:\n",
        "        jars = [j for j in f.read().strip().split(\":\") if j and os.path.exists(j)]\n",
        "    if jars:\n",
        "        print(f\"Spark Connect client JARs already resolved: {len(jars)} JARs\")\n",
        "    else:\n",
        "        os.remove(CP_FILE)\n",
        "        jars = []\n",
        "\n",
        "if not os.path.isfile(CP_FILE):\n",
        "    print(f\"Fetching {artifact} via coursier JAR launcher...\")\n",
        "    t0 = time.time()\n",
        "    classpath = cs_fetch(artifact)\n",
        "    elapsed = time.time() - t0\n",
        "    jars = [j for j in classpath.split(\":\") if j]\n",
        "    with open(CP_FILE, \"w\") as f:\n",
        "        f.write(classpath)\n",
        "    print(f\"Resolved {len(jars)} JARs in {elapsed:.1f}s\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bea36376-ec1f-46b5-acb3-188908d9d4b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 5. Connect Scala to local Spark Connect server\n",
        "\n",
        "Start a JVM via JPype with the `spark-connect-client-jvm` JARs on the\n",
        "classpath, then create a Scala SparkSession connected to `sc://localhost:15002`."
      ],
      "id": "a9931c3e-ae39-4ec9-949d-7fad126b08d7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 8 — Start JVM with Spark Connect JARs\n",
        "\n",
        "if not jpype.isJVMStarted():\n",
        "    cp_file = os.path.expanduser(\"~/spark_connect_jars/spark_connect_classpath.txt\")\n",
        "    if os.path.isfile(cp_file):\n",
        "        with open(cp_file) as f:\n",
        "            classpath = [j for j in f.read().strip().split(\":\") if j]\n",
        "    else:\n",
        "        raise RuntimeError(\"No classpath file found. Run the JAR fetch cell first.\")\n",
        "    \n",
        "    jvm_options = [\n",
        "        \"--add-opens=java.base/java.nio=ALL-UNNAMED\",\n",
        "        \"-Xmx1g\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"Starting JVM with {len(classpath)} JARs...\")\n",
        "    jpype.startJVM(\n",
        "        jpype.getDefaultJVMPath(),\n",
        "        *jvm_options,\n",
        "        classpath=classpath,\n",
        "        convertStrings=True,\n",
        "    )\n",
        "    import jpype.imports\n",
        "    print(f\"JVM started: {jpype.getDefaultJVMPath()}\")\n",
        "else:\n",
        "    print(f\"JVM already running: {jpype.getDefaultJVMPath()}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "77a2ef55-63f6-4bb2-9f19-fb57c0a1024a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 9 — Create Scala SparkSession via Spark Connect\n",
        "try:\n",
        "    SparkSession = jpype.JClass(\"org.apache.spark.sql.SparkSession\")\n",
        "    \n",
        "    scala_spark = (\n",
        "        SparkSession.builder()\n",
        "        .remote(f\"sc://localhost:{SERVER_PORT}\")\n",
        "        .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    \n",
        "    print(f\"Scala SparkSession created: {type(scala_spark)}\")\n",
        "    print(f\"Spark version: {scala_spark.version()}\")\n",
        "    print(\"\\n*** Scala Spark Connect via local proxy: WORKING ***\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to create Scala SparkSession: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fcf1da95-0718-450a-9442-b6e5c2322039"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 6. Scala Spark SQL tests\n",
        "\n",
        "If the Scala SparkSession is live, test running SQL on Snowflake."
      ],
      "id": "050287c4-3c47-46b7-9732-a6c5f601bcdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 10 — Scala Spark SQL basic test\n",
        "try:\n",
        "    result = scala_spark.sql(\"SELECT 1 AS id, 'hello from scala' AS msg\")\n",
        "    result.show()\n",
        "    print(\"Scala Spark SQL: WORKING\")\n",
        "except Exception as e:\n",
        "    print(f\"Scala SQL failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4cb41207-c2d2-4319-9cf5-0f5e7d1ec708"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 11 — Scala queries against Snowflake\n",
        "try:\n",
        "    result = scala_spark.sql(\"SELECT CURRENT_USER() AS user\")\n",
        "    result.show()\n",
        "except Exception as e:\n",
        "    print(f\"CURRENT_USER query failed: {e}\")\n",
        "\n",
        "try:\n",
        "    result = scala_spark.sql(\n",
        "        \"SELECT TABLE_NAME, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES LIMIT 3\"\n",
        "    )\n",
        "    result.show()\n",
        "except Exception as e:\n",
        "    print(f\"INFORMATION_SCHEMA query failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0526db23-1cc7-4d0f-a068-8fbdc7231819"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cell 12 — Interop: Snowpark Python writes, Scala Spark reads\n",
        "try:\n",
        "    session.sql(\"\"\"\n",
        "        CREATE OR REPLACE TRANSIENT TABLE _SCALA_CONNECT_TEST AS\n",
        "        SELECT 1 AS id, 'from_snowpark_python' AS source\n",
        "    \"\"\").collect()\n",
        "    print(\"Snowpark Python: wrote _SCALA_CONNECT_TEST\")\n",
        "    \n",
        "    scala_df = scala_spark.sql(\"SELECT * FROM _SCALA_CONNECT_TEST\")\n",
        "    scala_df.show()\n",
        "    print(\"Scala Spark read it via local proxy!\")\n",
        "    \n",
        "    session.sql(\"DROP TABLE IF EXISTS _SCALA_CONNECT_TEST\").collect()\n",
        "    print(\"Cleanup done\")\n",
        "except Exception as e:\n",
        "    print(f\"Interop test failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "964bec8f-ada3-4001-9056-e1fc9cc93f70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "## 7. Findings\n",
        "\n",
        "| Question | Result | Notes |\n",
        "|----------|--------|-------|\n",
        "| Local server starts? | YES | `spc.start_session(snowpark_session=session)` on localhost:15002, ~5s |\n",
        "| PySpark via local server? | YES | `SparkSession.builder.remote(\"sc://localhost:15002\")` |\n",
        "| JVM + spark-connect JARs? | YES | coursier JAR launcher (native binary crashes in containers) |\n",
        "| Scala SparkSession created? | YES | Needs `spark.sql.session.timeZone` config |\n",
        "| Scala SQL to Snowflake? | YES | `SELECT 1`, `CURRENT_USER()` work through local proxy |\n",
        "| Scala reads Snowflake tables? | YES | `INFORMATION_SCHEMA.TABLES` query works |\n",
        "| Snowpark <-> Scala Spark interop? | YES | Python writes transient table, Scala reads via Spark SQL |\n",
        "| Auth: no PAT needed? | YES | SPCS OAuth token consumed by Python proxy |\n",
        "\n",
        "**Key learnings:**\n",
        "- `JAVA_TOOL_OPTIONS` must be set before any JVM starts (snowpark_connect starts it internally)\n",
        "- Coursier native binary (GraalVM) fails in containers; use the JAR launcher instead\n",
        "- First run takes ~2 min (pip installs + coursier JAR resolution); re-runs are fast\n",
        "- `spark.sql.session.timeZone` must be set on the Scala SparkSession builder"
      ],
      "id": "a12b5ae5-e5ed-47fc-bd47-5eae45b9f282"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}