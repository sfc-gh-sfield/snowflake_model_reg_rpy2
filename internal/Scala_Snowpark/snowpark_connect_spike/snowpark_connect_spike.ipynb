{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "# Snowpark Connect for Spark — Workspace Notebook Spike\n",
        "\n",
        "**Goal:** Test whether Snowpark Connect for Spark can be installed and used\n",
        "within a Snowflake Workspace Notebook.\n",
        "\n",
        "**Key questions:**\n",
        "1. Can we `pip install snowpark-connect` in the Workspace?\n",
        "2. Can the local Spark Connect gRPC server bind to localhost inside the SPCS container?\n",
        "3. Can we authenticate — via SPCS OAuth token, PAT, or config.toml?\n",
        "4. Can we run PySpark DataFrame operations on Snowflake warehouse compute?\n",
        "5. Can we connect a Scala Spark client to the running server?\n",
        "\n",
        "**Docs:**\n",
        "- [Overview](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview)\n",
        "- [Jupyter / VS Code setup](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-workloads-jupyter)\n",
        "\n",
        "---\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. [Install Snowpark Connect](#1)\n",
        "2. [Environment Checks](#2)\n",
        "3. [Auth Strategy: SPCS OAuth Token](#3)\n",
        "4. [Auth Strategy: PAT (as used in R/ADBC)](#4)\n",
        "5. [Auth Strategy: config.toml](#5)\n",
        "6. [Start Spark Connect Session](#6)\n",
        "7. [PySpark DataFrame Test](#7)\n",
        "8. [Scala Client Test (if server works)](#8)\n",
        "9. [Findings](#9)"
      ],
      "id": "981e81ff-1fe0-4d42-8ba2-f82e017c159e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"1\"></a>\n",
        "## 1. Install Snowpark Connect\n",
        "\n",
        "Try installing with the `[jdk]` extra first (bundles Java). If it conflicts\n",
        "with an existing JDK (e.g. from the Scala prototype's micromamba install),\n",
        "we'll try without `[jdk]` and point to our existing Java."
      ],
      "id": "eaaa439e-1d64-4a96-9ea2-e12f9825d941"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import subprocess, sys, time\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Attempt 1: with bundled JDK\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"snowpark-connect[jdk]\", \"-q\"],\n",
        "    capture_output=True, text=True, timeout=300\n",
        ")\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(f\"Installed snowpark-connect[jdk] in {elapsed:.1f}s\")\n",
        "else:\n",
        "    print(f\"snowpark-connect[jdk] failed ({elapsed:.1f}s):\")\n",
        "    print(result.stderr[-500:] if result.stderr else \"no stderr\")\n",
        "    print(\"\\nRetrying without [jdk] extra...\")\n",
        "    \n",
        "    t0 = time.time()\n",
        "    result2 = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"snowpark-connect\", \"-q\"],\n",
        "        capture_output=True, text=True, timeout=300\n",
        "    )\n",
        "    elapsed2 = time.time() - t0\n",
        "    if result2.returncode == 0:\n",
        "        print(f\"Installed snowpark-connect (no jdk) in {elapsed2:.1f}s\")\n",
        "    else:\n",
        "        print(f\"snowpark-connect also failed ({elapsed2:.1f}s):\")\n",
        "        print(result2.stderr[-500:] if result2.stderr else \"no stderr\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b4fa3d9f-11c9-4ed6-b338-36104abf330e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Verify installation\n",
        "try:\n",
        "    import snowflake.snowpark_connect\n",
        "    print(f\"snowpark_connect imported OK\")\n",
        "    print(f\"  Location: {snowflake.snowpark_connect.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import pyspark\n",
        "    print(f\"PySpark version: {pyspark.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"PySpark not available: {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0db0854e-0376-423a-803c-4405ebb5b080"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"2\"></a>\n",
        "## 2. Environment Checks\n",
        "\n",
        "Check Python version, Java availability, architecture match, and\n",
        "whether localhost port binding is possible."
      ],
      "id": "40a9e998-a474-4227-84c0-77e3c2b8e84f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import platform, shutil, socket, os\n",
        "\n",
        "print(\"=== Environment ===\")\n",
        "print(f\"Python:       {sys.version}\")\n",
        "print(f\"Architecture: {platform.machine()}\")\n",
        "print(f\"OS:           {platform.platform()}\")\n",
        "\n",
        "# Java\n",
        "java_path = shutil.which(\"java\")\n",
        "print(f\"\\nJava binary:  {java_path or 'NOT FOUND'}\")\n",
        "if java_path:\n",
        "    jv = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
        "    print(f\"Java version: {jv.stderr.splitlines()[0]}\")\n",
        "\n",
        "java_home = os.environ.get(\"JAVA_HOME\", \"\")\n",
        "print(f\"JAVA_HOME:    {java_home or 'NOT SET'}\")\n",
        "\n",
        "# Port binding test\n",
        "print(\"\\n=== Port Binding Test ===\")\n",
        "test_port = 15002  # Spark Connect default\n",
        "try:\n",
        "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    sock.bind((\"127.0.0.1\", test_port))\n",
        "    sock.close()\n",
        "    print(f\"localhost:{test_port} — bind OK (port available)\")\n",
        "except OSError as e:\n",
        "    print(f\"localhost:{test_port} — bind FAILED: {e}\")\n",
        "\n",
        "# SPCS token\n",
        "spcs_token_path = \"/snowflake/session/token\"\n",
        "print(f\"\\n=== Auth Tokens ===\")\n",
        "print(f\"SPCS token:   {'EXISTS' if os.path.isfile(spcs_token_path) else 'NOT FOUND'}\")\n",
        "print(f\"SNOWFLAKE_PAT env: {'SET' if os.environ.get('SNOWFLAKE_PAT') else 'NOT SET'}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2413f12a-c597-427a-8907-ecb42547db86"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"3\"></a>\n",
        "## 3. Auth Strategy: SPCS OAuth Token\n",
        "\n",
        "Inside a Workspace Notebook, the container provides an OAuth token at\n",
        "`/snowflake/session/token`. Let's see if we can write a `config.toml`\n",
        "that Snowpark Connect accepts using this token.\n",
        "\n",
        "**Note:** This is speculative — the docs show user/password auth in\n",
        "config.toml. We'll try token-based auth and see what happens."
      ],
      "id": "dde4002b-80ec-4be0-b6d2-11f80c03daa8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "# Extract connection details from the active Python Snowpark session\n",
        "def _safe(fn):\n",
        "    try:\n",
        "        v = fn()\n",
        "        return v.strip('\"') if v else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "conn_info = {\n",
        "    \"account\": _safe(lambda: session.sql(\"SELECT CURRENT_ACCOUNT()\").collect()[0][0]),\n",
        "    \"user\": _safe(lambda: session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]),\n",
        "    \"role\": _safe(session.get_current_role),\n",
        "    \"database\": _safe(session.get_current_database),\n",
        "    \"schema\": _safe(session.get_current_schema),\n",
        "    \"warehouse\": _safe(session.get_current_warehouse),\n",
        "    \"host\": os.environ.get(\"SNOWFLAKE_HOST\", \"\"),\n",
        "}\n",
        "\n",
        "# Read SPCS token\n",
        "spcs_token = \"\"\n",
        "if os.path.isfile(\"/snowflake/session/token\"):\n",
        "    with open(\"/snowflake/session/token\") as f:\n",
        "        spcs_token = f.read().strip()\n",
        "\n",
        "print(\"Connection info extracted:\")\n",
        "for k, v in conn_info.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"  spcs_token: {'SET (' + str(len(spcs_token)) + ' chars)' if spcs_token else 'NOT FOUND'}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "02fbb318-8ca5-4ed1-83de-0637da30d3e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Write a config.toml for Snowpark Connect\n",
        "# Try with token auth first (speculative), fall through to PAT/password later\n",
        "import pathlib\n",
        "\n",
        "config_dir = pathlib.Path.home() / \".snowflake\"\n",
        "config_dir.mkdir(parents=True, exist_ok=True)\n",
        "config_file = config_dir / \"config.toml\"\n",
        "\n",
        "# Strategy A: Use token with authenticator=oauth\n",
        "toml_content = f'''[connections.spark-connect]\n",
        "host = \"{conn_info['host']}\"\n",
        "account = \"{conn_info['account']}\"\n",
        "user = \"{conn_info['user']}\"\n",
        "token = \"{spcs_token}\"\n",
        "authenticator = \"oauth\"\n",
        "warehouse = \"{conn_info['warehouse']}\"\n",
        "database = \"{conn_info['database']}\"\n",
        "schema = \"{conn_info['schema']}\"\n",
        "role = \"{conn_info['role']}\"\n",
        "'''\n",
        "\n",
        "config_file.write_text(toml_content)\n",
        "config_file.chmod(0o600)\n",
        "print(f\"Wrote {config_file}\")\n",
        "print(\"Auth strategy: SPCS OAuth token\")\n",
        "\n",
        "# Show (redacted)\n",
        "for line in toml_content.splitlines():\n",
        "    if 'token' in line.lower() and '=' in line:\n",
        "        key = line.split('=')[0].strip()\n",
        "        print(f\"  {key} = <redacted>\")\n",
        "    else:\n",
        "        print(f\"  {line}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "10c989c8-5146-452f-8a6c-f333f24f1629"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"4\"></a>\n",
        "## 4. Auth Strategy: PAT (as used in R/ADBC)\n",
        "\n",
        "If OAuth token auth doesn't work with Snowpark Connect, try PAT.\n",
        "We know PATs work for ADBC connections from inside Workspace Notebooks\n",
        "(proven in the R integration). Snowpark Connect's OSS client path\n",
        "explicitly supports PAT auth.\n",
        "\n",
        "**Skip this cell if Section 3 worked.**"
      ],
      "id": "77448d14-2a82-48d2-9920-32277ef9c437"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Uncomment and run this cell if OAuth auth (Section 3) didn't work.\n",
        "# You'll need a PAT — create one first if you don't have one.\n",
        "\n",
        "# from scala_helpers import ...  # or create PAT via SQL\n",
        "# pat = session.sql(\"ALTER USER SET ... ADD PAT ...\")  # etc.\n",
        "\n",
        "# For now, check if a PAT is already available\n",
        "pat = os.environ.get(\"SNOWFLAKE_PAT\", \"\")\n",
        "if pat:\n",
        "    toml_pat = f'''[connections.spark-connect]\n",
        "host = \"{conn_info['host']}\"\n",
        "account = \"{conn_info['account']}\"\n",
        "user = \"{conn_info['user']}\"\n",
        "token = \"{pat}\"\n",
        "authenticator = \"programmatic_access_token\"\n",
        "warehouse = \"{conn_info['warehouse']}\"\n",
        "database = \"{conn_info['database']}\"\n",
        "schema = \"{conn_info['schema']}\"\n",
        "role = \"{conn_info['role']}\"\n",
        "'''\n",
        "    config_file.write_text(toml_pat)\n",
        "    config_file.chmod(0o600)\n",
        "    print(\"Rewrote config.toml with PAT auth\")\n",
        "else:\n",
        "    print(\"No PAT available in SNOWFLAKE_PAT env var.\")\n",
        "    print(\"To test PAT auth, set os.environ['SNOWFLAKE_PAT'] = '<your-pat>'\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1e3a9a78-06bc-4b09-b2fb-7079070dc621"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"5\"></a>\n",
        "## 5. Auth Strategy: Direct URL with token (OSS client path)\n",
        "\n",
        "The Snowpark Connect OSS client docs show connecting via a URL with\n",
        "embedded PAT: `sc://host/;token=...;token_type=PAT`. This bypasses\n",
        "config.toml entirely.\n",
        "\n",
        "We'll also try the Snowpark Connect host URL for this account."
      ],
      "id": "68354c7d-59eb-46cb-b963-5dc4fde0bc62"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Discover the Snowpark Connect host for this account\n",
        "try:\n",
        "    rows = session.sql(\"\"\"\n",
        "        SELECT t.VALUE:type::VARCHAR as type,\n",
        "               t.VALUE:host::VARCHAR as host,\n",
        "               t.VALUE:port as port\n",
        "        FROM TABLE(FLATTEN(input => PARSE_JSON(SYSTEM$ALLOWLIST()))) AS t\n",
        "        WHERE type = 'SNOWPARK_CONNECT'\n",
        "    \"\"\").collect()\n",
        "    if rows:\n",
        "        for r in rows:\n",
        "            print(f\"Snowpark Connect endpoint: {r['HOST']}:{r['PORT']}\")\n",
        "    else:\n",
        "        print(\"No SNOWPARK_CONNECT entry in SYSTEM$ALLOWLIST()\")\n",
        "        print(\"Snowpark Connect may not be enabled for this account.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not query SYSTEM$ALLOWLIST(): {e}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "28b2aefe-916f-4f23-871f-ebbe97c5e136"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"6\"></a>\n",
        "## 6. Start Spark Connect Session\n",
        "\n",
        "This is the critical test — can `init_spark_session()` start the local\n",
        "gRPC server and create a SparkSession inside the Workspace container?"
      ],
      "id": "fa2219ed-f2ed-4ff1-b8fe-2ee331a134d7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "import time\n",
        "\n",
        "# Ensure JAVA_HOME is set (use micromamba JDK if available)\n",
        "if not os.environ.get(\"JAVA_HOME\") and shutil.which(\"java\"):\n",
        "    java_bin = shutil.which(\"java\")\n",
        "    java_home_guess = str(pathlib.Path(java_bin).resolve().parent.parent)\n",
        "    os.environ[\"JAVA_HOME\"] = java_home_guess\n",
        "    print(f\"Set JAVA_HOME={java_home_guess}\")\n",
        "\n",
        "try:\n",
        "    from snowflake import snowpark_connect\n",
        "    \n",
        "    t0 = time.time()\n",
        "    spark = snowpark_connect.server.init_spark_session()\n",
        "    elapsed = time.time() - t0\n",
        "    \n",
        "    print(f\"SparkSession created in {elapsed:.1f}s\")\n",
        "    print(f\"  Spark version: {spark.version}\")\n",
        "    print(f\"  Type:          {type(spark).__name__}\")\n",
        "    print(\"\\n*** Spark Connect session is RUNNING ***\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to start Spark Connect session: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5606e67e-ea61-4029-9047-6f746fc552fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"7\"></a>\n",
        "## 7. PySpark DataFrame Test\n",
        "\n",
        "If the SparkSession is running, test basic PySpark DataFrame operations\n",
        "that execute on the Snowflake warehouse."
      ],
      "id": "9dc27a07-2a51-4da6-9170-b12da05b1fe1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test 1: Create a DataFrame from local data\n",
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(id=1, name=\"Alice\", score=95.0),\n",
        "    Row(id=2, name=\"Bob\", score=87.5),\n",
        "    Row(id=3, name=\"Carol\", score=92.0),\n",
        "])\n",
        "\n",
        "print(\"=== Local DataFrame ===\")\n",
        "df.show()\n",
        "print(f\"Count: {df.count()}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "365c1930-188d-4bd6-b889-c13c53ececf1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test 2: Spark SQL against Snowflake\n",
        "result = spark.sql(\"SELECT CURRENT_USER() AS user\")\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c88f0f53-841a-4ab8-9c28-568f5931ffcf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test 3: Read Snowflake data via Spark SQL\n",
        "# Note: SHOW ... LIMIT N fails (Spark parser doesn't support LIMIT on SHOW).\n",
        "# INFORMATION_SCHEMA works fine.\n",
        "try:\n",
        "    info_df = spark.sql(\"SELECT TABLE_NAME, TABLE_TYPE, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES LIMIT 5\")\n",
        "    info_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"INFORMATION_SCHEMA query failed: {e}\")\n",
        "    print(\"\\nTrying a simple SELECT with VALUES...\")\n",
        "    spark.sql(\"SELECT * FROM VALUES (1, 'a'), (2, 'b') AS t(id, letter)\").show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6717d2f4-2c3d-47c9-88aa-c8254c82980f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "f7d4bfa2-a497-483e-9bd2-583bed623abe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Test 4: DataFrame transformations (pushed down to Snowflake)\n",
        "from pyspark.sql.functions import col, lit, upper\n",
        "\n",
        "transformed = (\n",
        "    df.filter(col(\"score\") > 88)\n",
        "      .withColumn(\"grade\", lit(\"A\"))\n",
        "      .withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "      .orderBy(col(\"score\").desc())\n",
        ")\n",
        "print(\"=== Transformed (pushed to Snowflake warehouse) ===\")\n",
        "transformed.show()\n",
        "\n",
        "# Convert to Pandas\n",
        "pdf = transformed.toPandas()\n",
        "print(f\"\\nAs Pandas DataFrame ({type(pdf).__name__}):\\n{pdf}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "043fe91e-d80d-4a8f-8986-2b9bef7364a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id=\"7b\"></a>\n",
        "## 7b. Spark ↔ Snowpark Python Interop\n",
        "\n",
        "Can we share data between the native Snowpark Python session and the\n",
        "Spark Connect session? Both connect to the same Snowflake account —\n",
        "test writing from one and reading from the other via transient tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test 5: Spark -> Snowpark Python (write from Spark, read from Snowpark)\n",
        "try:\n",
        "    spark.sql(\"CREATE OR REPLACE TRANSIENT TABLE _SPARK_INTEROP_TEST AS SELECT 1 AS id, 'from_spark' AS source\").collect()\n",
        "    print(\"Spark: wrote _SPARK_INTEROP_TEST\")\n",
        "    \n",
        "    snowpark_df = session.table(\"_SPARK_INTEROP_TEST\")\n",
        "    snowpark_df.show()\n",
        "    print(f\"Snowpark Python read it: {type(snowpark_df).__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"Spark -> Snowpark interop failed: {e}\")\n",
        "\n",
        "# Test 6: Snowpark Python -> Spark (write from Snowpark, read from Spark)\n",
        "try:\n",
        "    session.sql(\"CREATE OR REPLACE TRANSIENT TABLE _SNOWPARK_INTEROP_TEST AS SELECT 2 AS id, 'from_snowpark' AS source\").collect()\n",
        "    print(\"\\nSnowpark Python: wrote _SNOWPARK_INTEROP_TEST\")\n",
        "    \n",
        "    spark_df = spark.sql(\"SELECT * FROM _SNOWPARK_INTEROP_TEST\")\n",
        "    spark_df.show()\n",
        "    print(f\"Spark read it: {type(spark_df).__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"Snowpark -> Spark interop failed: {e}\")\n",
        "\n",
        "# Cleanup\n",
        "try:\n",
        "    session.sql(\"DROP TABLE IF EXISTS _SPARK_INTEROP_TEST\").collect()\n",
        "    session.sql(\"DROP TABLE IF EXISTS _SNOWPARK_INTEROP_TEST\").collect()\n",
        "    print(\"\\nCleanup done\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"8\"></a>\n",
        "## 8. Server Architecture Check\n",
        "\n",
        "Snowpark Connect uses a **remote** Spark Connect server on Snowflake's\n",
        "infrastructure — there is no local gRPC server in the container.\n",
        "The PySpark client connects directly to the Snowpark Connect endpoint.\n",
        "\n",
        "This means a Scala Spark Connect client would need to connect to the\n",
        "remote endpoint (not localhost), which requires different auth setup."
      ],
      "id": "6e29ec45-0292-4c8f-83c0-313ccdf34a64"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Check if the Spark Connect gRPC server is actually listening\n",
        "import socket\n",
        "\n",
        "def check_port(host, port):\n",
        "    try:\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        s.settimeout(2)\n",
        "        s.connect((host, port))\n",
        "        s.close()\n",
        "        return True\n",
        "    except (socket.error, OSError):\n",
        "        return False\n",
        "\n",
        "port_15002 = check_port(\"127.0.0.1\", 15002)\n",
        "print(f\"Spark Connect server on localhost:15002: {'LISTENING' if port_15002 else 'NOT LISTENING'}\")\n",
        "\n",
        "if not port_15002:\n",
        "    print(\"\\nThe server may use a different port. Checking common alternatives...\")\n",
        "    for p in [15001, 15003, 4040, 18080]:\n",
        "        if check_port(\"127.0.0.1\", p):\n",
        "            print(f\"  Port {p}: LISTENING\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2e2b06a9-b84e-449f-b9a1-b7ef8de8601b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": [
        "---\n",
        "<a id=\"9\"></a>\n",
        "## 9. Findings\n",
        "\n",
        "Record results after running the cells above.\n",
        "\n",
        "| Question | Result | Notes |\n",
        "|----------|--------|-------|\n",
        "| pip install works? | **YES** | `snowpark-connect[jdk]`, PySpark 3.5.6 |\n",
        "| Import succeeds? | **YES** | OpenTelemetry warning (harmless) |\n",
        "| Port binding works? | **YES** | localhost:15002 available, but server is remote |\n",
        "| Java needed locally? | **NO** | PySpark client doesn't need local JDK |\n",
        "| OAuth token auth? | **YES** | config.toml with SPCS token + `authenticator=oauth` |\n",
        "| PAT auth? | Not tested | OAuth worked; PAT likely works (proven for ADBC) |\n",
        "| init_spark_session() | **YES** | 17.9s, Spark 3.5.6, connects to remote endpoint |\n",
        "| PySpark createDataFrame | **YES** | Local data → Snowflake → show/count |\n",
        "| Spark SQL (standard) | **YES** | SELECT, INFORMATION_SCHEMA, CURRENT_USER() |\n",
        "| Spark SQL (SF-specific) | **PARTIAL** | CURRENT_ROLE() unsupported; SHOW LIMIT fails |\n",
        "| DataFrame transforms | **YES** | filter/withColumn/upper/orderBy pushed down |\n",
        "| toPandas() | | TBD |\n",
        "| Spark ↔ Snowpark interop | | TBD — transient table sharing |\n",
        "| Local gRPC server? | **NO** | Server is remote (Snowflake infra), not localhost |\n",
        "| Scala client | **N/A** | No local server to connect to |\n",
        "| Snowpark Connect endpoint | **YES** | `AK32940.snowpark.pdxaac.snowflakecomputing.com:443` |"
      ],
      "id": "3521bd55-47fb-4049-a11a-4b8bb8979436"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Summary: disk usage of key directories\n",
        "import shutil\n",
        "\n",
        "total, used, free = shutil.disk_usage(\"/\")\n",
        "print(f\"Disk: {used / (1024**3):.1f} GB used / {total / (1024**3):.1f} GB total / {free / (1024**3):.1f} GB free\")\n",
        "\n",
        "# Check installed package sizes\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"show\", \"snowpark-connect\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "if result.returncode == 0:\n",
        "    for line in result.stdout.splitlines():\n",
        "        if line.startswith((\"Name:\", \"Version:\", \"Location:\")):\n",
        "            print(line)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "11c0164d-e391-4da6-9945-c3a0037f6216"
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Cleanup: stop Spark session if running\n",
        "try:\n",
        "    spark.stop()\n",
        "    print(\"Spark session stopped\")\n",
        "except Exception:\n",
        "    print(\"No Spark session to stop\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d13b11d2-ce8b-4f03-8ac8-6126cc4a3ca9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}