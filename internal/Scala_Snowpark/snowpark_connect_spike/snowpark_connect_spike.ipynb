{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Snowpark Connect for Spark — Workspace Notebook Spike\n\n**Goal:** Test whether Snowpark Connect for Spark can be installed and used\nwithin a Snowflake Workspace Notebook.\n\n**Key questions:**\n1. Can we `pip install snowpark-connect` in the Workspace?\n2. Can the local Spark Connect gRPC server bind to localhost inside the SPCS container?\n3. Can we authenticate — via SPCS OAuth token, PAT, or config.toml?\n4. Can we run PySpark DataFrame operations on Snowflake warehouse compute?\n5. Can we connect a Scala Spark client to the running server?\n\n**Docs:**\n- [Overview](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview)\n- [Jupyter / VS Code setup](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-workloads-jupyter)\n\n---\n\n## Contents\n\n1. [Install Snowpark Connect](#1)\n2. [Environment Checks](#2)\n3. [Auth Strategy: SPCS OAuth Token](#3)\n4. [Auth Strategy: PAT (as used in R/ADBC)](#4)\n5. [Auth Strategy: config.toml](#5)\n6. [Start Spark Connect Session](#6)\n7. [PySpark DataFrame Test](#7)\n8. [Scala Client Test (if server works)](#8)\n9. [Findings](#9)",
      "id": "981e81ff-1fe0-4d42-8ba2-f82e017c159e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"1\"></a>\n## 1. Install Snowpark Connect\n\nTry installing with the `[jdk]` extra first (bundles Java). If it conflicts\nwith an existing JDK (e.g. from the Scala prototype's micromamba install),\nwe'll try without `[jdk]` and point to our existing Java.",
      "id": "eaaa439e-1d64-4a96-9ea2-e12f9825d941"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import subprocess, sys, time\n\nt0 = time.time()\n\n# Attempt 1: with bundled JDK\nresult = subprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"snowpark-connect[jdk]\", \"-q\"],\n    capture_output=True, text=True, timeout=300\n)\nelapsed = time.time() - t0\n\nif result.returncode == 0:\n    print(f\"Installed snowpark-connect[jdk] in {elapsed:.1f}s\")\nelse:\n    print(f\"snowpark-connect[jdk] failed ({elapsed:.1f}s):\")\n    print(result.stderr[-500:] if result.stderr else \"no stderr\")\n    print(\"\\nRetrying without [jdk] extra...\")\n    \n    t0 = time.time()\n    result2 = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"snowpark-connect\", \"-q\"],\n        capture_output=True, text=True, timeout=300\n    )\n    elapsed2 = time.time() - t0\n    if result2.returncode == 0:\n        print(f\"Installed snowpark-connect (no jdk) in {elapsed2:.1f}s\")\n    else:\n        print(f\"snowpark-connect also failed ({elapsed2:.1f}s):\")\n        print(result2.stderr[-500:] if result2.stderr else \"no stderr\")",
      "id": "b4fa3d9f-11c9-4ed6-b338-36104abf330e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Verify installation\ntry:\n    import snowflake.snowpark_connect\n    print(f\"snowpark_connect imported OK\")\n    print(f\"  Location: {snowflake.snowpark_connect.__file__}\")\nexcept ImportError as e:\n    print(f\"Import failed: {e}\")\n\ntry:\n    import pyspark\n    print(f\"PySpark version: {pyspark.__version__}\")\nexcept ImportError as e:\n    print(f\"PySpark not available: {e}\")",
      "id": "0db0854e-0376-423a-803c-4405ebb5b080"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"2\"></a>\n## 2. Environment Checks\n\nCheck Python version, Java availability, architecture match, and\nwhether localhost port binding is possible.",
      "id": "40a9e998-a474-4227-84c0-77e3c2b8e84f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import platform, shutil, socket, os\n\nprint(\"=== Environment ===\")\nprint(f\"Python:       {sys.version}\")\nprint(f\"Architecture: {platform.machine()}\")\nprint(f\"OS:           {platform.platform()}\")\n\n# Java\njava_path = shutil.which(\"java\")\nprint(f\"\\nJava binary:  {java_path or 'NOT FOUND'}\")\nif java_path:\n    jv = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n    print(f\"Java version: {jv.stderr.splitlines()[0]}\")\n\njava_home = os.environ.get(\"JAVA_HOME\", \"\")\nprint(f\"JAVA_HOME:    {java_home or 'NOT SET'}\")\n\n# Port binding test\nprint(\"\\n=== Port Binding Test ===\")\ntest_port = 15002  # Spark Connect default\ntry:\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.bind((\"127.0.0.1\", test_port))\n    sock.close()\n    print(f\"localhost:{test_port} — bind OK (port available)\")\nexcept OSError as e:\n    print(f\"localhost:{test_port} — bind FAILED: {e}\")\n\n# SPCS token\nspcs_token_path = \"/snowflake/session/token\"\nprint(f\"\\n=== Auth Tokens ===\")\nprint(f\"SPCS token:   {'EXISTS' if os.path.isfile(spcs_token_path) else 'NOT FOUND'}\")\nprint(f\"SNOWFLAKE_PAT env: {'SET' if os.environ.get('SNOWFLAKE_PAT') else 'NOT SET'}\")",
      "id": "2413f12a-c597-427a-8907-ecb42547db86"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"3\"></a>\n## 3. Auth Strategy: SPCS OAuth Token\n\nInside a Workspace Notebook, the container provides an OAuth token at\n`/snowflake/session/token`. Let's see if we can write a `config.toml`\nthat Snowpark Connect accepts using this token.\n\n**Note:** This is speculative — the docs show user/password auth in\nconfig.toml. We'll try token-based auth and see what happens.",
      "id": "dde4002b-80ec-4be0-b6d2-11f80c03daa8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# Extract connection details from the active Python Snowpark session\ndef _safe(fn):\n    try:\n        v = fn()\n        return v.strip('\"') if v else \"\"\n    except Exception:\n        return \"\"\n\nconn_info = {\n    \"account\": _safe(lambda: session.sql(\"SELECT CURRENT_ACCOUNT()\").collect()[0][0]),\n    \"user\": _safe(lambda: session.sql(\"SELECT CURRENT_USER()\").collect()[0][0]),\n    \"role\": _safe(session.get_current_role),\n    \"database\": _safe(session.get_current_database),\n    \"schema\": _safe(session.get_current_schema),\n    \"warehouse\": _safe(session.get_current_warehouse),\n    \"host\": os.environ.get(\"SNOWFLAKE_HOST\", \"\"),\n}\n\n# Read SPCS token\nspcs_token = \"\"\nif os.path.isfile(\"/snowflake/session/token\"):\n    with open(\"/snowflake/session/token\") as f:\n        spcs_token = f.read().strip()\n\nprint(\"Connection info extracted:\")\nfor k, v in conn_info.items():\n    print(f\"  {k}: {v}\")\nprint(f\"  spcs_token: {'SET (' + str(len(spcs_token)) + ' chars)' if spcs_token else 'NOT FOUND'}\")",
      "id": "02fbb318-8ca5-4ed1-83de-0637da30d3e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Write a config.toml for Snowpark Connect\n# Try with token auth first (speculative), fall through to PAT/password later\nimport pathlib\n\nconfig_dir = pathlib.Path.home() / \".snowflake\"\nconfig_dir.mkdir(parents=True, exist_ok=True)\nconfig_file = config_dir / \"config.toml\"\n\n# Strategy A: Use token with authenticator=oauth\ntoml_content = f'''[connections.spark-connect]\nhost = \"{conn_info['host']}\"\naccount = \"{conn_info['account']}\"\nuser = \"{conn_info['user']}\"\ntoken = \"{spcs_token}\"\nauthenticator = \"oauth\"\nwarehouse = \"{conn_info['warehouse']}\"\ndatabase = \"{conn_info['database']}\"\nschema = \"{conn_info['schema']}\"\nrole = \"{conn_info['role']}\"\n'''\n\nconfig_file.write_text(toml_content)\nconfig_file.chmod(0o600)\nprint(f\"Wrote {config_file}\")\nprint(\"Auth strategy: SPCS OAuth token\")\n\n# Show (redacted)\nfor line in toml_content.splitlines():\n    if 'token' in line.lower() and '=' in line:\n        key = line.split('=')[0].strip()\n        print(f\"  {key} = <redacted>\")\n    else:\n        print(f\"  {line}\")",
      "id": "10c989c8-5146-452f-8a6c-f333f24f1629"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"4\"></a>\n## 4. Auth Strategy: PAT (as used in R/ADBC)\n\nIf OAuth token auth doesn't work with Snowpark Connect, try PAT.\nWe know PATs work for ADBC connections from inside Workspace Notebooks\n(proven in the R integration). Snowpark Connect's OSS client path\nexplicitly supports PAT auth.\n\n**Skip this cell if Section 3 worked.**",
      "id": "77448d14-2a82-48d2-9920-32277ef9c437"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Uncomment and run this cell if OAuth auth (Section 3) didn't work.\n# You'll need a PAT — create one first if you don't have one.\n\n# from scala_helpers import ...  # or create PAT via SQL\n# pat = session.sql(\"ALTER USER SET ... ADD PAT ...\")  # etc.\n\n# For now, check if a PAT is already available\npat = os.environ.get(\"SNOWFLAKE_PAT\", \"\")\nif pat:\n    toml_pat = f'''[connections.spark-connect]\nhost = \"{conn_info['host']}\"\naccount = \"{conn_info['account']}\"\nuser = \"{conn_info['user']}\"\ntoken = \"{pat}\"\nauthenticator = \"programmatic_access_token\"\nwarehouse = \"{conn_info['warehouse']}\"\ndatabase = \"{conn_info['database']}\"\nschema = \"{conn_info['schema']}\"\nrole = \"{conn_info['role']}\"\n'''\n    config_file.write_text(toml_pat)\n    config_file.chmod(0o600)\n    print(\"Rewrote config.toml with PAT auth\")\nelse:\n    print(\"No PAT available in SNOWFLAKE_PAT env var.\")\n    print(\"To test PAT auth, set os.environ['SNOWFLAKE_PAT'] = '<your-pat>'\")",
      "id": "1e3a9a78-06bc-4b09-b2fb-7079070dc621"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"5\"></a>\n## 5. Auth Strategy: Direct URL with token (OSS client path)\n\nThe Snowpark Connect OSS client docs show connecting via a URL with\nembedded PAT: `sc://host/;token=...;token_type=PAT`. This bypasses\nconfig.toml entirely.\n\nWe'll also try the Snowpark Connect host URL for this account.",
      "id": "68354c7d-59eb-46cb-b963-5dc4fde0bc62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Discover the Snowpark Connect host for this account\ntry:\n    rows = session.sql(\"\"\"\n        SELECT t.VALUE:type::VARCHAR as type,\n               t.VALUE:host::VARCHAR as host,\n               t.VALUE:port as port\n        FROM TABLE(FLATTEN(input => PARSE_JSON(SYSTEM$ALLOWLIST()))) AS t\n        WHERE type = 'SNOWPARK_CONNECT'\n    \"\"\").collect()\n    if rows:\n        for r in rows:\n            print(f\"Snowpark Connect endpoint: {r['HOST']}:{r['PORT']}\")\n    else:\n        print(\"No SNOWPARK_CONNECT entry in SYSTEM$ALLOWLIST()\")\n        print(\"Snowpark Connect may not be enabled for this account.\")\nexcept Exception as e:\n    print(f\"Could not query SYSTEM$ALLOWLIST(): {e}\")",
      "id": "28b2aefe-916f-4f23-871f-ebbe97c5e136"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"6\"></a>\n## 6. Start Spark Connect Session\n\nThis is the critical test — can `init_spark_session()` start the local\ngRPC server and create a SparkSession inside the Workspace container?",
      "id": "fa2219ed-f2ed-4ff1-b8fe-2ee331a134d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import time\n\n# Ensure JAVA_HOME is set (use micromamba JDK if available)\nif not os.environ.get(\"JAVA_HOME\") and shutil.which(\"java\"):\n    java_bin = shutil.which(\"java\")\n    java_home_guess = str(pathlib.Path(java_bin).resolve().parent.parent)\n    os.environ[\"JAVA_HOME\"] = java_home_guess\n    print(f\"Set JAVA_HOME={java_home_guess}\")\n\ntry:\n    from snowflake import snowpark_connect\n    \n    t0 = time.time()\n    spark = snowpark_connect.server.init_spark_session()\n    elapsed = time.time() - t0\n    \n    print(f\"SparkSession created in {elapsed:.1f}s\")\n    print(f\"  Spark version: {spark.version}\")\n    print(f\"  Type:          {type(spark).__name__}\")\n    print(\"\\n*** Spark Connect session is RUNNING ***\")\n    \nexcept Exception as e:\n    print(f\"Failed to start Spark Connect session: {type(e).__name__}: {e}\")\n    import traceback\n    traceback.print_exc()",
      "id": "5606e67e-ea61-4029-9047-6f746fc552fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"7\"></a>\n## 7. PySpark DataFrame Test\n\nIf the SparkSession is running, test basic PySpark DataFrame operations\nthat execute on the Snowflake warehouse.",
      "id": "9dc27a07-2a51-4da6-9170-b12da05b1fe1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Test 1: Create a DataFrame from local data\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(id=1, name=\"Alice\", score=95.0),\n    Row(id=2, name=\"Bob\", score=87.5),\n    Row(id=3, name=\"Carol\", score=92.0),\n])\n\nprint(\"=== Local DataFrame ===\")\ndf.show()\nprint(f\"Count: {df.count()}\")",
      "id": "365c1930-188d-4bd6-b889-c13c53ececf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Test 2: Spark SQL against Snowflake\nresult = spark.sql(\"SELECT CURRENT_USER() AS user\")\nresult.show()",
      "id": "c88f0f53-841a-4ab8-9c28-568f5931ffcf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Test 3: Read a Snowflake table via Spark\ntry:\n    tables_df = spark.sql(\"SHOW TABLES LIMIT 5\")\n    tables_df.show(truncate=False)\nexcept Exception as e:\n    print(f\"SHOW TABLES failed: {e}\")\n    print(\"Trying a direct table read instead...\")\n    try:\n        info_schema = spark.sql(\"SELECT TABLE_NAME, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES LIMIT 5\")\n        info_schema.show()\n    except Exception as e2:\n        print(f\"Also failed: {e2}\")",
      "id": "6717d2f4-2c3d-47c9-88aa-c8254c82980f"
    },
    {
      "id": "f7d4bfa2-a497-483e-9bd2-583bed623abe",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Test 4: DataFrame transformations (pushed down to Snowflake)\nfrom pyspark.sql.functions import col, lit, upper\n\ntransformed = (\n    df.filter(col(\"score\") > 88)\n      .withColumn(\"grade\", lit(\"A\"))\n      .withColumn(\"name_upper\", upper(col(\"name\")))\n      .orderBy(col(\"score\").desc())\n)\nprint(\"=== Transformed (pushed to Snowflake warehouse) ===\")\ntransformed.show()",
      "id": "043fe91e-d80d-4a8f-8986-2b9bef7364a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"8\"></a>\n## 8. Scala Client Test\n\nIf the Spark Connect server is running on localhost:15002, test whether\nwe can connect from a Scala Spark client via JPype (reusing the Scala\nprototype's infrastructure).\n\nThis requires:\n- JDK (from micromamba or snowpark-connect[jdk])\n- `spark-connect-client-jvm` JAR on the classpath\n- JPype for in-process JVM\n\n**This section is exploratory — skip if the PySpark tests above didn't work.**",
      "id": "6e29ec45-0292-4c8f-83c0-313ccdf34a64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Check if the Spark Connect gRPC server is actually listening\nimport socket\n\ndef check_port(host, port):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(2)\n        s.connect((host, port))\n        s.close()\n        return True\n    except (socket.error, OSError):\n        return False\n\nport_15002 = check_port(\"127.0.0.1\", 15002)\nprint(f\"Spark Connect server on localhost:15002: {'LISTENING' if port_15002 else 'NOT LISTENING'}\")\n\nif not port_15002:\n    print(\"\\nThe server may use a different port. Checking common alternatives...\")\n    for p in [15001, 15003, 4040, 18080]:\n        if check_port(\"127.0.0.1\", p):\n            print(f\"  Port {p}: LISTENING\")",
      "id": "2e2b06a9-b84e-449f-b9a1-b7ef8de8601b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "---\n<a id=\"9\"></a>\n## 9. Findings\n\nRecord results after running the cells above.\n\n| Question | Result | Notes |\n|----------|--------|-------|\n| pip install works? | | |\n| Import succeeds? | | |\n| Port binding works? | | |\n| OAuth token auth? | | |\n| PAT auth? | | |\n| init_spark_session() works? | | |\n| PySpark DataFrames work? | | |\n| Spark SQL to Snowflake? | | |\n| Scala client connects? | | |\n| Install time | | |\n| Install size | | |",
      "id": "3521bd55-47fb-4049-a11a-4b8bb8979436"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Summary: disk usage of key directories\nimport shutil\n\ntotal, used, free = shutil.disk_usage(\"/\")\nprint(f\"Disk: {used / (1024**3):.1f} GB used / {total / (1024**3):.1f} GB total / {free / (1024**3):.1f} GB free\")\n\n# Check installed package sizes\nresult = subprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"show\", \"snowpark-connect\"],\n    capture_output=True, text=True\n)\nif result.returncode == 0:\n    for line in result.stdout.splitlines():\n        if line.startswith((\"Name:\", \"Version:\", \"Location:\")):\n            print(line)",
      "id": "11c0164d-e391-4da6-9945-c3a0037f6216"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Cleanup: stop Spark session if running\ntry:\n    spark.stop()\n    print(\"Spark session stopped\")\nexcept Exception:\n    print(\"No Spark session to stop\")",
      "id": "d13b11d2-ce8b-4f03-8ac8-6126cc4a3ca9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}